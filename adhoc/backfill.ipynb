{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f34cdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "from time import sleep\n",
    "from typing import List\n",
    "\n",
    "import duckdb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee2327d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedItem(BaseModel, arbitrary_types_allowed=True):\n",
    "    title: str\n",
    "    link: str\n",
    "    guid: str\n",
    "    categories: List[str]\n",
    "    dc_creator: str\n",
    "    pub_date: pd.Timestamp\n",
    "    atom_updated: pd.Timestamp\n",
    "    content_encoded: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afb6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata_wrapper = lambda x: f\"<![CDATA[{x}]]>\"\n",
    "def get_elements(url: str) -> FeedItem:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "\n",
    "    title = cdata_wrapper(soup.find(\"h1\").text.strip())\n",
    "    link = url\n",
    "    guid = f\"https://medium.com/p/{url.split('-')[-1]}\"\n",
    "    categories = [cdata_wrapper(i.text.strip()) for i in soup.find_all(\"a\", href=lambda x: x and \"medium.com/tag/\" in x)]\n",
    "    dc_creator = cdata_wrapper(soup.find(\"a\", {\"data-testid\": \"authorName\"}).text.strip())\n",
    "    pub_date = soup.find(\"span\", {\"data-testid\": \"storyPublishDate\"})\n",
    "    pub_date = pd.to_datetime(pub_date.text.strip())\n",
    "    atom_updated = pub_date\n",
    "    content = cdata_wrapper(soup.find(\"article\").get_text(separator=\"\\n\").strip())\n",
    "    \n",
    "    return FeedItem(\n",
    "        title=title,\n",
    "        link=link,\n",
    "        guid=guid,\n",
    "        categories=categories,\n",
    "        dc_creator=dc_creator,\n",
    "        pub_date=pub_date,\n",
    "        atom_updated=atom_updated,\n",
    "        content_encoded=content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "328dadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/backfill_urls.txt\", \"r\") as f:\n",
    "    urls = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37581f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs:   0%|          | 0/84 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing URLs: 100%|██████████| 84/84 [02:41<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction complete. Saved to ./data/medium_feed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for url in tqdm(urls, desc=\"Processing URLs\"):\n",
    "    try:\n",
    "        item = get_elements(url)\n",
    "        results.append(item)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "    sleep(1)  # To avoid hitting the server too hard\n",
    "df = pd.DataFrame([item.model_dump() for item in results])\n",
    "df.to_csv(\"./data/medium_feed.csv\", index=False)\n",
    "print(\"Data extraction complete. Saved to ./data/medium_feed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4838ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_frame = \"\"\"<rss xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:content=\"http://purl.org/rss/1.0/modules/content/\" xmlns:atom=\"http://www.w3.org/2005/Atom\" xmlns:cc=\"http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html\" version=\"2.0\">\n",
    "<channel>\n",
    "<title>\n",
    "<![CDATA[ Generative AI in the Newsroom - Medium ]]>\n",
    "</title>\n",
    "<description>\n",
    "<![CDATA[ The Generative AI in the Newsroom project is an effort to collaboratively figure out how and when (or when not) to use generative AI in news production. - Medium ]]>\n",
    "</description>\n",
    "<link>https://generative-ai-newsroom.com?source=rss----df04cefca135---4</link>\n",
    "<image>\n",
    "<url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>\n",
    "<title>Generative AI in the Newsroom - Medium</title>\n",
    "<link>https://generative-ai-newsroom.com?source=rss----df04cefca135---4</link>\n",
    "</image>\n",
    "<generator>GAIN GH Action</generator>\n",
    "<lastBuildDate>Thu, 29 May 2025 18:16:28 GMT</lastBuildDate>\n",
    "<atom:link href=\"\" rel=\"self\" type=\"application/rss+xml\"/>\n",
    "<webMaster>\n",
    "<![CDATA[ nicholas.hagar@northwestern.edu ]]>\n",
    "</webMaster>\n",
    "{items}\n",
    "</channel>\n",
    "</rss>\n",
    "\"\"\"\n",
    "\n",
    "item_frame = \"\"\"<item>\n",
    "<title>{title}</title>\n",
    "<link>{link}</link>\n",
    "<guid isPermaLink=\"false\">{guid}</guid>\n",
    "{categories}\n",
    "<dc:creator>{dc_creator}</dc:creator>\n",
    "<pubDate>{pub_date}</pubDate>\n",
    "<atom:updated>{atom_updated}</atom:updated>\n",
    "<content:encoded>{content_encoded}</content:encoded>\n",
    "</item>\"\"\"\n",
    "\n",
    "categories_frame = \"\"\"<category>{cat}</category>\"\"\"\n",
    "\n",
    "def make_item(row) -> str:\n",
    "    categories = \"\".join([categories_frame.format(cat=cat) for cat in ast.literal_eval(row[\"categories\"])])\n",
    "    return item_frame.format(\n",
    "        title=row[\"title\"],\n",
    "        link=row[\"link\"],\n",
    "        guid=row[\"guid\"],\n",
    "        categories=categories,\n",
    "        dc_creator=row[\"dc_creator\"],\n",
    "        pub_date=row[\"pub_date\"],\n",
    "        atom_updated=row[\"atom_updated\"],\n",
    "        content_encoded=row[\"content_encoded\"]\n",
    "    )\n",
    "\n",
    "def make_feed(df: pd.DataFrame) -> str:\n",
    "    items = \"\".join([make_item(row) for _, row in df.iterrows()])\n",
    "    return feed_frame.format(items=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51e16164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/medium_feed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33cdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed = make_feed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd3294fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSS feed generated and saved to ./data/gain_feed.xml\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/gain_feed.xml\", \"w\") as f:\n",
    "    f.write(feed)\n",
    "print(\"RSS feed generated and saved to ./data/gain_feed.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69279cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to duckdb\n",
    "con = duckdb.connect(database=\"./data/gain_feed.duckdb\", read_only=False)\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS medium_feed AS SELECT * FROM read_csv_auto('./data/medium_feed.csv')\")\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eb5c17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
