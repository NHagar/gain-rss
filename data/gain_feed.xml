<rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html" version="2.0">
<channel>
<title>
<![CDATA[ Generative AI in the Newsroom - Medium ]]>
</title>
<description>
<![CDATA[ The Generative AI in the Newsroom project is an effort to collaboratively figure out how and when (or when not) to use generative AI in news production. ]]>
</description>
<link>https://github.com/NHagar/gain-rss/blob/main/data/gain_feed.xml</link>
<image>
<url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
<title>Generative AI in the Newsroom - Medium</title>
<link>https://github.com/NHagar/gain-rss/blob/main/data/gain_feed.xml</link>
</image>
<generator>GAIN GH Action</generator>
<lastBuildDate>Thu, 29 May 2025 18:16:28 GMT</lastBuildDate>
<atom:link href="https://github.com/NHagar/gain-rss/blob/main/data/gain_feed.xml" rel="self" type="application/rss+xml"/>
<webMaster>
<![CDATA[ nicholas.hagar@northwestern.edu ]]>
</webMaster>
<item>
<title><![CDATA[ Tiny Tools: A Framework for Human-Centered Technology in Journalism ]]></title>
<link>https://generative-ai-newsroom.com/tiny-tools-a-framework-for-human-centered-technology-in-journalism-e2176dd66cbc?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/e2176dd66cbc</guid>
<category><![CDATA[ data-analysis ]]></category><category><![CDATA[ small-language-model ]]></category><category><![CDATA[ journalism ]]></category><category><![CDATA[ software-development ]]></category><category><![CDATA[ llm ]]></category>
<dc:creator><![CDATA[ Nick Hagar ]]></dc:creator>
<pubDate>Wed, 24 Sep 2025 10:31:52 GMT</pubDate>
<atom:updated>2025-09-24T10:31:51.235Z</atom_updated>
<content:encoded><![CDATA[ <h4>Journalists need small and focused tools that are interoperable but do one thing well.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*oEtQOSPNDh5uc1cf" /></figure><p><em>By Nick Hagar, Mandi Cai, and Jeremy Gilbert</em></p><p>When a data journalist needs to clean a messy dataset, they look to tools that are fast, lightweight, and easy to understand. They don’t need an enterprise data platform.</p><p>Yet many newsrooms rely on software built by outside vendors who don’t understand journalism’s unique needs — cloud-based tools without privacy protections, enterprise platforms that cost fortunes, and critical resources that disappear when companies pivot or fold. Journalists need scoped tools that enhance their expertise; what they get is software designed for everyone but them.</p><p>This dilemma was the inspiration for “Tiny Tools”, a framework for designing and evaluating software that we introduced at SRCCON 2025. A tiny tool is the antithesis of bloated enterprise software: It tackles a narrow set of high-value tasks and executes them well. It’s often home-grown, free, locally-controlled, and lightweight. Most importantly, it’s customizable — built for a particular community’s needs rather than mass-market appeal.</p><p>Our framework builds on a rich tradition, from the open source ethos of scoped, effective tools to recent calls for <a href="https://www.inkandswitch.com/essay/malleable-software/">customizable</a>, <a href="https://www.robinsloan.com/notes/home-cooked-app/">locally-controlled</a> software. We focus on understanding <em>newsroom workflows</em> and the tools that support them, and we pay special attention to where generative AI models might work as effective tiny tools.</p><h3>A Tool Does One Thing Well</h3><p>You import a messy CSV into <a href="https://openrefine.org/">OpenRefine</a> — donor records with inconsistent formatting, duplicate entries, and typos. The interface immediately shows you what’s possible: Cluster similar names, split columns, transform values. These aren’t buried in menus or hidden behind “smart” features. Every operation is a clear verb with a visible effect.</p><p>As you work, OpenRefine maintains transparency by allowing you to review changes before applying them and undo operations in any order. Nothing happens without your explicit consent, and everything that does happen is reversible and inspectable.</p><p>When you’re done, you export a clean CSV instead of some proprietary format locked in OpenRefine’s database. The tool enhanced your expertise without trapping your data. And if you need to do more, that CSV flows into <a href="https://duckdb.org/">DuckDB</a> for analysis, <a href="https://csvkit.readthedocs.io/en/latest/">csvkit</a> for further cleaning, or any other tool in your arsenal.</p><p>This is what we mean by doing one thing well: <strong>Clear verbs</strong> that make actions obvious. <strong>Transparent operations</strong> that show rather than hide complexity. <strong>Portability</strong> that respects your data ownership. <strong>Composability</strong> that acknowledges no single tool can do everything. These features are the difference between tools that empower journalists and platforms that entrap them.</p><h3>Workflows Are Composable Processes</h3><p>Individual tools matter, but the real power comes from how they work together. When you break complex tasks into discrete steps, you can build workflows that are both powerful and adaptable. This <em>interoperability</em> — the ability for different tools to exchange data and work together — is what transforms a collection of tiny tools into a powerful ecosystem.</p><p>Take a daily web traffic report for editors. In one sense, this is a discrete task, a to-do list item that an analyst might knock out as part of their daily work. But instead of treating this as one unwieldy task, we can break it down into steps: extract data from analytics platforms, clean the inevitable formatting issues, calculate the metrics editors actually want, articulate meaningful insights, and format everything for human consumption. Each step transforms data in a specific way, with clear inputs and outputs.</p><p>This approach requires methodical thinking:</p><p><strong>Define discrete operations.</strong> Each transformation should do one thing. Raw analytics data becomes a clean CSV. A clean CSV becomes calculated metrics. Metrics become written insights. Draw clear boundaries between steps — this clarity makes it possible to improve or replace individual components without breaking the entire workflow.</p><p><strong>Match operations to tools.</strong> Once you know exactly what each step needs to accomplish, you can evaluate tools with precision. Data cleaning might call for DuckDB’s SQL operations or csvkit’s specialized utilities. The choice depends on your constraints: Do you need local control? What’s your budget? How sensitive is the data?</p><p><strong>Identify automation boundaries.</strong> Not everything should be automated. Mechanical tasks — data cleaning, metrics calculation — are ripe for automation. But insight generation might benefit from human expertise, or perhaps carefully constrained LLM assistance. The key is knowing where machines excel and where human judgment remains essential.</p><p><strong>Design for flexibility.</strong> Newsroom needs change constantly. When editors suddenly want hourly updates instead of daily reports, a composed workflow lets you adjust one component rather than starting from scratch. New data source? Add a step. Different output format? Swap the final tool. The interoperable nature of your workflow means these changes are simple adjustments, not complete rebuilds.</p><p>Data pipelines are a familiar example for thinking through automation, but there are many newsroom workflows that could benefit from a similar step-by-step breakdown: parsing and organizing documents, responding to reader feedback, managing event listings, or any other process that requires multiple, repeated tasks.</p><h3>Small Language Models Solve Specific Problems</h3><p>The composable workflows we’ve described have historically hit a wall when they involve tasks that require language understanding. Extracting key points from meeting notes, identifying patterns in reader feedback, and drafting routine summaries are steps that resist traditional automation. Until recently, they remained stubbornly human-only.</p><p>LLMs changed this equation, but their typical deployment <a href="https://generative-ai-newsroom.com/is-this-the-beginning-of-the-end-for-ai-summarized-news-196432290e9e">violates</a> <a href="https://generative-ai-newsroom.com/the-current-state-of-genai-copyright-lawsuits-203a1bd0f616">every</a> <a href="https://www.tandfonline.com/doi/full/10.1080/17512786.2024.2394558">principle</a> we’ve established. Cloud-based chatbots with unclear operations, vendor lock-in, and promises of doing everything are the antithesis of a tiny tool. This disconnect reflects a broader tension in AI development. “General Purpose AI” — a system designed to handle any task — is a central focus of Silicon Valley and features in AI regulation such as the <a href="https://artificialintelligenceact.eu/">EU AI Act</a>. But journalism doesn’t need a universal assistant; it needs precise, controllable tools that augment specific capabilities.</p><p>What might a more newsroom-friendly language model look like? It wouldn’t be a chatbot that tries to help with everything; it would just tackle one useful task, like summarizing documents (<strong>Clear verbs</strong>). It would have visible <a href="https://platform.openai.com/docs/guides/text#prompt-engineering">prompts</a> and <a href="https://www.promptingguide.ai/introduction/settings">parameters</a> (<strong>Transparent operations</strong>), and it would run on hardware that journalists control (<strong>Portability</strong>). It would also produce <a href="https://platform.openai.com/docs/guides/structured-outputs">reliable, structured outputs</a> that could plug into other tools (<strong>Composability</strong>).</p><p>These goals are all possible with today’s language models. Small, open LMs like OpenAI’s <a href="https://openai.com/index/introducing-gpt-oss/">gpt-oss</a>, Microsoft’s <a href="https://huggingface.co/microsoft/phi-4">Phi-4</a>, and Alibaba’s <a href="https://qwenlm.github.io/blog/qwen3/">Qwen3</a> can run on a laptop. They’re often released under permissive licenses. And they’re capable enough to be comprehensible and controllable, serving as tools that enhance journalists’ expertise rather than replacing it.</p><p>The key to using these models effectively is to treat them like any other tool in a workflow: Define the input (e.g., a document), the verb (e.g., extract key claims), and the output (e.g., structured JSON). Use techniques like few-shot prompting and constrained generation to make outputs predictable. Make prompts visible and editable. Let journalists verify and modify results. Consider whether an <a href="https://simonwillison.net/2024/Nov/6/yet-another-applied-llm-benchmark/">applied</a> <a href="https://generative-ai-newsroom.com/benchmarking-llm-performance-for-journalism-9831e6f918e4">benchmarking approach</a> or an evaluation tool like <a href="https://chainforge.ai/">ChainForge</a> might help you keep track of what’s working and what’s not.</p><p>This same approach could also strengthen more ambitious, agentic AI applications. Rather than relying on a single monolithic LLM to handle every task, agents could compose specialized tools, each optimized and validated for its specific purpose. This would make agent behavior more predictable, debuggable, and trustworthy.</p><p>To help newsrooms think through how they might want to use small language models, we built a set of choose-your-own-adventure scenarios that you can work through: project planning, exploratory data analysis, document searching, and app prototyping. Each one asks you to think through what kind of tool you might use for different tasks, and what tradeoffs are involved with your choices. You can try out these scenarios for yourself <a href="https://tiny-tools-zeta.vercel.app/">here</a>.</p><h3>Build Your Own Tiny Tools ecosystem</h3><p>Implementing the Tiny Tools framework doesn’t require a complete overhaul of your existing systems. Whether you’re an individual journalist or part of a larger newsroom, you can start small and build incrementally.</p><h4>For Individual Journalists</h4><p>The best place to begin is with a single workflow — ideally one that you find yourself repeating regularly. Maybe it’s cleaning up interview transcripts, organizing source documents, or formatting data for publication. Pick something concrete where the pain points are clear and the potential improvements are obvious.</p><p>Once you’ve identified your target workflow, use the framework to evaluate your current tools. Are they doing one thing well, or are you wrestling with bloated software that does too much? Look for opportunities to replace monolithic applications with focused alternatives.</p><h4>For Newsrooms</h4><p>At the organizational level, the first step is identifying high-value, repetitive workflows that affect multiple team members. The workflows that cause the most friction or consume the most time are often the best candidates for the Tiny Tools approach.</p><p>Identify a few target workflows and build tool menus for common tasks. This doesn’t mean dictating which tools everyone must use, but rather curating a set of vetted options that align with your newsroom’s values and technical capabilities. Include both traditional software and appropriate LLM tools, always with clear guidance on when each option is most suitable.</p><p>Documentation and training are crucial for adoption. Create clear guides that explain not just how to use individual tools, but how they fit into larger workflows. Include examples from real newsroom scenarios and emphasize the principles behind the tools — transparency, portability, and composability — so journalists can make informed decisions even as specific tools evolve.</p><p>Finally, consider sharing tools both within and between newsrooms. The same document cleaning script that helps your investigative team might benefit the data desk down the hall. And the report generation workflow you’ve perfected could help journalists at other publications facing similar challenges. This kind of collaboration strengthens the entire journalism ecosystem while embodying the open-source ethos that inspired the Tiny Tools framework.</p><p>The goal of this framework isn’t to become a software developer or to chase every new technical trend. It’s to thoughtfully adopt tools that enhance journalistic work while respecting the values and constraints of journalism itself. By focusing on specific needs, maintaining transparency, and building composable workflows, we can create technology that truly serves journalists rather than the other way around.</p><p><em>We’d love to hear from you. If you’re experimenting with your own Tiny Tools, or if there’s a workflow in your newsroom that feels like a candidate for this framework, reach out and share your experiences.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e2176dd66cbc" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/tiny-tools-a-framework-for-human-centered-technology-in-journalism-e2176dd66cbc">Tiny Tools: A Framework for Human-Centered Technology in Journalism</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ The News Industry’s GenAI Cautionary Tales ]]></title>
<link>https://generative-ai-newsroom.com/the-news-industrys-genai-cautionary-tales-84387d1ca087?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/84387d1ca087</guid>
<category><![CDATA[ journalism-industry ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Wed, 17 Sep 2025 10:01:50 GMT</pubDate>
<atom:updated>2025-09-17T10:01:50.078Z</atom_updated>
<content:encoded><![CDATA[ <figure><img alt="Broken egg" src="https://cdn-images-1.medium.com/max/1024/1*CekNwz6b0DLF4oEHx0QURw.png" /></figure><h4>Generative AI failures have shown, among other things, the value of scrutinizing outsourced work.</h4><p>When I attend AI training sessions these days I often come across what I call “the slide of shame” — the collection of mistakes that news organizations have made when using generative AI. In this article I collect some of the various failures that have been made public about news organizations’ use of generative AI. Whether lacking transparency and labeling, introducing inaccuracies, or managing submissions from freelancers that didn’t have adequate editorial oversight, we can all learn from these mistakes and try to avoid similar pitfalls in the future.</p><h4><strong>The Fake Reporters</strong></h4><p>In June 2025, the main news channel of the Flemish national broadcasting company <a href="http://vrt.be/vrtnws/nl/2025/06/26/modeblad-elle-gebruikt-nepjournalisten-voor-online-artikels/">VRT revealed</a> that a large proportion of articles in some Belgian glossy magazines had fake bylines. VRT found that 44 of the 46 articles published by Belgium’s Psychologies magazine had the fake persona Femke, a psychologist, on the byline. VRT found that, over a three month period, half the articles published in Belgium’s version of Elle’s website had the bylines Marta Peeters and Sophie Vermeulen. But these were not real people. VRT even found the picture for Sophie on the site “<a href="https://this-person-does-not-exist.com/en">This Person Does Not Exist</a>” which makes AI generated faces. VRT looked at the Belgian versions of Elle, Marie Claire, Forbes, and Psychologies magazine which are all owned by Ventures Media. The company told VRT NWS in an interview that the articles were part of a “test” and acknowledged making a mistake in presenting Femke as a psychologist. They said they have changed their approach to clarify when content is AI-generated.</p><p>Unfortunately, it appears these magazines hadn’t learned from the mistakes of Sports Illustrated who were <a href="https://futurism.com/sports-illustrated-ai-generated-writers">caught by Futurism</a> doing the same thing in November 2023.</p><h4><strong>The Fake Freelancer</strong></h4><p>In May 2025, Wired Magazine published an article by a freelancer going by the name Margaux Blanchard entitled <em>“They Fell in Love Playing Minecraft. Then the Game Became Their Wedding Venue.”</em></p><p>The suspicion of the team at Wired was aroused when Margaux would only accept payment by PayPal or Check. “A closer look at the details of the story, though, along with further correspondence from the writer, made it clear to us that the story had been an AI fabrication,” <a href="https://www.wired.com/story/how-wired-got-rolled-by-an-ai-freelancer/">Wired published in an explanation.</a> They <a href="https://www.wired.com/story/they-fell-in-love-playing-minecraft-then-the-game-became-their-wedding-venue/">removed the article</a> that same month.</p><p>Business Insider, Cone Magazine, and Naked Politics also removed articles written by Margaux, <a href="https://pressgazette.co.uk/publishers/digital-journalism/wired-and-business-insider-remove-ai-written-freelance-articles/">reports Press Gazette</a>.</p><p><a href="https://www.wired.com/story/how-wired-got-rolled-by-an-ai-freelancer/">Wired cautioned</a>: “This story did not go through a proper fact-check process or get a top edit from a more senior editor. First-time contributors to WIRED should generally get both, and editors should always have full confidence that writers are who they say they are… Fabulists and plagiarists are as old as media itself. But AI presents a new challenge. It lets anyone craft a perfect pitch with a simple prompt and play-act the role of journalist convincingly enough to fool, well, us. We acted quickly once we discovered the ruse, and we’ve taken steps to ensure this doesn’t happen again. In this new era, every newsroom should be prepared to do the same.”</p><h4><strong>Fabricated Sources</strong></h4><p>In May 2025, a list entitled “Summer Reading List for 2025” was published in the Heat Index supplement of the Chicago Sun-Times and The Philadelphia Inquirer. A book reviewer who calls herself Tina Books <a href="https://www.instagram.com/reel/DJ46n9dScyz/?igsh=MXJ5cm9hOTRoNWlmdA==">posted on her Instagram</a> that the books didn’t actually exist.</p><p><a href="https://www.npr.org/2025/05/20/nx-s1-5405022/fake-summer-reading-list-ai">NPR counted</a> a total of five of the 15 books in the reading list were real. The freelance journalist who wrote the article, Marco Buscaglia, <a href="https://www.theatlantic.com/technology/archive/2025/05/ai-written-newspaper-chicago-sun-times/682861/">told The Atlantic</a> that he had used ChatGPT to research it but clearly didn’t check the output. He said that he submitted it to a division of Hearst called King Features who produced the supplement. He didn’t tell them he had used ChatGPT.</p><p><a href="https://chicago.suntimes.com/press-room/2025/05/20/chicago-sun-times-response-to-may-18-special-section">The Chicago Sun-Times responded by sa</a>ying “This should be a learning moment for all of journalism”. Melissa Bell, the CEO of Chicago Public Media, which Chicago Sun-Times is a part of <a href="https://chicago.suntimes.com/opinion/2025/05/29/lessons-apology-from-sun-times-ceo-ai-generated-book-list">apologized and </a>said that both Marco and King Features should have checked their work. But she also suggested a lesson she has learned is that Chicago Sun-Times should have labelled who exactly had done the work. “Our circulation department did not submit the pages for review to anyone on our editorial team, nor acknowledge in print that the content was created by a third party. Instead, the department packaged it under a Sun-Times banner and sent it to homes across the region. Going forward, we’ve changed our editorial policy to ensure that any third-party licensed content 1) clearly states where it comes from, 2) is not presented as if it were created by our newsrooms, and 3) is reviewed by our new Standards team with editors from our newsrooms.”</p><h4><strong>The AI-generated Opinions</strong></h4><p>In March 2025, The LA Times’ owner, <a href="https://www.latimes.com/california/story/2025-03-03/a-letter-to-readers">Patrick Soon-Shiong, announced</a> that the paper would start publishing AI-generated points of view at the end of some articles, in a project called <a href="https://www.latimes.com/insights/">L.A. Times Insights</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*MgGnpSJ2aIhZ_p9bBKDJTQ.png" /></figure><p>The next day, under <a href="https://www.latimes.com/california/story/2025-02-25/anaheim-ku-klux-klan-council-recall-1925">an opinion piece about the 100th anniversary</a> of members of the Ku Klux Klan being recalled from a local council, the paper added one such AI-generated point of view which said: “Local historical accounts occasionally frame the 1920s Klan as a product of ‘white Protestant culture’ responding to societal changes rather than an explicitly hate-driven movement, minimizing its ideological threat.”</p><p>New York Times reporter Ryan Mac <a href="https://bsky.app/profile/rmac.bsky.social/post/3ljjqjkgfs22f">drew attention</a> to the point of view and <a href="https://edition.cnn.com/2025/03/05/media/la-times-ai-kkk-comments">multiple </a>newspapers <a href="https://www.forbes.com/sites/douglaslaney/2025/03/06/la-times-ai-bot-coddles-the-klan/">accused</a> the L.A. Times of sympathizing with the KKK. The L.A. Times appears to have deleted the comment.</p><p>The writer of the column, <a href="https://www.latimes.com/california/story/2025-03-07/la-times-insights-ai-controversy">Gustavo Arellano argued against accusations</a> that the L.A. Times defended the KKK but did complain about the phrasing and that elsewhere it cited his work out of context. “The AI tool may have been guilty of fuzzy and clumsy phrasing, but it did not defend or sympathize with the KKK… this was a case of journalists not reading past the first clause of a sentence… that citation was correct. I was actually shocked AI got such a crucial point right. But I was also annoyed that the two other bullet points — including one that linked to one of <a href="https://capitalandmain.com/orange-county-parents-change-name-school-honors-klan-member-0219">my columns in 2018 </a>about the Klan in O.C. — were wildly out of context, but no one else seemed to care.”</p><p>As of September 15 2025, the Insights project still exists. The L.A. Times labels it with a disclaimer that staff do not create or edit the content.</p><h4><strong>Inaccuracies in Summaries</strong></h4><p>In <a href="https://www.bloomberg.com/company/press/bloomberg-launches-gen-ai-summarization-for-news-content/?utm_source=boost&amp;utm_campaign=boost_2025_ao&amp;utm_content=post_boost-advocacy%26tactic%3D835937">January 2025 Bloomberg announced</a> that it would publish “AI-Powered” News Summaries on Bloomberg Terminals. Just over three months after the launch, on March 29, <a href="https://www.nytimes.com/2025/03/29/business/media/bloomberg-ai-summaries.html">the NY Times reported</a>: “The news outlet has had to correct at least three dozen A.I.-generated summaries of articles published this year… errors have included incorrect figures, incorrect attribution and references to the wrong U.S. presidential election”.</p><p>Whether humans check those summaries is still a mystery: Bloomberg’s Global Head of Core Product Communications Rob Madden would not answer that question when I asked him about it back in January. Bloomberg’s public response has been sparse, other than this statement from the Times’ article: “Bloomberg News said in a statement that it publishes thousands of articles each day, and ‘currently 99 percent of A.I. summaries meet our editorial standards.’”</p><figure><img alt="Chess piece falling" src="https://cdn-images-1.medium.com/max/1024/1*5ShS--1xDK8j6DnNRB7_ww.png" /><figcaption>AI fails have brought into the spotlight the news industry’s complicated power dynamic with Big Tech</figcaption></figure><h4><strong>More Mistakes in Summaries</strong></h4><p>In late 2024 Apple released a feature that summarized your notifications, including notifications from news apps. But these were error ridden. One notification, with the BBC logo next to it, said that the darts player Luke Littler had won the PDC (the Professional Darts Corporation) world championship — only he hadn’t actually played yet.</p><p>Other errors included:</p><ul><li>A summary that said The Israeli Prime Minister Benjamin Netanyahu had been arrested. He hadn’t.</li><li>A summary that said Luigi Mangione — The man charged with the murder of the UnitedHealthcare boss — had shot himself outside court. He hadn’t.</li><li>A summary saying that tennis player Rafael Nadal came out as gay. He hadn’t.</li></ul><p>News organizations including the BBC and Sky news publicly complained. In January, Apple announced they were temporarily withdrawing the feature.</p><p><a href="https://www.cnet.com/tech/services-and-software/ios-26-beta-brings-ai-summaries-back-to-news-apps-with-a-warning/https://www.cnet.com/tech/services-and-software/ios-26-beta-brings-ai-summaries-back-to-news-apps-with-a-warning/">CNET reports</a> that this feature was reintroduced in August 2025 with some modifications. Now the user has to select that they want news notifications summarized. And these summaries now come with a warning to the user: “Summarization may change the meaning of the original headline. Verify information.”</p><p>Here we again learned that we should be cautious of AI summarization because generative AI adds in errors. But this also seems to show an example where news organizations complaining of the failure may have had some influence over big tech companies, at least in temporarily getting them to pull the feature.</p><h4><strong>Liquid Content on Sensitive Topics</strong></h4><p>In October 2023, Microsoft’s news aggregation service Microsoft Start published an automated poll next to a Guardian story about the death of a 21-year-old water polo coach. The poll is no longer online but the <a href="https://www.theguardian.com/media/2023/oct/31/microsoft-accused-of-damaging-guardians-reputation-with-ai-generated-poll">Guardian reports</a> that it asked: “What do you think is the reason behind the woman’s death?” Readers were then asked to choose from three options: murder, accident or suicide.”</p><p>The Guardian reported that their chief executive Anna Bateson complained directly to Microsoft’s president, Brad Smith, writing in a letter that the poll had caused “significant reputational damage”. “This is clearly an inappropriate use of genAI [generative AI] by Microsoft,” she wrote. Bateson added that it had demonstrated “the important role that a strong copyright framework plays in enabling publishers to be able to negotiate the terms on which our journalism is used”.</p><p>The Guardian reported that a Microsoft spokesperson said: “We have deactivated Microsoft-generated polls for all news articles and we are investigating the cause of the inappropriate content. A poll should not have appeared alongside an article of this nature, and we are taking steps to help prevent this kind of error from reoccurring in the future.”</p><p>I have not found a publication of the findings of Microsoft’s investigation. It appears that, as of September 15 2025, Microsoft still has a license to use the Guardian’s content but it isn’t clear if they renegotiated the terms.</p><h4><strong>The Gibberish Obituary</strong></h4><p>In September 2023, MSN published an obituary of NBA star Brandon Hunter with the headline “Brandon Hunter useless at 42”, <a href="https://futurism.com/msn-ai-brandon-hunter-useless">reports Futurism</a>.</p><p>Journalist Victor Tangermann writes in his Futurism article that the rest of the obituary is “even more incomprehensible, informing readers that Hunter ‘handed away’ after achieving ‘vital success as a ahead [sic] for the Bobcats’ and ‘performed in 67 video games.’”</p><p>Tangermann suggests that the problem here is in Microsoft not checking out the publications that it is aggregating. He says that the original article was from a site called Race Track. “​​Red flags abound, starting with the fact that its articles are bylined simply by an anonymous ‘Editor.’… Most obviously, a quick perusal of Race Track’s profile shows that it has been using MSN to publish an uninterrupted stream of incoherent gobbledygook… none of this bodes well for the information ecosystem.” Even more, he warned: “Accusing an NBA legend of being ‘useless’ the week he died isn’t just an offensive slip-up by a seemingly unsupervised algorithm, in other words. It’s also a threat looming over the future of journalism.”</p><p>MSN deleted their obituary and Race Track no longer exists. At the same time, evidence is circumstantial here and we don’t know with absolute certainty if or how AI was used to write this obituary.</p><h4><strong>The Inaccurate Finance Advice</strong></h4><p>In November 2022, CNET published personal finance articles produced with the help of an “internally designed AI engine”. The articles were riddled with errors. For example, <a href="https://futurism.com/cnet-ai-errors">Futurist pointed out</a> that an article that was meant to be breaking down how compound interest works actually got the sums disastrously wrong in a “dumb error”. The Verge <a href="https://www.theverge.com/2023/1/25/23571082/cnet-ai-written-stories-errors-corrections-red-ventures">reported</a> that more than half of the AI-generated stories contained factual errors. <a href="https://www.wired.com/story/cnet-published-ai-generated-stories-then-its-staff-pushed-back/">Wired counted</a> that CNET corrected 41 out of 77 bot-written articles.</p><p>Then-editor-in-chief Connie Guglielmo <a href="https://www.cnet.com/tech/cnet-is-testing-an-ai-engine-heres-what-weve-learned-mistakes-and-all/">wrote</a> about the process: “Editors generated the outlines for the stories first, then expanded, added to and edited the AI drafts before publishing. After one of the AI-assisted stories was cited, rightly, for factual errors, the CNET Money editorial team did a full audit.”</p><p>She said that she learned that the AI tool makes mistakes, transposes names, or uses vague language. She also said she would make the bylines clearer next time so you don’t have to hover over a byline to see that an AI engine contributed. Finally she said that they found their plagiarism checker didn’t work and were working on improving citations.</p><p>This all feels like old advice — that’s because it is — more than two and a half years in human years which feels like a lifetime in AI years. CNET were at the forefront, testing generative AI before others. One of their editors suggested that being a pioneer can bring problems. Tom Clarke, the science and technology editor at CNET <a href="https://www.bbc.co.uk/programmes/m001pmlz">said on the BBC Media Show</a> that this was a first test of this “world changing technology and we haven’t really grasped the consequences of what deploying that technology in full means. And unfortunately for us at CNET we did deploy it without really thinking about what it could mean or what it could do for some of our credibility and it was a really harsh lesson we had to learn.”</p><p>He questioned whether a news organization should be working in this way: “One of the things that I am personally quite worried and concerned about in the AI world is that it is the Silicon Valley mentality of move fast and break things, see what happens… We need to slow down with a lot of this stuff being pushed out.”</p><h3><strong>Final Thoughts</strong></h3><p>There are some themes we can draw out of all of these cases. It matters that news organisations and news aggregators trust those they are buying their content from, check their work, and credit who (or what) has created that work. This was also true before generative AI came on the scene. But now liars can lie more efficiently so editors need to be even more on their guard. These are the public failures. We should all be grateful to the organizations that have owned their mistakes and shared what went wrong. It allows us to learn and hopefully avoid the same pitfalls. So from now on, when the subject comes up in training, I am not going to think of it as the “slide of shame” but rather the slide of “lessons learned”. What lessons have <em>you</em> learned about generative AI in news production? Email them to me at clare.spencer@northwestern.edu.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=84387d1ca087" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/the-news-industrys-genai-cautionary-tales-84387d1ca087">The News Industry’s GenAI Cautionary Tales</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Administrative Boon, Editorial Question: Generative AI’s Role in Small Newsrooms ]]></title>
<link>https://generative-ai-newsroom.com/administrative-boon-editorial-question-generative-ais-role-in-small-newsrooms-62e376764685?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/62e376764685</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ evaluation ]]></category>
<dc:creator><![CDATA[ Stuart Duncan ]]></dc:creator>
<pubDate>Wed, 10 Sep 2025 06:01:43 GMT</pubDate>
<atom:updated>2025-09-10T06:01:42.514Z</atom_updated>
<content:encoded><![CDATA[ <h4>Forget robot reporters. A study of 20 small newsrooms reveals that the real value of generative AI isn’t in writing stories, but in handling the administrative tasks that keep their operations afloat.</h4><p>The use of AI in journalism has been significant, but often the implementation of the technology within newsrooms has been limited to larger, well-resourced news organizations. Unlike outlets like the <a href="https://www.nytimes.com/2024/10/07/reader-center/how-new-york-times-uses-ai-journalism.html">New York Times</a>, <a href="https://www.bbc.co.uk/mediacentre/2025/articles/bbc-to-launch-new-generative-ai-pilots-to-support-news-production/">the BBC</a>, or <a href="https://time.com/7201556/generative-ai-time-journalism/">TIME magazine,</a> a mix of financial, labor and technical barriers has often limited small news publications’ ability to implement the technology.</p><p>Given this, my research has examined how the emergence and popularization of generative AI systems are reshaping how journalism is produced within small news organizations. My work explores the use of AI in journalism through the creation of a generative AI chatbot prototype, which answers audience questions on a specific topic.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/865/1*rQ1VnlSeuWzhxJHOaHP1pQ.jpeg" /><figcaption><em>Example of a Conversation from the Prototype Chatbot built.</em></figcaption></figure><p>This prototype served as a research tool to spur conversations during interviews with small newsroom staff from Canada and the U.S., providing a working blueprint for small newsrooms that might be interested in implementing AI chatbots within their own organizations. Study participants were invited to use the chatbot prototype during these interviews, and feedback gathered from this use informed redevelopment of an <a href="https://github.com/stuartduncan416/ARRRBot">open-source version of this prototype</a> while revealing insight into small newsroom use of generative AI.</p><p>Defining a “small newsroom” is challenging because the term lacks a universal definition and can be categorized in many ways, including by delivery method, geographical scope, ideological focus, or business model. Though many small news organizations face limited resources compared to their larger counterparts, using this as a universal classification is problematic since a smaller size doesn’t inherently preclude healthy funding. For simplicity’s sake, for my research, “small” newsrooms were considered to be any newsroom with fewer than 20 full-time employees. The majority of the 20 participants in my study worked within digitally native news organizations, with less representation from radio, newspapers, and magazines. The geographical scope of the news organizations varied and included outlets with a local, regional, national, and international focus.</p><p>To power the chatbot’s conversational elements I tapped into OpenAI’s models via their API. I used a retrieval augmented generation (RAG) approach to help ensure questions were answered from a defined knowledge base and to minimize response hallucinations. The text repository used to ground responses to user questions was gathered from <a href="https://theconversation.com/topics/opioids-1046">articles about opioids published by The Conversation</a>.</p><p>One advantage of using models via an API is that small news organizations don’t have to invest in the often costly computing and developer resources required to host and maintain their own LLM on a server. Costs can be kept reasonably low for running the bot — about $30 USD would be enough to respond to 100,000 user queries.</p><h3><strong>Lessons from Small News Organizations</strong></h3><p>During my interviews, use of the chatbot prototype revealed concerns about AI broadly and generative AI systems specifically within the context of small news organizations. Among the people I interviewed, most were active users of generative AI, with its application most commonly focused on supporting administrative tasks.</p><p>In these small news organizations, my interviewees were often fulfilling multiple roles, including positions that straddled both administration and editorial work. Generative AI systems, such as ChatGPT, offered administrative efficiency. As one publisher at a digital hyperlocal publication said, “I think AI has a huge potential to be a boon for small independent outlets that are under-resourced. It’s been hugely beneficial to my business and bottom line, and it’s allowed me to do a lot more with a lot fewer resources.”</p><p>Small news organization staff described that they were using generative AI systems to answer emails, develop marketing campaigns, and create grant applications. Use for editorial purposes was limited to support tasks such as transcription, translation, copyediting, and SEO-friendly headline brainstorming.</p><p>Use of AI systems was moderated by a significant concern over the impact the use of the technology would have on the unique relationship that small news organizations have developed with their audiences. Several participants explained that their direct human connection with their audiences was one of their biggest assets and protecting this relationship was a guiding principle of technology use.</p><p>As such, the use of generative AI systems in audience-facing ways was limited. News staffers were concerned that employing such systems would alter adherence to journalistic norms such as accuracy and transparency. These concerns were also revealed through their use of the generative AI chatbot prototype created as part of this project.</p><p>Effective AI implementation often relies on robust data sources, and when using the prototype, access to enough data was a concern of several study participants. Small news organizations may simply not have enough content to build a chatbot that can accurately answer user questions about specific topics. As one publisher of a local politics website stated, “The reality is that the amount of information that a large language model needs to be able to get to a level of accuracy, we just don’t have that level of information in a local setting.”</p><p>This potentially limited knowledge base also led to concerns about the timeliness of the information provided by generative AI systems. Without the publishing volume of larger news organizations, small publications expressed worry about whether custom generative AI systems could accurately share information with audiences, as an editor at an American digital news outlet specified, “Making the knowledge base off of what we’ve already written in the past, circumstances could change and I don’t know if that accounts for the nuances of the moment.”</p><p>For the publications that had extensive archives of content, limited staffing resources could make it challenging to do the data preparation required to build effective LLMs products. “My concern is, how do we implement it? What workload is involved in that?”, noted a staff member at a radio station.</p><p>When reflecting on the prototype, several small newsroom staff described that to maintain trust with their audience, use of such systems would need to be transparent. In particular, the prototype’s listing of the source links of the articles used to build the response to audience queries was viewed as a valued feature.</p><p>Despite these concerns most study participants believed the prototype could offer value to their audience, often citing frustration with traditional methods of searching for content on news sites as a key reason. One participant summarized it: “The biggest problem with information these days is the onslaught of it, like the sheer amount of information, which is what causes people to turn away from it. This makes it a lot more digestible.”</p><h3><strong>Take Aways</strong></h3><p>As this work reveals, small news organizations are broadly open to generative AI technologies, as these tools offer efficiencies and opportunities to engage with audiences that would not otherwise be possible given resource constraints unique to small news organizations.</p><p>While the use of AI in accordance with journalistic norms is not exclusive to small news organizations, the influence of transparency and accuracy on the distinctive relationships these organizations maintain with their audiences is significant.</p><p>My interviews revealed that small news organizations are most often using generative AI as an administrative support tool, rather than a journalism creation tool. Given this usage pattern, future AI tool development efforts may be more impactful if they focus on enhancing back-end administrative workflows rather than content generation.</p><p>With its simplified implementation approach and focus on affordable adoption, positive reception to the prototype reveals that there is nonetheless an appetite among small news organizations to develop custom generative AI systems. Chatbot features such as transparent sourcing and a retrieval-augmented generation approach that minimizes hallucinations, support desired journalistic norms of transparency and accuracy.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=62e376764685" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/administrative-boon-editorial-question-generative-ais-role-in-small-newsrooms-62e376764685">Administrative Boon, Editorial Question: Generative AI’s Role in Small Newsrooms</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ How GenAI Tools are Helping Journalists Monitor Public Meetings ]]></title>
<link>https://generative-ai-newsroom.com/how-genai-tools-are-helping-journalists-monitor-public-meetings-17c2752ac81b?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/17c2752ac81b</guid>
<category><![CDATA[ generative-ai-tools ]]></category><category><![CDATA[ story-discovery ]]></category><category><![CDATA[ civic-journalism ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Thu, 04 Sep 2025 14:01:38 GMT</pubDate>
<atom:updated>2025-09-04T14:01:38.191Z</atom_updated>
<content:encoded><![CDATA[ <h4>From a story about a school board selling off a multi-million dollar plot of land to a suburb running out of water, journalists are using GenAI to help with stories covering public meetings.</h4><p>Journalists are increasingly using tools which leverage generative AI to help monitor public meetings. These tools have enabled news stories ranging from a school board selling off a multi-million dollar plot of land to a suburb running out of water.</p><p>Some unusually dramatic words got Suzie Glassman’s attention in one of the tool’s alerts. “A board member said something like ‘this process has been a disaster’ or ‘this long nightmare is over’”, she told me. “I might have glossed over it if I had just been watching the YouTube video, but when I was reading through the transcript I was like, ‘oh, that’s really interesting’…So I watched the full 20 minutes. And then I thought, okay, that’s a story.” Suzie, a senior education reporter for the Colorado Trust for Local News, went on to write <a href="https://coloradocommunitymedia.com/2025/07/09/adams-12-approves-land-sale/">a story for Colorado Community Media</a> about a public school district selling off a big plot of land for $9M after a decade of delays.</p><figure><img alt="Suzie Glassman’s story, with the headline “Adams 12 approves $9M sale of surplus land near Eastlake Station" src="https://cdn-images-1.medium.com/max/1024/1*YDuDJkdVtEOKY1f6t88cjQ.png" /></figure><p>Suzie was alerted to the school board meeting via email from <a href="https://seegov.org/about">SeeGov</a> which shares video highlights of local government meetings. Alex Rosen, the executive director of SeeGov, set up Suzie’s service to send her emails of all the board meetings for five different school boards, which works out to around 10 emails per month. “If I had missed a board meeting last night I just get an email with a link to the recording. It ensures that I never miss one,” she explained.</p><p>She told me she wouldn’t have found this story without the email from SeeGov. “It was only a 20-minute discussion. Because I cover five different school districts I just don’t think it would have caught my attention.” She also finds that it is easier to skip to the relevant part on SeeGov than it used to be on a YouTube video. “Usually school board meetings can be anywhere from an hour to six or seven hours… it [SeeGov] clips it according to the agenda item. So I can skip over every other part of the meeting that I’m not interested in and then just skip to that specific discussion. Then I can either just watch that section — and maybe they talked about it for 45 minutes so I can watch the 45 minutes — or I can scroll through the transcript and understand what happened. I will specifically pull out quotes that I can then use in the article. That has been what’s most helpful.”</p><p>Alex described a perfect storm paving the way for SeeGov. “Because of the pandemic, a lot more governments started putting their meetings online. So we’re taking advantage of that and the AI capabilities and lowering costs to make all of this feasible,” he told me.</p><h4><strong>Build it in-house</strong></h4><p>SeeGov is one of a growing number of off-the-shelf generative AI tools journalists could use to help report on public meetings. Some of the others include <a href="https://myhamlet.com/home/home">Hamlet</a>, <a href="https://www.satchelai.com/">Satchel</a>, <a href="https://www.snapstream.com/">Snap Stream</a>, <a href="https://council-search.bellingcat.com/">Council Watch</a>, <a href="https://thelocallens.org/">LocalLens</a>, <a href="https://citizenportal.ai/">Citizen Portal</a>, <a href="https://civicsunlight.ai/">Civic Sunlight</a>, <a href="https://agendawatch.org/">Agenda Watch</a> and <a href="https://calmatters.digitaldemocracy.org/">Digital Democracy</a>. The off-the-shelf versions present a challenge: they might not cover your beat. This doesn’t have to stop your newsroom from using generative AI to help monitor public meetings though. Multiple newsrooms, from <a href="https://www.sujo.no/prosjekt/utvikler-nyskapende-ki-verktoy-for-lokalpolitikk">Norway</a> to the <a href="https://www.cambsnews.co.uk/news/cambsnews-and-the-rise-of-civic-ai-extracting-wisdom-from-the-noise/38568/">UK</a>, have built their own tool in-house.</p><p>In the U.S. Hearst has built their own tool called Assembly. Sam González Kelly, a reporter for the Hearst-owned Houston Chronicle, used Assembly when he couldn’t get into a public meeting. “I tried to go to a City Council meeting and the chambers were full by the time I got there. So the next morning I checked Assembly, checked the transcript and then did a ‘control F’ search for some of the terms I was looking for and found them in the transcript. It was helpful that it was time-stamped and so I went back to the video and watched the section that I was interested in to make sure that it was accurate in the transcript, and it was.”</p><p>A typical user workflow might involve the reporter first reviewing the meeting information Assembly sends to Slack:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/965/1*zAaCl2CiqgSKVRlbTGxSYw.png" /></figure><p>The user can then click the “summarize” button and Assembly will provide a summary which shows up in the comments of the initial Slack post:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/953/1*jW-jocz4z009Y9ZW2R-TQQ.png" /></figure><p>Assembly also provides a full meeting transcription in Google Sheets:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/522/1*C-ZcOiTQH8RDkoFlK3IKtw.png" /></figure><p>Assembly includes timestamped links back to the webpage which contains the video from the meeting in question, so the reporter can go back and confirm the transcript is accurate:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*unS78uW3MDVV-n6Xy1Whvw.png" /></figure><p>In his particular case, <a href="https://www.houstonchronicle.com/news/houston-texas/housing/article/houston-suburbs-water-population-boom-20807698.php">Sam reported that the council didn’t extend a moratorium</a> on building houses that they brought in after a feared water shortage.</p><figure><img alt="Houston Chronicle story with the headline “The population boom in Houston suburbs is straining water systems.”" src="https://cdn-images-1.medium.com/max/1024/1*ouP856hhcnry7licOr4ceA.png" /></figure><p>“Where assembly was helpful is that I knew that they didn’t bring it up again later on in the meeting,” Sam told me.</p><p>The Vice President of Editorial Innovations and AI Strategy at Hearst, Tim O’Rourke, pointed me towards numerous stories he said Assembly has helped with, including <a href="https://www.sfchronicle.com/tahoe/article/incline-village-beach-20382588.php">a debate over allowing employees beach access at a resort</a>, <a href="https://www.sfchronicle.com/outdoors/article/yosemite-summer-reservations-entrance-20814910.php">the busiest time to visit Yosemite National Park</a> (between 9–10am on a Saturday at the south entrance, in case you’re wondering), <a href="https://www.sfchronicle.com/bayarea/article/mountain-bikes-park-20809609.php">anger over a proposed mountain bike trail</a>, and <a href="https://www.mysanantonio.com/business/article/goodwill-thrift-store-boerne-20760767.php">an accusation that the thrift store Goodwill did not do enough for a Texan community during a flood</a>.</p><p>Both Suzie and Sam used their public meeting monitoring tool to help them do their job rather than do their job for them. This is a shift from some previous efforts, which have raised concerns. For instance, i<a href="https://medium.com/centerforcooperativemedia/ai-generated-news-has-arrived-in-new-jersey-6971f9949c47">n 2023, Joe Amditis</a>, the then associate director of products + events at the Center for Cooperative Media at Montclair State University, was worried about relying on the AI tool LocalLens, given the possibility of inaccuracies. Joe asked at the time: “Do the founders of LocalLens see themselves as something akin to a bot-driven version of <a href="https://www.documenters.org/">Documenters</a> that supplies much-needed notetaking and documentation of public meetings and records?”. Two years on and the people involved in the Documenters project have expressed concerns themselves after NiemanLab reporter <a href="https://www.niemanlab.org/2025/03/local-newsrooms-are-using-ai-to-listen-in-on-public-meetings/">Andrew Deck highlighted </a>Chalkbeat’s use of LocalLens and Midcoast Villager’s use of Civic Sunlight. “… the growing reliance on automation also raises critical questions about trust, accuracy, and the implications for community engagement,” <a href="https://www.citybureau.org/notebook/2025/6/24/ai-is-increasingly-shaping-local-journalism-how-do-we-keep-people-at-the-center#:~:text=We&#39;ve%20been%20reflecting%20on,t%20attend%20in%20real%20time.">wrote City Bureau’s Director of Product Eve Lacivita</a>.</p><p>Being physically in the same room is important for Eve. “At City Bureau, we believe democracy isn’t just about information; it’s about participation. We are committed to keeping people present in the spaces where decisions affecting their lives and communities are made.” Arguably, these tools don’t <em>necessarily</em> put an end to journalists physically attending council meetings, though they do create the possibility that journalists could cover more events without attending.</p><p>Something Alex said — about it being possible to make SeeGov thanks to lockdown forcing public meetings to go online — reminded me of an amusing moment where leaders were held to account in the UK. During lockdown, councillors of a tiny parish council resigned after their angry rants went viral. The clerk of the meeting was praised for her stoicism and became a kind of celebrity. Because of the lockdown, the meeting took place on Zoom and was uploaded to YouTube. But here’s the thing — back then, <a href="https://www.bbc.com/news/newsbeat-55955288">according to the BBC</a>, the meeting only came to light because a 17-year-old, with a peculiar interest in watching local council meetings, found it and sent it to his friend who <a href="https://x.com/janinemas0n/status/1357371421442396162?ref_src=twsrc%5Etfw">shared it on Twitter</a>. So one can argue that, while we shouldn’t rely on AI tools, with the generative AI capabilities we have now, we also don’t need to rely on a couple of 17-year-old local politics nerds to find newsworthy information from council meetings. AI can help.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=17c2752ac81b" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/how-genai-tools-are-helping-journalists-monitor-public-meetings-17c2752ac81b">How GenAI Tools are Helping Journalists Monitor Public Meetings</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ AI Search, Users, and News: Insights from LM Arena Data ]]></title>
<link>https://generative-ai-newsroom.com/ai-search-users-and-news-insights-from-lm-arena-data-bd1d1ac96712?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/bd1d1ac96712</guid>
<category><![CDATA[ google-ai-overview ]]></category><category><![CDATA[ news ]]></category><category><![CDATA[ search-engines ]]></category><category><![CDATA[ audience ]]></category>
<dc:creator><![CDATA[ Nick Diakopoulos ]]></dc:creator>
<pubDate>Thu, 28 Aug 2025 14:00:06 GMT</pubDate>
<atom:updated>2025-08-28T14:00:06.259Z</atom_updated>
<content:encoded><![CDATA[ <h4>A trove of data from LM Arena offers a glimpse into user search behavior.</h4><p>AI search has led to a lot of hand-wringing in the news industry, with worries over <a href="https://digiday.com/marketing/wtf-is-google-zero">“Google Zero”</a> — a future world where Google no longer operates as a reliable referrer of traffic to news sites. Our <a href="https://generative-ai-newsroom.com/ai-overviews-chatbots-and-news-traffic-the-story-so-far-c010b3bf53cb">previous analysis</a> showed that Google’s AI Overviews weren’t having a measurable impact on traffic referrals in late 2024, and <a href="https://pressgazette.co.uk/media-audience-and-business-data/media_metrics/publisher-traffic-sources-2019-2025/">recent data from Chartbeat</a> supports that traffic referrals from Google have been pretty steady through 2025 too. Yet, other <a href="https://www.theguardian.com/technology/2025/jul/24/ai-summaries-causing-devastating-drop-in-online-news-audiences-study-finds">reports</a> indicate that AI Overviews can siphon off traffic and <a href="https://www.pewresearch.org/short-reads/2025/07/22/google-users-are-less-likely-to-click-on-links-when-an-ai-summary-appears-in-the-results/">reduce click throughs</a>, with an <a href="https://www.economist.com/business/2025/07/14/ai-is-killing-the-web-can-anything-save-it">uneven impact across different categories</a> of online content.</p><p>In trying to figure out how AI search is going to impact the news media business, one thing seems clear: <em>we need to understand more about user behavior around AI search</em>. People’s attention patterns will play a key role in the eventual impact of these new interfaces on media organizations.</p><p>Enter <a href="https://lmarena.ai/">LM Arena</a> — a site where AI models go head-to-head and users vote on which AI response they prefer. Once all those user preferences are aggregated together it provides a nice benchmark of model performance, even if that performance is biased towards people-pleasing user satisfaction. Beyond testing the basic text models from all the main players, LM Arena also runs specific competitions for different model capabilities, including for image, video, and code generation, as well as for search-enabled models.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*QvrbqADi7S3eQTm4" /><figcaption>The interface for LM Search Arena.</figcaption></figure><p>In June of this year, LM Arena released a trove of data from their users of LM Search Arena <a href="https://arxiv.org/pdf/2506.05334">together with some initial analysis</a> by Miroyan and colleagues at UC Berkeley that starts to shed new light on how people interact with search-enabled chatbots. Using that open source data, <a href="https://www.kaichengyang.me/">Kaicheng Yang</a> from SUNY Binghamton <a href="https://arxiv.org/pdf/2507.05301">further analyzed it</a> with specific attention to how news sources are cited.</p><p>The data release includes more than 24,000 user interactions with search-enabled LLMs from OpenAI, Perplexity, and Google from more than 11,000 users in 136 countries. This includes about 12,000 user preference votes. The data was collected fairly recently, between March 18 and May 8, 2025. Though it’s important to caveat that the users of LM Arena are not an unbiased sample of the web.</p><p>Miroyan’s analysis of the full dataset has some interesting observations for news organizations. For one, they find that user preferences are <em>positively influenced by the number of cited sources</em>, even when the cited content does not directly support the attributed claims. This suggests users may be influenced by the <em>presence</em> of citations rather than their proper attribution or relevance. This is not a good sign for quality media if users don’t register the quality of citations and only look to see that a response has <em>more</em> citations.</p><p>Another observation from Miroyan was that users tended to prefer responses that didn’t cite Wikipedia. The authors suggest that this may be because Wikipedia information is not always up-to-date. This interpretation emphasizes <em>the value for user satisfaction of integrating timely news information into AI search responses</em>.</p><p>The other aspect of the Miroyan paper that is intriguing is their intent taxonomy and their description of different user intents across the dataset. The taxonomy consists of eight different categories of user intent which users expressed in their prompt to the AI model. These are text processing, creative generation, factual lookup, information synthesis, analysis, recommendation, explanation, and guidance, as well as a catch-all “other”. From their prevalence in the dataset (see below) we can see that there are many categories of content that news organizations produce which align with these user intents. Facts, synthesis and analysis, recommendations, explanations, and how-tos (i.e. guidance) are all in demand from AI search users. How might this guide content production strategy?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-WKuPwoDGLTI7gzf" /></figure><p>Yang’s analysis is specifically focused on news citations in the dataset. Of the 366,087 citations from 83,533 domains he finds that <em>only 9% of those are to news outlets</em>. As shown in the next figure, OpenAI tended to cite more news sources than Google or Perplexity.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*gvIMS7Ha5kGoQRXY" /></figure><p>The analysis makes clear that different model providers tend to cite different sets of sources (something <a href="https://generative-ai-newsroom.com/genai-as-news-gatekeeper-what-traffic-data-shows-783795ba2103">we also noted in one of our prior analyses</a>). OpenAI cites reuters.com most often, whereas Google favors indiatimes.com and Perplexity tends to prefer bbc.com. Perplexity cites more unique news sources (1,430) in comparison to Google (881) and OpenAI (707). Yet despite the hundreds of sources that could be cited there was considerable concentration, where a few sources garnered the majority of impressions. For OpenAI models, the top 20 most frequent news sources accounted for 67.3% of all news citations. For Google and Perplexity it was a bit less concentrated at 31.9% and 28.5% respectively for the top 20.</p><p>Yang also grades news sites based on their political lean (left, center, right) and quality (high vs. low) to understand how those dimensions relate to AI model responses. He finds that most sources were tagged as high quality (OpenAI: 96.2%, Google: 92.2%, and Perplexity: 89.7%) and that, much as for traditional search engines, sources are mostly in the center but with a leftward skew (only 0.3% of sources were right-leaning for OpenAI, 0.8% for Google, and 1.2% for Perplexity). There is of course the possibility that users’ queries bias the model responses, but the analysis didn’t look at whether the query itself might reflect a more left or right-leaning framing. Yang also analyzes how some of this varies based on user intent and query topic, though interpreting a clear story there is difficult.</p><p>The main take-away from the Yang paper seems to be that it can really matter what model you choose in terms of what sources you are exposed to. Each model leans into a different set of sources, with some more concentrated than others. And while most sources were deemed high quality there’s also some lower quality information integrated as well. Combined with the skew towards left-leaning sources, this all paints a picture where individuals’ source exposure could both be highly variable based on the service they use, but also generally skewed to a relatively concentrated set of sources</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd1d1ac96712" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/ai-search-users-and-news-insights-from-lm-arena-data-bd1d1ac96712">AI Search, Users, and News: Insights from LM Arena Data</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ What I Learned from Making a ‘Liquid Content’ Machine ]]></title>
<link>https://generative-ai-newsroom.com/what-i-learned-from-making-a-liquid-content-machine-6a7ddce0bb9e?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/6a7ddce0bb9e</guid>
<category><![CDATA[ generative-ai-tools ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ product-management ]]></category><category><![CDATA[ prototyping ]]></category><category><![CDATA[ liquid-content ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Mon, 04 Aug 2025 14:16:40 GMT</pubDate>
<atom:updated>2025-08-04T14:16:39.633Z</atom_updated>
<content:encoded><![CDATA[ <h4><em>We developed a way to accurately turn podcasts into TikTok videos but they were lousy</em></h4><p><a href="https://generative-ai-newsroom.com/how-a-danish-news-service-made-a-profit-with-its-transcription-tool-285bc05b7cf9">Good Tape</a>, <a href="https://generative-ai-newsroom.com/can-sophina-turn-me-into-a-tiktok-star-930f9c489408">Sophiana</a>, <a href="https://particle.news/">Particle</a> and <a href="https://www.heynota.com/">Nota</a> are all news products promising to use generative AI to reformat content. Turning audio into text, translating, turning text into video, summarizing, extracting clips, or changing the tone are just a few ways they promise to transform your content. This is increasingly known as “liquid content”, what tech journalist <a href="https://podcasts.apple.com/gb/podcast/the-media-copilot/id1712909951?i=1000718386292">Pete Pachal defines as</a> “the new term for when AI remixes stories into different formats”.</p><p>In 2024, I was a product manager making our own liquid content machine, or liquidizer, if you will. Now as a reporter at Generative AI in the Newsroom, I want to help you replicate uses of generative AI that work well and avoid repeating others’ mistakes. So, before I turn my attention to reporting on yet another generative AI tool or use case, I thought it only fair that I share some of my own mistakes. Here’s 8 things I learned.</p><h4>#1 — You can test output before deciding to build a tool</h4><p>We built a tool called PodMorph which an audio producer could use to upload their finished podcast and download new formats inspired by that content. Podmorph, using the “magic” of generative AI, took their audio as a starting point to create content in multiple formats including a text description of the show, a square picture for Instagram, and a vertical video for TikTok.</p><p>A podcast producer could upload their audio to PodMorph:</p><figure><img alt="A graphical interface of PodMorph which shows an audio upload feature." src="https://cdn-images-1.medium.com/max/1024/0*ySoS1oMx5ps0eap5" /></figure><p>…then PodMorph generated content based on the audio in multiple formats:</p><figure><img alt="A graphical interface of PodMorph which shows options to play your audio, generate an Instagram video, generate audio clips, create a LinkedIn post, among other features." src="https://cdn-images-1.medium.com/max/1024/0*xrO0D_LN7l6UrSnF" /></figure><p>We gave audio producers PodMorph to test out. <a href="https://www.tiktok.com/@overcoat_media/video/7478297165958319382">This is the video</a> one producer uploaded to TikTok made by PodMorph based on their <a href="https://www.bbc.co.uk/programmes/m0027v9q">30 minute radio documentary about Beethoven’s emotional journey</a>:</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fwww.tiktok.com%2Fembed%2Fv2%2F7478297165958319382&amp;display_name=tiktok&amp;url=https%3A%2F%2Fwww.tiktok.com%2F%40overcoat_media%2Fvideo%2F7478297165958319382&amp;image=https%3A%2F%2Fp16-common-sign-no.tiktokcdn-us.com%2Ftos-no1a-p-0037-no%2Fo4IGqewiIzACQ6LAPxfAcPHeg3YjDTIkIILXqz%7Etplv-tiktokx-origin.image%3Fdr%3D9636%26x-expires%3D1754229600%26x-signature%3Dt90tlnB8ymviaOc60KX1QoOTB%252F4%253D%26t%3D4d5b0474%26ps%3D13740610%26shp%3Db59d6b55%26shcp%3D43f4a2f9%26idc%3Duseast8&amp;type=text%2Fhtml&amp;schema=tiktok" width="340" height="700" frameborder="0" scrolling="no"><a href="https://medium.com/media/8a20f939584f92a278ee7ef9cc202feb/href">https://medium.com/media/8a20f939584f92a278ee7ef9cc202feb/href</a></iframe><p>There are multiple problems with this video, not least that it is boring (in my opinion as a former social media producer for the BBC). We didn’t need to build Podmorph to find this out.</p><p>If I were to do this again, I would take the podcast audio, manually put it through our stream of generative AI tools and give the video intended for TikTok to the podcast’s producer. I would ask the producer to assess the content. If they deemed it good enough to share, then I would then measure the audience reaction and use all that feedback to help decide whether to build a tool and inform what features the tool should have.</p><h4>#2 — It’s challenging to build a tool during rapid technological change</h4><p>In late 2024, I drew up a long list of AI tools to look at and decide whether we should use them within PodMorph. It was overwhelming:</p><figure><img alt="A list of AI tools including Pictory, Stability AI, Luma AI, Kapwing, Descript, Kling AI, QuickMagic, HeyGen, Hypernatural, Augie, Runway, Pika Labs" src="https://cdn-images-1.medium.com/max/1024/0*_pWy_6L57JdhZ4uW" /></figure><p>I didn’t find a video generation tool that fit our requirements. We couldn’t wait because our funders required us to spend the money within their timeframe. So our software engineer created video using image generation tools and some good old-fashioned coding. But all the way through our build we were anticipating releases from the big tech companies. Notably, there were rumours OpenAI was due to release a video generation tool, Sora. It didn’t become available in the UK until 28 February 2025, long after we had built PodMorph.</p><p>This made me realise how little control we had when we were relying on others’ tools within our product. I wondered whether it would have been better to teach journalists how to use the general AI tools to work for them instead of creating a customized tool that would be challenging to keep updated and competitive.</p><h4>#3 — I was surprised how much value journalists derive from a “wrapper”</h4><p>Some might describe PodMorph as a wrapper. This is a term used for tools that have customised others’ tech. It’s sometimes used as an insult to say they aren’t doing anything technically impressive themselves. I would describe PodMorph as a fancy wrapper — AI tools plus some coding from an experienced software engineer.</p><p>Either way, users don’t care. They are focused on whether the tool helps them solve their problem.</p><p>Our user testing showed that our least technically complicated feature was our most popular with our users. It didn’t matter to them that turning audio into video was a real technical feat. They liked the text descriptions most.</p><figure><img alt="PodMorph’s graphical interface which shows a short, medium and long description of a podcast." src="https://cdn-images-1.medium.com/max/1024/0*AlOc1-ycLFATDOl_" /><figcaption>Audio producers write a short, medium and long description of their show for their podcast platform. They welcomed starting with an AI generated first draft.</figcaption></figure><p>They could have just uploaded their audio into <a href="https://journaliststudio.google.com/pinpoint/about/">Google Pinpoint</a> and asked for a description. But they didn’t. Drawing from when I was a producer for BBC Radio 4, I can imagine that our users didn’t have time to learn how to use more general-purpose AI tools, so a wrapper meant for a targeted task was valuable to them.</p><h4>#4 — Solve the problem instead of automating the workflow</h4><p>I fell into the trap of automating the audio producers’ manual workflow rather than creating a product that would solve their underlying problem. In our case, the underlying problem was that podcasters found it hard to find an audience and audience members found it hard to find podcasts. The problem is known as “discoverability”.</p><p>Podcasters told me that they felt there was a potential audience on video social media platforms but they had various things stopping them from making videos. I reasoned from this that we should make a tool that used generative AI to generate the video they said they needed. However, the video content PodMorph made did not appear to lead to more listeners.</p><p>I learned that I should have spent more time investigating the problem of discoverability before starting to build a solution.</p><h4>#5 — Check with a software engineer that you definitely need generative AI</h4><p>Don’t assume you have to use generative AI. Maybe the reason the tool you want didn’t previously exist is because a software engineer just never knew you wanted it. Generative AI, in my view, really helps with being able to articulate your needs across different disciplines. But it’s a starting point and I would urge you to investigate whether it is possible to solve your problem without generative AI.</p><p>There are plenty of reasons why you might not want to use generative AI, including vulnerability created by potential changes in subscription costs, and <a href="https://generative-ai-newsroom.com/the-often-overlooked-water-footprint-of-ai-models-46991e3094b6">the environmental costs</a>.</p><h4>#6 — Generative AI tools’ safeguards cut out on distressing news content</h4><p>Here’s another reason to find a way to build a tool for newsrooms without generative AI: We found that our generative AI tools didn’t give us output for sensitive content. Their tools are made for general usage, not for news. But news, current affairs and documentaries can be gruesome. For example, we tried PodMorph on a documentary looking back at when the IRA bombed a seaside hotel in 1984. The content was broadcast on BBC Radio 4 on a Monday afternoon. So editors at the BBC didn’t see it as too sensitive to broadcast. But the generative AI tools’ didn’t appear to be capable of making such a nuanced judgement as an experienced BBC editor and refused to generate output about this subject.</p><h4>#7 — Involve journalists in prompt writing</h4><p>The first iteration of our prompts were written by our software engineer without any consultation. Some of our users’ feedback criticized the tone of the output. If I were to do this again, I would experiment with obtaining our users’ good examples of well-written output and add that to the prompts.</p><p>Echoing this,<a href="https://generative-ai-newsroom.com/what-tools-can-newsrooms-use-to-evaluate-generative-ai-prompts-ed682c42c225"> Senior Engineer in the New York Times AI team Duy Nguyen</a> told me recently that a big chunk of his work is in extracting the rules for good output from the editorial team. “Figure out what it is that you mean when you say ‘this copy is good versus this copy is bad’” he told me.</p><h4>#8 — If you don’t have a financial strategy you may not exist soon</h4><p>PodMorph was funded by a one-off grant from the UK government which covered the cost of paying me as product manager, a software engineer, and a designer to make and test the prototype over a six month period. I didn’t find another way of paying for the next stage immediately after the grant money ran out so our project ended. I learned from that that to continue I would have needed skills within the team to build a financial strategy for the next stage of PodMorph.</p><p>That was the harshest lesson for me but I also enjoyed being reminded throughout this project that it’s important to know your users, understand their problems deeply, test before you build, and to not confuse technical novelty with actual value.</p><p>I’ve shared my “learning experience”, and now I’d love to hear about yours: clare.spencer@northwestern.edu.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6a7ddce0bb9e" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/what-i-learned-from-making-a-liquid-content-machine-6a7ddce0bb9e">What I Learned from Making a ‘Liquid Content’ Machine</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ The Search Terms That Trigger Google’s AI Overviews ]]></title>
<link>https://generative-ai-newsroom.com/the-search-terms-that-trigger-googles-ai-overviews-8418068be46a?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/8418068be46a</guid>
<category><![CDATA[ audience ]]></category><category><![CDATA[ google-ai-overview ]]></category><category><![CDATA[ news ]]></category><category><![CDATA[ generative-ai-tools ]]></category>
<dc:creator><![CDATA[ Ashlyn Wang ]]></dc:creator>
<pubDate>Mon, 28 Jul 2025 14:03:00 GMT</pubDate>
<atom:updated>2025-07-29T04:34:51.967Z</atom_updated>
<content:encoded><![CDATA[ <h4>Which keywords does Google’s AI choose to summarize in search?</h4><p><strong>By </strong><a href="https://medium.com/@ashlynxwang"><strong>Ashlyn Wang</strong></a><strong> and </strong><a href="https://nicholashagar.medium.com/"><strong>Nick Hagar</strong></a></p><figure><img alt="Image via Google" src="https://cdn-images-1.medium.com/max/1024/1*NBhxa5lA_vZhjPvzlG_jCQ.jpeg" /><figcaption>Image via Google</figcaption></figure><p>For many news websites, <a href="https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2025">Google Search traffic is vital</a><em>.</em> Sites that rank higher in search results get more clicks — and the added revenue from ads, subscriptions, or sales. But Google’s AI Overviews may upend this dynamic.</p><p>Introduced by Google last year, AI Overviews are <a href="https://support.google.com/websearch/answer/14901683">AI-generated summaries</a> that appear at the top of search results when its systems believe that “generative AI can be especially helpful.” These summaries aim to deliver direct answers to user queries, so that they don’t have to peruse results pages to find answers. But while AI Overviews cite their sources, linking out to external sites with supporting evidence, users appear less likely to click when a summary is already provided.</p><p><a href="https://ahrefs.com/blog/ai-overview-keywords/">Research</a> from marketing intelligence platform Ahrefs finds that search keywords triggering AI Overviews have eight times less traffic potential than those that don’t. A <a href="https://www.pewresearch.org/short-reads/2025/07/22/google-users-are-less-likely-to-click-on-links-when-an-ai-summary-appears-in-the-results/">Pew Research Center analysis</a> of users’ browsing behavior in March 2025 concurred, finding that “users were less likely to click on result links when visiting search pages with an AI summary compared with those without one.” Sites are looking at losing a substantial share of their search traffic, forcing a reassessment of traffic strategies.</p><p>But it’s not clear yet what search terms AI Overviews actually tend to show up on — where will traffic be most impacted? Google has <a href="https://www.wsj.com/tech/ai/google-ai-news-publishers-7e687141?st=VijXkj&amp;reflink=desktopwebshare_permalink">noted</a> that it often doesn’t display AI Overviews for trending news queries. Earlier studies, like Ahrefs’, have investigated various potential factors behind AI Overviews. Here we hope to better understand how much these generative responses might affect news publishers, on what kinds of searches, and how that compares to other types of sites.</p><h3>Our Approach</h3><p>In May, we collected data on many thousands of organic keywords — specific words or phrases users type into search engines to find content — and analyzed whether or not they triggered an AI Overview and under what circumstances. These keywords often represent the topics or queries that websites want to rank for in search results to attract relevant, non-paid traffic.</p><p>To minimize variation, the keyword data was gathered in batches within a short timeframe using <a href="https://ahrefs.com/">Ahrefs</a>, a comprehensive Search Engine Optimization (SEO) platform that offers tools for keyword research, traffic analysis, and other insights to help websites improve search engine visibility. For our research, we utilized two of its features: <a href="https://ahrefs.com/site-explorer">Site Explorer</a>, to collect top-searched keywords for which our sample of sites rank in Google’s search results (discussed below), and <a href="https://ahrefs.com/keywords-explorer">Keywords Explorer</a>, to analyze the AI Overview feature of those keywords.</p><figure><img alt="Example keyword research results from Site Explorer (Image via Ahrefs)" src="https://cdn-images-1.medium.com/max/1024/1*gTjHlQ0Z36c0Kw42oktCdg.png" /><figcaption><em>Example keyword research results from Site Explorer (Image via Ahrefs)</em></figcaption></figure><p>The analysis was conducted across two dimensions: <strong>site category and site frequency.</strong></p><h3>Our Dataset</h3><p>We selected three site categories that align with our research focus and generate substantial search traffic:</p><ul><li><strong>News sites</strong>: Major news publications covering current events</li><li><strong>Informational sites</strong>: Reference websites, government pages, and educational resources</li><li><strong>Popular sites</strong>: High-traffic websites across all categories</li></ul><p>For each category, we chose 10 prominent websites based on Ahrefs’ “<a href="https://ahrefs.com/websites/united-states/news">Top Websites in the United States</a>” rankings.¹ The sites were selected from specific sections: “News” for news sites, “Reference” and “Law and Government” for informational sites, and top performers across all classifications for popular sites.</p><p>We then compiled data for the 30 websites by extracting their top 1,000 search keywords, ranked by search volume. This gave us a dataset of 30,000 keyword entries (with possible repetitions across sites) reflecting the most common terms people searched for at the time of our data collection that led them to these sites.</p><p>To ensure data quality, we applied three filters:</p><ul><li>Included only U.S.-based searches</li><li>Selected keywords where the target site ranked in the top 10 search results (roughly the first page)</li><li>Excluded branded queries (searches containing company/product names)</li></ul><p>These filters helped us focus on keywords with real traffic potential and that capture general search behavior — the space where AI Overviews would actually compete with traditional organic search results.</p><h3>Analysis Framework</h3><p>We examined how many sites each keyword drove traffic to within each site category — from single-site keywords (narrower, publisher-specific queries) to those leading to 3+ sites (more broadly relevant topics). In news contexts, search keywords shared across multiple sites may signal wider coverage and newsworthiness, while site-specific queries suggest more specialized content. This distinction helps identify whether there is a relationship between the level of specificity in a user’s search query and the appearance of an AI Overview.</p><figure><img alt="Example of keywords in our dataset that drive traffic to two or more sites" src="https://cdn-images-1.medium.com/max/1024/1*q-1U8Y2wUAwRgo-tDWOCyA.png" /><figcaption><em>Example of keywords in our dataset that drive traffic to two or more sites</em></figcaption></figure><h3>Key Findings</h3><p>Our analysis reveals two main patterns about when and where Google’s AI Overviews appear in search results.</p><h4><strong>AI Overviews Appear Less Frequently for News Sites</strong></h4><p>News site keywords trigger AI Overviews at a notably lower rate compared to informational sites:</p><ul><li><strong>News sites:</strong> 10.5% of distinct keywords triggered AI Overviews</li><li><strong>Informational sites:</strong> 20.9% of distinct keywords triggered AI Overviews</li><li><strong>Popular sites:</strong> 8.0% of distinct keywords triggered AI Overviews</li></ul><p>The disparity indicates that AI Overviews are more prevalent for queries leading to informational sites than for those driving traffic to news sites.</p><h4><strong>AI Overviews Target Specific, Focused Queries</strong></h4><p>The chart below further illustrates keyword patterns across three site frequency thresholds in our analysis where we had sufficient data. Beyond this range, the number of keywords in the informational site category became too small to produce a reliable proportion.</p><figure><img alt="Our analysis suggests a relationship between specificity and AI Overviews: As search keywords appear across more sites, the percentage triggering an AI Overview decreases" src="https://cdn-images-1.medium.com/max/989/1*I5TdXQYTNSCXk_rODMaQ8g.png" /><figcaption><em>Our analysis suggests a relationship between specificity and AI Overviews: As search keywords appear across more sites, the percentage triggering an AI Overview decreases</em></figcaption></figure><p>The evidence suggests that AI Overviews are somewhat more prevalent for queries seeking narrow-coverage topics rather than broader-coverage topics. Keywords that appeared across multiple sites in our dataset were less likely to generate AI Overviews, indicating that more broadly covered topics are less likely to have AI summarization, while more specific, targeted queries trigger it more often. This was true across all three site types.</p><p>These findings underscore a strategic challenge for news organizations. While AI Overviews may only threaten traffic for about 1 in 10 news-relevant queries, the variance in appearance of AI Overviews suggests that publishers may face an unequal erosion of traffic for different kinds of content.</p><h4>Limitations</h4><p>While our findings reveal discernible trends, there are important limitations to consider in exploring AI Overviews.</p><p>First, <a href="https://blog.google/products/search/ai-overview-expansion-may-2025-update/">Google</a> is still actively testing and refining its AI Overview rollouts, so an AI summary might not show up for a keyword one day but display the next. This fluctuation means our data captures a snapshot in time, not a stable or definitive state. For example, <a href="https://ahrefs.com/blog/ai-overview-growth/">Ahrefs</a> reported that following the company’s March 2025 Core Update, the presence of AI Overviews in search results more than doubled. Furthermore, AI Overviews, like other generative AIs, are also susceptible to hallucinations and factual inaccuracies, which raise valid concerns about the reliability of the information they provide. As such, these results should still be treated carefully.</p><p>Second, the tool we utilized to detect whether a keyword activates an AI Overview is neither static nor foolproof. Even though we rely on it to identify the presence of AI Overviews, there can be discrepancies between what the tool reports and what actually appears in live search results.</p><p>And in practice, the line between news and other types of information can be blurred in search, and our method of evaluating keywords may not have clearly distinguished between the two. Future research could explore more precise ways to tag queries to offer deeper insights into their relevance to news-related topics.</p><h4>In Closing</h4><p>Our analysis reveals that AI Overviews currently appear less frequently for queries leading to news sites than for queries leading to information sites. To the extent that some of the queries leading to information sites could overlap with news coverage, such as for service journalism or explainers on general information topics, such news coverage might be more heavily impacted.</p><p>We also find that AI Overviews appear to target narrower, publisher-specific queries more so than queries associated with topics that are more widely covered. This would suggest that more unique coverage might be more vulnerable to AI Overviews than more commodity coverage, which is reported by many outlets. The differential targeting poses a particular challenge because it may erode traffic more so on distinctive and original content.</p><p>As AI Overviews become more sophisticated and potentially expand their coverage, especially if <a href="https://search.google/ways-to-search/ai-mode/">Google AI Mode</a> becomes a default user experience, publishers may see erosion in these referral streams. The relatively low prevalence of AI Overviews for news keywords shouldn’t inspire complacency. As our results demonstrate, queries leading to information sites show a higher prevalence of AI Overviews. Publishers should closely monitor which of their high-traffic queries surface AI Overviews, particularly across different content categories. As Google’s AI initiatives continue to evolve, newsrooms may increasingly need to reconsider their reliance on organic search traffic.</p><h4><strong><em>Endnotes</em></strong></h4><p>[1] Information sites: <a href="http://britannica.com">britannica.com</a>, <a href="http://ca.gov">ca.gov</a>, <a href="http://en.wikipedia.org">en.wikipedia.org</a>, <a href="http://irs.gov">irs.gov</a>, <a href="http://merriam-webster.com">merriam-webster.com</a>, <a href="http://ny.gov">ny.gov</a>, <a href="http://quora.com">quora.com</a>, <a href="http://ssa.gov">ssa.gov</a>, <a href="http://timeanddate.com">timeanddate.com</a>, <a href="http://wikihow.com">wikihow.com</a></p><p>News sites: <a href="http://bbc.com">bbc.com</a>, <a href="http://cbsnews.com">cbsnews.com</a>, <a href="http://cnn.com">cnn.com</a>, <a href="http://foxnews.com">foxnews.com</a>, <a href="http://nbcnews.com">nbcnews.com</a>, <a href="http://nypost.com">nypost.com</a>, <a href="http://nytimes.com">nytimes.com</a>, <a href="http://usatoday.com">usatoday.com</a>, <a href="http://usnews.com">usnews.com</a>, <a href="http://news.yahoo.com">news.yahoo.com</a></p><p>Popular sites: <a href="http://amazon.com">amazon.com</a>, <a href="http://espn.com">espn.com</a>, <a href="http://facebook.com">facebook.com</a>, <a href="http://fandom.com">fandom.com</a>, <a href="http://google.com">google.com</a>, <a href="http://imdb.com">imdb.com</a>, <a href="http://instagram.com">instagram.com</a>, <a href="http://pinterest.com">pinterest.com</a>, <a href="http://reddit.com">reddit.com</a>, <a href="http://youtube.com">youtube.com</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8418068be46a" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/the-search-terms-that-trigger-googles-ai-overviews-8418068be46a">The Search Terms That Trigger Google’s AI Overviews</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ What Tools Can Newsrooms Use to Evaluate Generative AI Prompts? ]]></title>
<link>https://generative-ai-newsroom.com/what-tools-can-newsrooms-use-to-evaluate-generative-ai-prompts-ed682c42c225?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/ed682c42c225</guid>
<category><![CDATA[ prompt-evaluation ]]></category><category><![CDATA[ prompt-engineering ]]></category><category><![CDATA[ evaluation ]]></category><category><![CDATA[ llm-evaluation ]]></category><category><![CDATA[ model-evaluation ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Mon, 21 Jul 2025 13:03:13 GMT</pubDate>
<atom:updated>2025-07-21T13:03:12.078Z</atom_updated>
<content:encoded><![CDATA[ <h4>A growing list of tools may help you improve your generative AI prompts, but sometimes all you need is a spreadsheet</h4><p>If your newsroom is using generative AI then you will also want to understand how well your prompts are performing. This is called prompt evaluation, sometimes referred to in industry jargon as “evals”. “…evals are what shape the reliability, usability, and ultimately, the success of AI systems,” <a href="https://www.braintrust.dev/blog/eval-playgrounds">according to the marketing </a>material for Braintrust. <a href="https://www.braintrust.dev/">Braintrust</a> is one of a long list of tools that offer to help you evaluate prompts. Others include: <a href="https://www.promptfoo.dev/">Promptfoo,</a> <a href="https://evallm.kixlab.org/">EvalLLM</a>, <a href="https://www.comet.com/docs/opik/">Opik,</a> <a href="https://www.evidentlyai.com/">Evidently,</a> <a href="https://github.com/confident-ai/deepeval">Deep Eval</a>, <a href="https://mlflow.org/">MLFLow</a>, <a href="https://ai.pydantic.dev/evals/">Pydantic’s evals</a>, <a href="https://github.com/AlmogBaku/pytest-evals">Pytest evals</a>, <a href="https://www.langchain.com/langsmith">LangSmith</a> and <a href="https://chainforge.ai/">ChainForge</a>. In addition, from the big businesses, Open AI has its own <a href="https://platform.openai.com/docs/guides/evals">Evals API</a>, Microsoft has the <a href="https://ai.azure.com/">Azure AI Foundry portal</a>, Amazon has <a href="https://aws.amazon.com/bedrock/">Bedrock</a>, and Google offers evaluations in <a href="https://cloud.google.com/vertex-ai?hl=en">Vertex</a>.</p><p>Your evals could be part of a cycle of iteratively improving your prompts, as <a href="https://medium.com/generative-ai-in-the-newsroom/personalizing-newsletters-a-case-study-on-iterative-prompt-evaluation-and-improvement-a9f1141900fd">Ashlyn Wang wrote in a previous article on GAIN</a>.</p><figure><img alt="Diagram of an iterative prompting workflow which starts with “define the objective”, then “design the prompt”, then “execute the prompt”, then “evaluate the output”, then “diagnose issues”, then “identify improvements”, then “refine the prompt”, then “test and reiterate” and then back to the start of the cycle." src="https://cdn-images-1.medium.com/max/1024/0*wRBUcm1pGfJx2IEq" /></figure><p>Ashlyn used ChainForge to evaluate prompts for generating newsletter headlines. The tool promises to help “evaluate the robustness of prompts and text generation models in a way that goes beyond anecdotal evidence.”</p><p>ChainForge helped her manually analyze 300 AI generated newsletter headlines that she evaluated based on accuracy, hallucination, style, and fluency. She asked the following questions:</p><ul><li>Does the headline align with the story’s main focus and framing? Could the wording mislead a general reader, even if factually correct?</li><li>Does the headline include details, claims, or implications that are factually incorrect, nonexistent, or unsupported by the article?</li><li>Are there any major deviations from the publication’s intended style?</li><li>Is the text well-structured and easy to read?</li></ul><p>In addition she also set up automated evaluations to test for relevance and consistency by asking if the headline reflected the user’s reading history and if the headlines were consistent across multiple generations of the same prompt.</p><p>A prompt evaluation on ChainForge can be set out visually, as they demonstrate in an example evaluation on their website:</p><figure><img alt="Chain Forge’s graphical interface which shows a pipeline of tasks" src="https://cdn-images-1.medium.com/max/1024/0*nAp7vO-8hO6_er15" /></figure><p>Ashlyn said ChainForge allowed her to create evaluations to compare headlines from multiple prompts.</p><p>My colleague in Northwestern University’s Computational Journalism Lab, the post doc researcher Nick Hagar, showed me behind the scenes of his own evaluation process he is setting up. He is making a prototype which gives journalists insights into the components of headlines that performed well with the audience. He wants to then evaluate the prompts he gave generative AI to compose those insights. He is trying out an <a href="https://wandb.ai/site/">eval tool provided on the platform Weights and Biases.</a> It’s early days in this project but already he particularly likes using this eval tool because of the level of detail it goes into, including the expected costs of the prompts. “You can see exactly what is happening at each step. You can see the input, you can see the response from the LLM. You can see what each of these costs depending on what model I’m using. This is all super rich data.”</p><p>Curious as to who was using evaluation tools in Newsrooms, I asked a few colleagues across the industry. Kaveh Waddell was kind enough to talk me through how he intends to use Opik for his next project. Previously a journalist for The Atlantic and Axios, he teamed up with his colleague on the JSK fellowship, Patrick Swanson, to form Verso, a consultancy, in San Francisco. He told me that while setting up to work with the American Journalism Project to build audience personas with large language models he spent a couple of hours comparing evaluation tools and chose Opik partly because you can compare prompts. “You can run experiments against multiple prompts so you can compare a small tweak and see how it works,” he told me. Kaveh showed me how he accessed Opik by navigating through the command line, though it does also have a graphical user interface for the less technically adept.</p><p>It occurred to me that this could become a somewhat technical task — and that led me to Senior Engineer in the New York Times AI team Duy Nguyen. He built Echo, an internal tool <a href="https://www.theverge.com/news/613989/new-york-times-internal-ai-tools-echo">widely reported in the tech press</a> that summarizes Times articles, suggests social posts, generates quizzes, pulls out quotes, extracts names of people interviewed, generates alt text to support accessibility, and generates SEO headline suggestions.</p><p>One thing Duy doesn’t want to do with generative AI, is replace the writer. “Outside of The Times, there’s always people saying ‘LLMs are going to replace journalists, LLMs are going to replace writers.’ And that is not something that we want.” Quite the opposite, Duy is keen to involve journalists in the process of using and evaluating generative AI tools. “We really value getting editor buy-in, getting journalists’ buy-in. We loop them into the process. We invite them to talk to one another, to talk to us, figuring out ‘what do you mean by good?’ And collectively building upon our institutional knowledge and drawing from that institutional knowledge to build systems that would help us evaluate. That is the more important thing that I’m not seeing a lot of eval tools out there cover, and for good reasons because I don’t think any one of them are specifically tuned to the challenges that journalism is facing,” he told me. Instead he argues that sometimes all you need for an evaluation is a vanilla spreadsheet. At the New York Times he uses Google Sheets and what he called a “homegrown” tool that he refrained from sharing more detail on.</p><p>For him the evaluation <em>process</em> is far more important than the tool itself. Part of this process is asking editorial teams to define evaluation criteria. “I tend to focus on the actual evaluation processes, regardless of what the interface or the platform is. Most importantly: how do I make sure that editors are looped in? That they understand why we’re doing evaluations and get their buy in. To do evaluations you need the criteria, the qualities, the touchstones that you’re evaluating for, and you can only get those by asking the editors what is important or publishable.” But, as was emphasized at the recent <a href="https://generative-ai-newsroom.com/benchmarking-llm-performance-for-journalism-9831e6f918e4">GAIN benchmarks workshop</a>, it can be challenging to get that editorial time investment.</p><p>He sees evaluation as a two step process: “Figure out what it is that you mean when you say ‘this copy is good versus this copy is bad’ specifically for your team and then devise a scoring framework that helps you translate those qualities into actual metrics and scores that you can tally.”</p><p>With the example of writing an SEO headline, he explains that once people start using that prompt every day that “They’re starting to think ‘how do I know that this is actually a good headline?’ That prompts the need to evaluate. When you are confronted with an increasing amount of copy, then just spot checking with your eye starts to be intimidating and you need some sort of data. You need to be more rigorous about what you’re evaluating.”</p><p>He described a team effort to pull out information that is “trapped inside the editor’s head”.</p><figure><img alt="AI generated illustration of a man writing and a drawing of two men inside his head with notices which say “good content” and “bad content”." src="https://cdn-images-1.medium.com/max/1024/0*rhvaQXhUmC19Uc2H" /><figcaption>Image created by ChatGPT with the prompt “Please generate a picture of extracting the criteria for what makes good and bad content “trapped inside the editor’s head”.”</figcaption></figure><p>For example, he described a scenario where the morning team had been using Echo to generate suggestions for summaries that go in the newsletter every day in the form of bullet points. “We all sit down in one room and we enumerate ‘what do we mean by good bullet versus bad bullet?’ Those are the things that are already inside editors’ heads because they have to edit these bullets every day. We synthesize all of those into a defined list of well-laid criteria that are mutually exclusive, and we tell them ‘here are the different facets of good’. It could be something like ‘a bullet should be no more than 25 words, but it should be no less than 19 words’ or ‘it should have at most two sentences’ or ‘it shouldn’t include any names of individuals that are not well known’ or ‘it shouldn’t include any big numbers or statistics that doesn’t mean anything to the reader’.”</p><p>The idea of laying this all out as criteria to mark in a Google sheet got me thinking about the difficulties I’ve faced before with asking journalists to use spreadsheets. As the press lead at the open-source AI platform Hugging Face, Florent Daudens, <a href="https://generative-ai-newsroom.com/bringing-open-source-models-to-spreadsheets-c440fc4818b4">wrote previously for GAIN</a>: “It can be challenging to talk to the spreadsheet, master the formulas, etc., something I’ve often observed when teaching data journalism to journalists and students.”</p><p>I am reminded of the former principal data scientist in the BBC’s responsible AI team, Oli Elliott, <a href="https://medium.com/generative-ai-in-the-newsroom/how-to-check-if-ai-assistants-are-distorting-your-news-stories-1fd7d2002a1c">telling me his one cool trick</a>. He had the gargantuan task of gathering journalists’ evaluations of 400 AI generated outputs. Even though some journalists were reluctant — seemingly allergic even — to using spreadsheets, he still managed to tap into their excellent editorial judgement in order to evaluate the quality of AI-generated responses. Oli did this by getting them to submit their evaluations on Microsoft forms instead. The form listed a rating scale from 1 to 4 for accuracy, attribution of sources, impartiality, context, editorialisation, and distinguishing opinions from facts.</p><p>But Nick suggests spreadsheets may not be enough for more complicated projects and explains why he opted for a dedicated tool.</p><p>“When is it not enough to use a spreadsheet? When do you need a dedicated tool?” he posed. “As soon as you get beyond question-and-answer one step conversation with an LLM For anything that’s more involved — multi-step, anything that’s with reasoning tools — the data is so complicated, putting it in a spreadsheet is way too much to keep track of. These tools are helpful for going back through and tracing ‘this is what happened in step one…’ so being able to rewind time and see what actually happened and what the outputs look like and you can’t really do that [with a] spreadsheet.”</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=ed682c42c225" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/what-tools-can-newsrooms-use-to-evaluate-generative-ai-prompts-ed682c42c225">What Tools Can Newsrooms Use to Evaluate Generative AI Prompts?</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ The Newspapers Using Generative AI to Surface Restaurant Reviews ]]></title>
<link>https://generative-ai-newsroom.com/the-newspapers-using-generative-ai-to-surface-restaurant-reviews-4ed4a75d21c2?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/4ed4a75d21c2</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ ai-accuracy ]]></category><category><![CDATA[ newspaper-archives ]]></category><category><![CDATA[ discoverability ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Wed, 16 Jul 2025 13:01:54 GMT</pubDate>
<atom:updated>2025-07-16T15:23:28.684Z</atom_updated>
<content:encoded><![CDATA[ <h4>Examples in leveraging local content archives to create new user value</h4><p>In February last year, the San Francisco Chronicle launched a tool which used AI to recommend restaurants from their archive of reviews. Other newspapers have followed their lead and learned some valuable lessons about the limits of archives and the differing expectations of AI within newsrooms.</p><p><a href="https://www.sfchronicle.com/projects/where-to-eat-sf-bay-area-chowbot/?utm_source=marketing&amp;utm_medium=copy-url-link&amp;utm_campaign=article-share&amp;hash=aHR0cHM6Ly93d3cuc2ZjaHJvbmljbGUuY29tL3Byb2plY3RzL3doZXJlLXRvLWVhdC1zZi1iYXktYXJlYS1jaG93Ym90Lw==&amp;time=MTc1MjU0NTY3OTAzNw==&amp;rid=NzBhZGI4OTgtYzcyOS00Y2E0LTgyYjctNzIzNmE3ZTk1MzNl&amp;sharecount=MQ==">Chowbot</a> is a chatbot you can ask to give you recommendations for where to eat in the San Francisco Bay area. It promises to “help you search through nearly 1,000 reviews from more than 50 restaurant guides, all written, edited and vetted by the San Francisco Chronicle’s Food &amp; Wine team”.</p><p>If you know what food you want and where you want it you can ask Chowbot. I asked it for pizzerias near the 16th Street BART Station and Chowbot recommended Sunset Squares for its strange toppings, Angie’s Pizza which also has tender meatballs, and Montesacro Pinseria, which serves pinsa, “the oval-shape Roman cousin to pizza”. I want them all.</p><figure><img alt="Screen grab of San Francisco Chronicle’s Chowbot tool. Text reads: “I’m looking for… pizzerias near the 16th Street BART Station. Printing recommendation” a picture of a receipt being printed accompanies the text." src="https://cdn-images-1.medium.com/max/1024/0*K8fqUmYoU5pPMdTa" /></figure><figure><img alt="Screen shot of Chowbot’s recommendations of three restaurants." src="https://cdn-images-1.medium.com/max/1024/0*1YrDlnjmqoWXKbou" /></figure><p>Chowbot also served me links to articles from the Chronicle:</p><figure><img alt="Screenshot of three recommended articles about restaurants and bakeries with pictures of food accompanying the text." src="https://cdn-images-1.medium.com/max/1024/0*otlMGnsoZDTzuq6P" /></figure><p>Ok, that one was easy (it was actually suggested in the Chronicle’s introduction so was unlikely to fail). Now for something more tricky. Recently my favorite meal was Jamaican curry goat in a bao bun. It’s such an unusual fusion that I suspect you can’t buy it anywhere in San Francisco. That’s why I’m testing the Chowbot with this question: to see what it says when it doesn’t have an answer. It comes up blank.</p><figure><img alt="Screenshot of Chowbot with the text: “You asked: I’m looking for curry goat and bao. Recommednation based on SF Chronicle food guides; I’m having trouble answering your question. Can you please rephrase it?”" src="https://cdn-images-1.medium.com/max/1024/0*SJcnquAzG7w6l8aq" /></figure><p>This is frustrating but it is a conscious trade off made by the newspaper. The Vice President of Editorial Innovations and AI Strategy at the Chronicle’s owner, Hearst, Tim O’Rourke told me that is a trade-off he is comfortable making. “We’ve got a pretty conservative approach to AI. We don’t want our readers to have to go and verify accuracy” They do this by only answering questions they have answers to in their archive. They would rather leave people unsatisfied with no answer than create the possibility that Chowbot might say something incorrect. It sets Chowbot apart from general use generative AI chatbots which aim to please by pretty much almost always giving an answer and being renowned for providing factually inaccurate information.</p><p>Learning from this experience, Hearst then launched a <a href="https://www.expressnews.com/projects/where-to-eat-drink-san-antonio-restaurants-chowbot/">restaurant recommender at the San Antonio Express-News</a>. A key difference here is that they made it more human by putting one restaurant critic’s perspective at the centre:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*j-6jPDou_wnnmaIl" /></figure><p>You can see in the recommendations the tool has pulled out extracts from Mike Sutter’s review:</p><figure><img alt="Screenshot of San Antonio version of Chowbot which shows a user has asked for curry goat and been recommended an Indian restaurant." src="https://cdn-images-1.medium.com/max/1024/0*mdVOarHPlhEhQvOa" /></figure><p>This time I didn’t specify Jamaican so I was recommended Indian cuisine. I have now looked at the menu and, while I couldn’t find goat, it does look mouth watering.</p><p>I asked the tool a harder question, for that rare Jamaican curry goat in bao. In contrast to the San Francisco Chowbot, it gave me a response. It still recognized it couldn’t find anything and indicated it was conveying that back to the food team, but also suggested three other, wildly different, restaurants.</p><figure><img alt="A screenshot of the San Antonio version of Chowbot where the user has asked for Jamaican curry goat in bao but the tool returned the answer “while I don’t have a specific answer to your quesion, I’ve made a note of this topic for our food team to review. In the meantime, take a look through the search results below — you might discover some unexpected dining gems”." src="https://cdn-images-1.medium.com/max/1024/0*cDgXmz6iKvUTJryu" /></figure><figure><img alt="A screen grab of three restaurant recommendations." src="https://cdn-images-1.medium.com/max/1024/0*k4h5IPRWtM815VCB" /></figure><p>Tim says they have learned from the process and think the later, San Antonio, version is better than the original San Francisco version. “Return sessions to the tool have increased with the San Antonio version and that mirrors the fact that we had maybe done a better job communicating, that the database remains updated, and that it gets promoted heavily.” Based on their ongoing learning they intend to update the SF version.</p><p>I’ve personally experienced news organizations trying to grapple with how they might leverage their previously somewhat untapped archive now that generative AI may be able to help them do things they previously couldn’t. This also touches on a familiar problem that content creators call discoverability: we have loads of good content but people struggle to find it. So it’s no surprise that another newspaper, outside of the Hearst group, has also made a restaurant recommender.</p><p>In June the Minnesota Star Tribune quietly published <a href="https://www.startribune.com/compass">Culinary Compass</a>. Product Manager Frank Bi said that they were inspired by Chowbot but didn’t work with the team behind it at all. A notable difference is that this is not a chatbot where a user types in questions. Instead it is a multiple choice quiz. Also, unlike the Chronicle’s Chowbot, it gave me a restaurant recommendation every single time I tried it.</p><p>Compass’s quiz asked me questions about the budget, the occasion, time of day, ambience, outdoor space, dress code, speed of service, kids, and dietary requirements. For ambience, I selected “I’m feeling tropical”. For the type of occasion, I typed in “bachelorette” party in a free text space.</p><figure><img alt="A screengrab of Culinary Compass’s quiz, asking about ambience." src="https://cdn-images-1.medium.com/max/1022/0*3I-cBRNSXDGS_sAZ" /></figure><figure><img alt="A screengrab of Compass’s quiz asking what occasion you are going out to." src="https://cdn-images-1.medium.com/max/1024/0*5XgIN-HQ00iUKtVt" /></figure><p>Compass suggested a restaurant with Mexican food called Mito which it described as “a vibrant spot perfect for big groups and celebrations”.</p><figure><img alt="Screengrab of compass’s recommendation, Mito, which it describes as a place with “sparklers, piñatas, and colorful shots abound. Over-the-top cocktails served in quirky glassware — from a bronze foot to an IV bag — add to the lively vibe.“" src="https://cdn-images-1.medium.com/max/1024/0*YwOwxdoK3eNGYXI8" /></figure><p>Its 100-word description “from our experts” painted a vivid picture of a place with “sparklers, piñatas, and colorful shots abound”. It’s the kind of useful wording you get from trusted food critics rather than marketing patter. Compass went on to add glorious detail: “Over-the-top cocktails served in quirky glassware — from a bronze foot to an IV bag — add to the lively vibe. Don’t miss the photo ops or the tres leches cake, served with a Bailey’s shot under a piñata.” A flood of FOMO came over me as I imagined the photos of drinking out of an IV bag.</p><p>At the top of the recommendation, were five categories, what I’d later learn the team call the “vibes” of the restaurant. For Mito these were “vibrant”, “Latin American”, “cocktails”, “group dining” and “colourful”. News innovation engineer Dana Chiueh told me that these vibe key terms are AI generated based on prior restaurant reviews and then checked by humans.</p><p>Product Manager Frank told me that Compass is trying to prevent the kind of overthinking that leads to no decision. Simultaneously, Compass also makes it easier to access the expertise of the Star Tribune’s trusted food critics.</p><p>“Something that’s really unique to the Minnesota Star Tribune is our depth of restaurant coverage. And one of the problems was that it was hard to show that depth,” Frank told me. “We wanted to pair that with the problem that we saw with our audience of what we’re calling ‘analysis paralysis’ — just thinking about how hard it is to find where to go eat.”</p><p>He said this is all partly possible with the thanks of generative AI which allowed the AI team to “pull up all the articles that we had, turn it all into structured data so that we could query it”.</p><figure><img alt="A man looks at phone displaying the Culinary Compass tool" src="https://cdn-images-1.medium.com/max/1024/0*EI6vWpF8Vc_x-ESL" /><figcaption>Caption: Compass asks you a series of questions — including about your budget — before recommending one restaurant.</figcaption></figure><p><a href="https://github.com/MinneapolisStarTribune/agate-ai/?tab=readme-ov-file">In a Star Tribune Github repository </a>that supports the project, the development team explains that their work is “part funded by a <a href="https://www.niemanlab.org/2024/10/the-lenfest-institute-launches-10-million-ai-news-program-for-big-city-dailies-with-backing-from-openai-and-microsoft/">grant from the Lenfest Institute</a>, with support from Microsoft and OpenAI. As a result, we’ve prioritized that platform…”.</p><p>Dana said generative AI played a couple of roles in this project. Firstly, they used OpenAI to extract information from the newspaper’s archive of restaurant reviews. She went on to tell me that “after we extracted that information, we also embedded all of the relevant excerpts and articles for each restaurant.” This makes the reviews searchable using whatever the user types in.</p><p>“They [the readers] can click some whimsical questions and all of that gets embedded through OpenAI and are searched against our database which has both hard filters (such as dietary restrictions, city, budget) as well as softer (ambiance / vibes related preference). At the end of it our system also generates an explanation of why this restaurant was chosen for this user: that’s the only place that [AI] generated text is produced on the fly and personalized to each user.”</p><p>So for my recommendation, a section entitled “How we chose your path” said: “Mito offers a lively patio and happy hour, perfect for budget-friendly group celebrations”. This neatly summed up all my quiz answers in one sentence. If nothing else, this has helped me articulate what I want, something I could use help with more generally in life.</p><p>That section is “data-driven, so didn’t require our expertise,” the Taste Editor Nicole Hvidsten told me.</p><p>However, in contrast, the short description “from our experts” was written especially for the Compass quiz by Nicole and three food critics on her team. She shared the thought process behind this decision: “The purpose of the product was to highlight the expertise of our food team, and we were adamant that any ‘recommendations’ be from us, the people who have visited the restaurants and can speak authoritatively about what they have to offer. That it be human written was never a question.”</p><p>The team utilized their knowledge of local restaurants to hand-select 160 restaurants and write a blurb for each one. The Star Tribune have appeared to opt for assurance that the quality of that text in particular is consistent and in line with editorial expectations, which leads to a trade-off insofar as they are only able to recommend from that limited set of restaurants.</p><p>“We have a pretty deep bench who has quite a bit of knowledge about the restaurant scene. So a lot of it was just written off of the top of our heads from previous experience at the restaurants.” Nicole told me. “We would not put anything in there that we didn’t have any idea of what it was.”</p><p>In projects where AI is used, the manual work involved can be revealing. The Star Tribune Food editorial team wrote all 160 blurbs of around 100 words manually. Nicole said this was because Compass is “public facing”, which I took to mean that the food team would be the ones responsible to the audience if the content wasn’t good enough.</p><p>Compass was published without fanfare and the team are still working out some kinks — as Nicole told me that there are some issues with the locations not working correctly.</p><p>While the world is going through some big news stories right now, restaurant reviews may seem trivial. But these three attempts to use newspaper archives teach us something about what it means to use AI in a news organization. Accuracy really matters. So glitches which spew out factual inaccuracies may cause delays in production. Or it may mean that you can’t go as broad as you would like with your content. Discoverability really matters. News organisations have been struggling to make sure their audiences get to see the full extent of their rich content — especially their archive — and are keen to experiment with new technology to do this. Finally, humans really matter. Tim’s team went for the human touch by centering around one food critic’s reviews. And spare a thought for Nicole and her team manually writing 160 restaurant blurbs. This feels like an irony that sometimes AI doesn’t take the human work away, it adds a lot more.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4ed4a75d21c2" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/the-newspapers-using-generative-ai-to-surface-restaurant-reviews-4ed4a75d21c2">The Newspapers Using Generative AI to Surface Restaurant Reviews</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Agentic Search for Investigative Journalism ]]></title>
<link>https://generative-ai-newsroom.com/agentic-search-for-investigative-journalism-43faa44ade99?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/43faa44ade99</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ agentic-ai ]]></category>
<dc:creator><![CDATA[ Evan Chen ]]></dc:creator>
<pubDate>Mon, 14 Jul 2025 14:03:09 GMT</pubDate>
<atom:updated>2025-07-14T14:03:08.824Z</atom_updated>
<content:encoded><![CDATA[ <h4><em>Developing a tool to enable analysis of TIF Spending in Chicago</em></h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*PcK_cuirUvLGvCMq" /></figure><p>In Chicago, Tax Increment Financing (TIF) districts publish annual reports in PDF form and detailed expenditure data in CSVs. Both formats contain valuable information — but parsing them, cross-referencing them, and answering even simple questions is often too labor-intensive for the average reporter.</p><p>Over the last couple months I built a prototype system that tries to bridge that gap: a command-line tool that uses an AI agent to understand user queries, search across both structured and unstructured data, and return a clean, well-sourced answer. You can ask it a question like “How much did Kinzie spend in 2023 and what were their goals?” and it will generate SQL to query the budget table, search the PDFs for planning documents, and summarize the results in a way that’s easy to understand.</p><p>The result is a functioning demo of agentic search applied to public finance: a system that combines SQL, PDF retrieval, and language model reasoning in a transparent pipeline.</p><h3>What Is TIF?</h3><p><a href="https://www.chicago.gov/city/en/depts/dcd/provdrs/tif.html">Tax Increment Financing (TIF)</a> is a redevelopment strategy used by districts in Chicago to direct investment into specific neighborhoods. Once a TIF district is created, any increased revenue from property taxes in that district over the next 23 years goes into a special fund that can be used to fund redevelopment projects in that district.</p><p>This was intended to allow the city to reinvest in infrastructure or subsidize development in areas that wouldn’t otherwise attract private capital. However in practice, TIF has drawn criticism for not being transparent enough about what the funds are being used for. Furthermore, TIF has been used in districts that were already very wealthy, causing further controversy.</p><p>Chicago has over 130 active TIF districts. Every year, the city publishes a long PDF report for each district, outlining major projects, planning goals, and funding decisions. It also publishes a detailed CSV of its expenditures.</p><h3>My Goals</h3><p>The core idea behind this project was to test whether a language model could answer questions that require both quantitative and qualitative reasoning synthesizing across both the unstructured documents as well as data files for a TIF district. Specifically, I wanted to see if a model could:</p><ul><li>Understand a database schema and generate valid SQL queries</li><li>Retrieve relevant passages from long, unstructured PDFs</li><li>Combine those results into a single, coherent, human-readable answer</li><li>Do all of the above transparently, showing its process</li></ul><h3>How It Works</h3><p>Currently, <a href="https://github.com/ecz2515/agentic-tif-search">the tool</a> is a terminal-based AI assistant for TIF data. It loads two main data sources:</p><ul><li>A <a href="https://duckdb.org/">DuckDB database</a> with structured expenditure data</li><li>A <a href="https://www.llamaindex.ai/">vector index</a> of all available TIF PDF reports, searchable via embedding similarity</li></ul><p>Once the system initializes, the user is prompted to type in a question. The model, powered by OpenAI’s <a href="https://platform.openai.com/docs/guides/function-calling?api-mode=chat">function-calling interface</a>, chooses among several custom functions that I’ve developed to support the use case:</p><ol><li>get_schema_info: Reads and remembers the structure of the SQL database</li><li>query_sql_database: Generates and runs a SQL query if the question is numeric</li><li>search_pdf_documents: Searches PDFs if the answer is not in the table</li><li>humanize_result: Synthesizes the result into a clear, readable response</li></ol><p>Each step is visible in the terminal, including the time taken and the exact SQL query or search string used. This gives the user some transparency into how the answer was generated.</p><h3>An Example Walkthrough</h3><p>To illustrate how the system works in practice, here’s an example:</p><p>Let’s say that a user asks: “What was Kinzie’s total spending in 2023, and what were their main goals for that year?” (Kinzie is a local neighborhood in Chicago)</p><p>The agent begins by translating the question into SQL. Specifically, it constructs a query that adds up every relevant expenditure category from the structured database:</p><pre>SELECT SUM(<br>  &quot;Cost of Studies&quot; + &quot;Administrative Cost&quot; + &quot;Marketing Sites&quot; +<br>  &quot;Site Preparation Costs&quot; + &quot;Renovation, Rehab, Etc.&quot; + &quot;Public Works&quot; +<br>  &quot;Removing Contaminants&quot; + &quot;Job Training&quot; + &quot;Financing Costs&quot; +<br>  &quot;Capital Costs&quot; + &quot;School Districts&quot; + &quot;Library Districts&quot; +<br>  &quot;Relocation Costs&quot; + &quot;In Lieu of Taxes&quot; + &quot;Job Training/Retraining&quot; +<br>  &quot;Interest Cost&quot; + &quot;New Housing&quot; + &quot;Day Care Services&quot; + &quot;Other&quot;<br>) AS total_spent<br>FROM expenditures<br>WHERE &quot;TIF District&quot; = &#39;Kinzie Industrial Corridor&#39; AND &quot;Report Year&quot; = 2023;</pre><p>Once the model retrieves the spending total, it initiates a second step: searching the PDF reports for Kinzie’s 2023 objectives. It creates a semantic query over the vector index for “Kinzie 2023 goals objectives”</p><p>The final answer, generated after both steps, is:</p><blockquote><strong>In 2023, Kinzie’s total TIF spending was $46,120,349.</strong></blockquote><blockquote>According to the amended redevelopment plan for the Kinzie Industrial Corridor, the district’s goals for 2023 included:</blockquote><blockquote><strong>Economic Growth</strong> — Maintain and expand the corridor as a major job center supporting businesses at all scales.</blockquote><blockquote><strong>Transportation Improvements</strong> — Upgrade multi-modal transit infrastructure for both workers and residents.</blockquote><blockquote><strong>Historic and Cultural Preservation</strong> — Encourage adaptive reuse of historic buildings and maintain district character.</blockquote><blockquote><strong>Mixed-Use Development</strong> — Promote equitable housing targets (30% affordable where required, 20% elsewhere), with a focus on design excellence.</blockquote><blockquote><strong>Local Employment</strong> — Increase job access by connecting developers with local job-training agencies.</blockquote><blockquote><strong>Tax Base Enhancement</strong> — Catalyze private investment to grow the city’s long-term property tax base.</blockquote><p>The plan also emphasized an Equity, Diversity, and Sustainability framework to ensure that TIF-funded projects include historically marginalized communities and maximize neighborhood benefits.</p><p>Behind the scenes, the system transparently logs which tools it used: SQL for the financial total, PDF search for the policy goals — giving users information about the provenance of each part of the answer.</p><h3>Key Takeaways</h3><p>This project revealed a few key takeaways. First, multimodal retrieval works pretty well. With only a few functions and a clear prompt, the LLM was able to intelligently route each question to the right data source — whether that meant executing a SQL query or retrieving information from PDFs.</p><p>Second, I found that you don’t really need much fine-tuning. OpenAI’s GPT-4o model was functional for understanding the database schema, and gave surprisingly strong results without requiring custom training or further adjustments.</p><p>Third, transparency helps debugging. Because every step of the analytic process is logged, including which function was called and how long it took, I could quickly diagnose failures, tweak system prompts, and understand behavior.</p><p>Finally, it’s clear that the model’s reasoning is still shallow. While it answers direct questions effectively, it doesn’t take initiative. It won’t propose follow-ups, raise uncertainties, or dig deeper without being asked. It responds to your question, but it doesn’t investigate further.</p><h3>What’s Next</h3><p>There’s still plenty of room to make the system more useful. On a technical level, things like better parameter control, smarter function chaining, and customizable user preferences could still go a long way. For example, letting users prioritize recent data or request more detailed sourcing. I could also improve how results are cited and linked, especially when answers are drawn from PDFs.</p><p>But the bigger question is whether the system can move beyond just answering questions. Can it flag anomalies on its own? Suggest better ways to phrase a query? Notice when something in the data doesn’t really add up? The end goal would be to build an agentic tool that can really help drive forward an investigation.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=43faa44ade99" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/agentic-search-for-investigative-journalism-43faa44ade99">Agentic Search for Investigative Journalism</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Benchmarking LLM Performance for Journalism ]]></title>
<link>https://generative-ai-newsroom.com/benchmarking-llm-performance-for-journalism-9831e6f918e4?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/9831e6f918e4</guid>
<category><![CDATA[ ai-in-journalism ]]></category><category><![CDATA[ ai-benchmarks ]]></category><category><![CDATA[ human-centered-ai ]]></category><category><![CDATA[ co-design-workshop ]]></category><category><![CDATA[ evaluation ]]></category>
<dc:creator><![CDATA[ Charlotte Li ]]></dc:creator>
<pubDate>Mon, 07 Jul 2025 15:32:54 GMT</pubDate>
<atom:updated>2025-07-07T15:32:54.090Z</atom_updated>
<content:encoded><![CDATA[ <h4>A recent workshop at Northwestern convened newsroom practitioners to outline how to evaluate LLMs for news tasks and align with journalistic values.</h4><p>By: <a href="https://medium.com/@zihaoli2027">Charlotte Li</a>, <a href="https://medium.com/@jeremygilbert">Jeremy Gilbert</a>, and <a href="https://medium.com/@ndiakopoulos">Nicholas Diakopoulos</a></p><p>Figuring out which generative-AI model to use can be intimidating. It can be dizzying trying to parse the grid of model names and scores. Each model seems to vie for bragging rights in things like “graduate-level reasoning”, “agentic coding”, “multilingual Q&amp;A”, and so on. Anthropic’s latest Claude release (shown below) is emblematic. Other benchmarks for performance include the popular <a href="https://lmarena.ai/">LM Arena</a>, which measures user preferences between model outputs. But as these various kinds of metrics proliferate, there’s a question that’s been nagging us: <em>Do any of these scores tell us which models we should use for journalism and when?</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*y6bq8MAor-B0j_R8" /><figcaption>Benchmarks shown in one of the recent <a href="https://www.anthropic.com/news/claude-4">Claude releases.</a></figcaption></figure><h3><strong>The GAIN Benchmarking Workshop</strong></h3><p>To try to address this question, the <a href="https://generative-ai-newsroom.com/">Generative AI in the Newsroom Initiative</a> convened 23 journalists to work through what a news benchmark tailored to journalism might look like. The workshop, held at Northwestern, aimed to sketch news-oriented benchmarks supported by the community. Such benchmarks might serve as a clearer compass for comparing and choosing generative AI models and systems appropriate to news use cases and inform tech companies about journalistic needs as they improve their models.</p><p>Workshop participants tackled questions such as: What matters to journalists in terms of <em>what</em> to measure? <em>How</em> might we measure those things with validity and realism with respect to practice? And what data would be needed to <em>implement</em> a rigorous yet practical benchmark? Participants worked through these questions in the context of six umbrella use cases: Information/Data Extraction, Semantic Search, Summarization, Content Transformation, Background Research, and Fact Checking; and six values that are typically important to the practice of journalism: Accuracy, Transparency, Confidence/Uncertainty, Accountability, Objectivity/Bias, and Timeliness/Recency. These selections were informed by our <a href="https://drive.google.com/file/d/1rXruz2wQLAXmUtzm1B7lJCpxdWbOHijS/view?usp=sharing">previous</a> <a href="https://nishalsach.github.io/pdfs/2025-vsd-journalism.pdf">research</a> and a pre-survey of the workshop participants.</p><h3><strong>What We Found</strong></h3><p>During the workshop, journalists raised a series of questions and difficulties for developing comprehensive evaluations for the use of AI tools in their respective organizations, including (1) technical issues around LLM use, (2) dataset construction specifically for journalism, and (3) balancing between generalizable evaluation and specific usage contexts.</p><p>In terms of the technical dimensions of models, the participants voiced concerns regarding the quick rate of deprecation of models, or the fact that their behavior can change over time. AI models seem to be constantly in flux, creating ongoing demands for evaluating each new iteration of models. Care also needs to be given to the possibility that newer models might improve performance only because test data might have been incorporated into training. Another technical challenge that emerged was that the right scope of analysis was unclear: it could make sense to test LLM models in some cases, but more often than not an evaluation needs to be of an entire AI system, which could even include aspects of the interface, parameter choices, and so on. And finally, issues with the consistency, reliability, and reproducibility of LLM outputs were frequently mentioned.</p><p>The journalists also raised concerns about the creation of datasets suitable for evaluation and that could also be shared as part of a public benchmark. While much data is collected and used in journalistic reporting, it is hard to identify a unique set that is <em>representative</em> <em>of all journalistic work</em>. Moreover, even if such a set of datasets exists, it is difficult to define ground truth answers to typical tasks that journalists perform — a multiple-choice setup, which is typical of many benchmarks, may not always make sense. To develop benchmark datasets that are representative of journalistic work and values, organizations need to invest editorial attention and time to detail tasks and label data according to a rubric that defines successful or unsuccessful completion of the task as well as alignment or violation of values. Yet many of the participants described obstacles in garnering editorial resources for this kind of evaluation.</p><p>The journalists found it difficult to translate context-specific uses of generative AI and preferences for particular outputs based on their journalistic instinct (e.g. “vibes-based” evaluation) into scalable and reproducible evaluations for general use. This challenge emphasizes the tension between <em>generalizability</em> and <em>specificity</em> for an evaluation. When prompted to think about tasks like “summarization” or “semantic search,” it is much easier to think through a specific story example for which these tasks are done, but harder to characterize success and failure for those tasks in general terms as they can differ substantially across each story, individual use case, and organizational context. Similarly, even definitions of core values, such as “accuracy” and “transparency,” can vary across different tasks. Moreover, these tasks can be broken down into more granular tasks within a workflow, where intermediate evaluations of these tasks are often necessary. For example, entity extraction could be a precursor task to tasks such as summarization — should the benchmark focus on the sub-tasks or only the larger, more aggregate task?</p><p>Based on the discussion of use cases and values, we identified a set of <em>variabilities</em> that influence journalism tasks and the evaluation standards for those tasks across the journalism sector. One of those variabilities is <em>internal versus external usage</em> of AI outputs — is the output of the AI system for journalists in the newsroom or will it be directly consumed by end-users (i.e. audience members). For instance, the process and preferences for summaries created during the reporting process for use by journalists would be quite different from summaries of articles created for a public audience. Factual accuracy matters for both types of summarization, but one might have different priorities for narrative tensions, simplicity of language, tone, and source attribution due to the difference in demands between internal and external uses. Another example of variability is the <em>type of content</em> being produced: a fact-based breaking news story might lean more heavily on values like timeliness and require specific outputs in fact-checking compared to a long-form narrative-based feature story, which might prioritize authenticity as the driving component of accuracy. Other dimensions of variability include the level of human oversight expected of a given task when put into production, as well as the modality of inputs expected (e.g. image, video, audio, text, etc.).</p><h3><strong>Next Steps</strong></h3><p>Journalistic work can vary greatly across different news organizations and individual journalists’ work contexts. Characterizing variabilities in this landscape helps start assessing the journalistic relevance of different generative AI systems. By understanding the variabilities among journalistic tasks, we can focus on constructing evaluations that cover as many different types of tasks as possible and match as much of the variability found in the real world as possible. This is important for addressing one of the key critiques of AI benchmarks, which is that they don’t measure things that actually matter in practice. With a deeper understanding of task variability, this would enable journalists to determine appropriate evaluation strategies for tasks at hand based on their specific context. Based on the workshop and analysis, we believe there are at least a couple of directions where the research community and journalism industry could move forward.</p><p>We could attempt to develop an<strong> expansive benchmark which includes many tasks with the understanding that it might be impossible to comprehensively capture <em>all</em> the nuances of how tasks can vary</strong>. But such a unified benchmark might help orient tech companies to consider journalistic perspectives in their development and could be used as a measuring stick for informing coarse decisions about which models tend to perform better across the board.</p><p>Or we might take an approach that diverges from this more top-down way of constructing benchmarks and recognize the limitations of trying to have a generalized benchmark, namely that it will be hard to align with all the nuances of practice and truly be representative of a Journalism (big “J”) Benchmark that measures enough of what matters to be deserving of that label. In this approach, we would support <strong>evaluations of models in specific journalism contexts by providing guidelines that help structure and make sense of specific situations of evaluation</strong>. Specifically, this approach could look like a set of templates for benchmarks (perhaps as computational notebooks to facilitate replication) that individual newsrooms can pick up and adapt to their local evaluation needs. This might largely lose the potential as a lever to steer big tech, but might ultimately be more useful in understanding generative AI utility across journalism’s many facets.</p><p>Regardless of the approach we choose to take with this project, the workshop underscores the importance of focusing the community’s attention on <strong>contributing datasets</strong> that capture different use cases of generative AI models in their specific contexts, and to <strong>invest editorial time and attention</strong> in evaluating generative models in newsrooms.</p><p>We’d like your feedback on next steps! Please <a href="https://docs.google.com/forms/d/1T6jQrzOPtdmuNes1fkkZ_VP9yIYR2CqWnzVuH6Sm1cU/preview">fill out this form</a> to indicate which of the two approaches you think would be more valuable and why, and also please consider sharing pointers to datasets that you think could be the basis for a specific benchmark — we would be happy to be in touch to consult and develop such data resources further.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9831e6f918e4" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/benchmarking-llm-performance-for-journalism-9831e6f918e4">Benchmarking LLM Performance for Journalism</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Personalizing Newsletters: A Case Study on Iterative Prompt Evaluation and Improvement ]]></title>
<link>https://generative-ai-newsroom.com/personalizing-newsletters-a-case-study-on-iterative-prompt-evaluation-and-improvement-a9f1141900fd?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/a9f1141900fd</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ personalization ]]></category><category><![CDATA[ llm ]]></category><category><![CDATA[ news ]]></category>
<dc:creator><![CDATA[ Ashlyn Wang ]]></dc:creator>
<pubDate>Mon, 30 Jun 2025 11:02:51 GMT</pubDate>
<atom:updated>2025-06-30T11:02:50.544Z</atom_updated>
<content:encoded><![CDATA[ <h4>Exploring a practical workflow for how targeted prompting can help refine AI-generated newsletter headlines</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZCYZ_CCCLQX4oxGmQJBnPA.png" /><figcaption><em>Image generated by OpenAI’s ChatGPT</em></figcaption></figure><p>In <a href="https://generative-ai-newsroom.com/personalizing-newsletter-headlines-testing-llms-against-human-editors-6f14135089da?gi=160560eb3700">past work</a>, we’ve explored using large language models (LLMs) to personalize the subject lines of email newsletters. Our results were promising: based on a sample of The Evening newsletter from the New York Times, we found that participants in a controlled study preferred AI-generated subject lines to human-written ones 62% of the time.</p><p>But to get from a controlled experiment to real-world use, we need to consider factors beyond reader preference. Newsrooms need audience-facing copy that is not only engaging but also accurate, robust, and stylistically consistent.</p><p>To explore those aspects further, we conducted a large-scale evaluation of the subject lines generated from our prior experiment. And while we found that AI-generated subject lines generally avoided hallucinations and stylistic errors, there were generated subject lines that would not be suitable for publication.</p><p>In the following sections, we delineate the evaluation process we used to identify these errors and the prompting strategies we implemented to improve model outputs. This process offers a potential roadmap for newsrooms looking to mitigate some of the weaknesses of LLMs via targeted prompting.</p><h3>Output Analysis</h3><p>We began by closely reviewing the existing outputs to better pinpoint possible improvements to the original prompt. During this review, we noticed several recurring problems: some headlines failed to intuitively reflect the core message of the underlying story, introduced factual inaccuracies, strayed off-topic, or risked confusing readers at a glance.</p><p>After reviewing the LLM-generated subject lines, we recognized several initial issues that may adversely impact comprehension and reading experience:</p><ul><li><strong>Overemphasis on minor details:</strong> Headlines fixate on secondary elements, overlooking the main point of the original stories.</li><li><strong>Introduction of extraneous content:</strong> Headlines include details or keywords not present in the source material.</li><li><strong>Irrelevant headline generation:</strong> The LLM produces headlines unrelated to any of the input stories.</li><li><strong>Inconsistency with house style:</strong> Headline formatting deviates from the New York Times’ stylistic standards (e.g., unnecessary capitalization).</li></ul><p>While any error is undesirable in reader-facing writing, in this review, we found that most of the LLM outputs were of reasonably high quality, with headlines that were brief, straightforward, and showed minimal stylistic and tonal variation. As such, we focused our prompt refinement on three high-impact objectives:</p><ul><li>Reduce the likelihood of hallucinated, misleading, inaccurate, or irrelevant headline generation.</li><li>Enhance consistency across multiple outputs.</li><li>Improve alignment with the publication’s editorial style and formatting conventions.</li></ul><h3>Prompt Iteration</h3><p>Following the identification of key concerns in the initial outputs, the next stage involved applying a core technique of prompt engineering: prompt iteration, a systematic process of revising and testing prompts repeatedly to improve the quality, reliability, and relevance of LLM-generated content.</p><figure><img alt="A cycle of iterative prompting for continuous improvement" src="https://cdn-images-1.medium.com/max/1024/1*6E8jPB-Ye4Qo-dO1as7B6g.png" /><figcaption>Cycle of iterative prompting for continuous improvement</figcaption></figure><p>To address the identified challenges, we developed prompt revisions drawing on insights gathered from each iteration. To keep track of how changes to the prompt shifted the LLM’s outputs, we used two tools. First, we employed a prototype built on <a href="https://streamlit.io/">Streamlit</a>, which allowed us to quickly iterate on prompt versions and model temperature for a small development sample. Then, we used <a href="https://chainforge.ai/">ChainForge</a>, a visual LLM evaluation tool, to build out a robust iteration pipeline and compare results at a large scale.</p><p>ChainForge enabled us to design a reproducible workflow for testing newsletter generation. We defined target users and their selected headlines, then ran multiple prompts that generated relevant newsletters for each user. We also created evaluations to measure how each variant of the headline-writing prompt affected our key outcomes, allowing us to refine the prompt based on observations across multiple samples simultaneously.</p><figure><img alt="An example of our ChainForge workflow" src="https://cdn-images-1.medium.com/max/1024/1*2KWtD-jmjZK65Jg0lW76yw.jpeg" /><figcaption>Example of our ChainForge workflow</figcaption></figure><p>Across repeated rounds of iteration and testing, we settled on a handful of targeted adjustments that made meaningful enhancements to the model’s outputs:</p><ul><li><strong>Structured Task Composition:</strong> Reorganized and broke down the complex headline-generation task into a clearly sequenced, step-by-step workflow with labeled sub-tasks and specific formatting instructions.</li><li><strong>Detailed and Explicit Instructions:</strong> Replaced general guidance with clearly defined, task-specific requirements to reduce confusion in model behavior.</li><li><strong>Clarified Content Extraction and Identified Prioritization:</strong> Framed the instructions as focused extraction that prioritizes the core message from the source material, rather than open-ended summarization, to enhance accuracy.</li><li><strong>Expanded Instructional Coverage (Dos and Don’ts):</strong> Built on the original prompt’s prohibitive guidance to develop a more complete and consistent set of prescriptive and prohibitive rules across task sections.</li><li><strong>Reinforced Style and Formatting Rules:</strong> Added additional instructions specifically guiding style-wise generations, in addition to providing examples of the publication’s intended house style.</li><li><strong>Specified Focus Classification:</strong> Included explicit classification criteria to prevent the model from elevating minor details to a central focus.</li><li><strong>Self-Checking:</strong> Added a final reflection step that prompts the model to review its output against all prior guidelines, encouraging self-correction before delivery.</li><li><strong>Failsafe for Imperfect Inputs:</strong> Anticipated cases where story alignment with reader preferences might be weak and offered fallback instructions.</li></ul><h3>Prompt Evaluation</h3><p>In this final step, we carried out both manual and automated comparative analyses between the original and refined prompt variants, using headline samples generated by the 2024-08-06 version of GPT-4o through ChainForge.</p><p>In line with our previously outlined goals, we evaluated six key aspects of the generated headlines: accuracy, hallucination, style, fluency, relevance, and consistency. To streamline the evaluation process, we applied the following guiding questions:</p><ul><li><strong>Accuracy:</strong> Does the headline align with the story’s main focus and framing? Could the wording mislead a general reader, even if factually correct?</li><li><strong>Hallucination:</strong> Does the headline include details, claims, or implications that are factually incorrect, nonexistent, or unsupported by the article?</li><li><strong>Style:</strong> Are there any major deviations from the publication’s intended style?</li><li><strong>Fluency:</strong> Is the text well-structured and easy to read?</li><li><strong>Relevance (automated evaluation):</strong> Does the headline reflect the user’s reading history and preferences, as indicated by selected stories and the interest profile?</li><li><strong>Consistency (automated evaluation):</strong> Are the headlines consistent across multiple generations of the same prompt?</li></ul><p>Overall, the new prompt corresponded with measured improvements, specifically a reduction in responses that failed to meet the defined criteria, for five indicators: accuracy, hallucination, style, relevance, and consistency. Fluency, on the other hand, remained consistently strong in both the original and revised versions.</p><p>In terms of the automated evaluation, we assessed temperature settings (0.5 vs. 1.0) and prompt variations (original vs. revised) against two performance indicators — consistency and relevance — for 150 headlines. Our results suggest that, although prompt refinement and temperature adjustments did not produce a statistically significant difference in headline relevance, they were associated with a statistically significant improvement in consistency (the lower temperature had a higher consistency).</p><p>In the manual evaluation, we analyzed 300 responses, 150 per prompt variant, using a mid-range temperature setting of 0.5. The evaluation results demonstrated that the revised prompt outperformed the original across three metrics: accuracy, hallucination, and style. It achieved 92.7% accuracy with 11 inaccuracies, while the original prompt lagged behind at 85.3%, producing 22 inaccuracies. For hallucinations, the revised prompt had a 6% rate (9 instances), compared to 10% (15 instances) for the original. In terms of style, the revised prompt fully aligned with the New York Times’ headline style; the original prompt came close but recorded a slightly lower 98% consistency, with three deviations. Both prompts were equally strong in fluency, with all outputs rated as clear and comprehensible.</p><figure><img alt="A chart comparing error rates for original and refined prompts across accuracy, hallucination, style, and fluency" src="https://cdn-images-1.medium.com/max/989/1*69LdFclVd0aIGZWTCSQBrg.png" /><figcaption>Comparison of error rates for original and refined prompts across accuracy, hallucination, style, and fluency</figcaption></figure><p>While the new prompt exhibited improvements in manual evaluations, the <a href="https://support.minitab.com/en-us/minitab/help-and-how-to/statistics/tables/supporting-topics/other-statistics-and-tests/why-should-i-use-mcnemar-s-test/">McNemar’s test</a> we conducted did not yield statistically significant differences across the three metrics, providing no evidence of consistently improved performance.</p><p>Interestingly, we noticed that certain stories were more likely to produce off-target responses across both the original and revised prompts. This suggests that the issue may also stem from factors beyond prompt construction, such as the input data. Because the LLM lacks direct access to the full text of the New York Times articles, its ability to generate responses that accurately reflect the original stories may be inherently constrained. Discrepancies between the actual story content and the newsletter headlines or descriptions supplied as inputs indicate a potential upstream bottleneck in the generation pipeline. Future improvements may be achieved by further refining the quality and relevance of input data.</p><h3>Limitations</h3><p>Language models can be highly sensitive to the phrasing of prompts, and even slight modifications may lead to inconsistent or unpredictable results. Although prompt tuning can improve surface-level features such as clarity and alignment, it does not necessarily resolve deeper challenges, including nuanced misrepresentations or framing biases.</p><p>Due to the inherent brevity of newsletter headlines, our simplified methodology focused primarily on straightforward refinements and specific issues like factual accuracy and clarity. Nevertheless, this level of simplification carries the risk of producing outputs that are overly formulaic and less adaptive to varied editorial styles, failing to capture the complexity or stylistic richness in more comprehensive writing formats.</p><p>And in practice, the rising use of LLMs in newsrooms has not eliminated the lingering concerns regarding the validity of their outputs. A recent <a href="https://generative-ai-newsroom.com/how-to-check-if-ai-assistants-are-distorting-your-news-stories-1fd7d2002a1c">BBC research</a> article found that AI assistants can “distort news articles” and “introduce factual errors.” Even though the accuracy of LLM-generated content can be improved through various techniques, editorial oversight remains as essential as ever, just as it was before these technologies became prevalent in the newsroom.</p><h3>In Closing</h3><p>In this work, we explored a structured, hands-on approach to prompt refinement and evaluation. Our findings point to the potential of targeted prompt adjustments in supporting the performance of LLMs in generating effective newsletter headlines through output-informed input strategies, moving beyond general prompting rules. In other words, close analysis of model outputs helps identify specific issues requiring adjustment, which can then be further refined through iterative prompt revisions and evaluations. Providing additional instructions and context grounded in previous output quality can sometimes create a more nuanced prompting flow.</p><p>Despite its limitations, this process offers journalists a practical model and method for continuously improving and assessing the prompts they use to guide LLMs in news-related writing. As newsrooms incorporate models into their editorial pipelines, maintaining human oversight not only over the outputs — the content generated — but also over the inputs — the cues that influence model responses — can help build a more reliable and systematic AI-assisted operation.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a9f1141900fd" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/personalizing-newsletters-a-case-study-on-iterative-prompt-evaluation-and-improvement-a9f1141900fd">Personalizing Newsletters: A Case Study on Iterative Prompt Evaluation and Improvement</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Semantic Search in a Box: Microsoft’s NLWeb for News Publishers ]]></title>
<link>https://generative-ai-newsroom.com/semantic-search-in-a-box-microsofts-nlweb-for-news-publishers-8dd345735e27?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/8dd345735e27</guid>
<category><![CDATA[ search ]]></category><category><![CDATA[ llm ]]></category><category><![CDATA[ open-source ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ news ]]></category>
<dc:creator><![CDATA[ Nick Hagar ]]></dc:creator>
<pubDate>Mon, 23 Jun 2025 11:01:37 GMT</pubDate>
<atom:updated>2025-06-23T11:01:36.781Z</atom_updated>
<content:encoded><![CDATA[ <h4>A new open source framework from Microsoft delivers the power of modern semantic search without the high costs and vendor lock-in of traditional “black box” solutions.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*JATGrK3WYZxt6pnt" /><figcaption>Microsoft’s NLWeb offers publishers a way to build semantic search on top of their coverage.</figcaption></figure><p><em>Semantic search</em> has gotten a lot of buzz as a promising LLM use case for publishers. Instead of just matching keywords, this technology uses a sophisticated understanding of language to grasp the deeper meaning behind a user’s query. A search for “vegetarian recipes,” for example, will correctly surface articles featuring tofu and vegetables while filtering out those with chicken — all without the user having to type those specific ingredients. This lets your audience search in natural, conversational language, freeing them from the frustrating guesswork of finding the right keywords.</p><h3>From “Nice-to-Have” to Revenue Driver</h3><p>Mounting anxiety over platform “resets” is pushing newsrooms to find new ways to keep audiences engaged, and semantic search is emerging as a core part of the solution. As the Reuters Institute’s <a href="https://reutersinstitute.politics.ox.ac.uk/digital-news-report/2025">2025 Digital News Report</a> highlights, publishers are increasingly investing in their own AI-powered search layers to protect their traffic. The results can be dramatic: <a href="https://wan-ifra.org/2025/02/how-denmarks-ekstra-bladet-used-ai-to-boost-subscriptions-by-35/">Denmark’s Ekstra Bladet</a>, for instance, saw a 120% jump in related-article views and a 35% increase in subscription conversions after deploying its AI-driven features.</p><p>Of course, the leap from keywords to true semantic search is fraught with challenges. Implementation is complex and expensive, and generative AI adds new layers of difficulty. Suddenly, publishers must navigate the risk of vendor lock-in with a specific LLM, design for sophisticated conversational and agentic interfaces, and confront the reality that LLM API calls can be just as costly as legacy search infrastructure.</p><h3>Enter NLWeb: A Protocol for the Open Web</h3><p>Microsoft’s answer to these challenges, unveiled at its Build conference in May, is <a href="https://news.microsoft.com/source/features/company-news/introducing-nlweb-bringing-conversational-interfaces-directly-to-the-web/"><strong>NLWeb</strong></a> (short for Natural Language Web). While it offers a powerful shortcut for adding semantic search to a website, at its core, NLWeb provides a backend pipeline that you control and integrate. Content is stored in a database that you control locally (or in your cloud provider), and interactions with LLMs happen via their APIs, which are excluded from training data by <a href="https://privacy.anthropic.com/en/articles/7996868-is-my-data-used-for-model-training">many</a> <a href="https://platform.openai.com/docs/concepts">providers</a>. This allows for a comprehensive protocol for querying any collection of content with natural language, <em>without</em> surrendering your data to a third-party model.</p><p>The project’s practical power became immediately clear when I built a<a href="https://nlweb-gain.onrender.com/"> prototype implementation</a> using the GAIN blog archives (<a href="https://github.com/NHagar/NLWeb">code here</a>). The setup required minimal configuration; I was able to ingest the entire site from a simple RSS feed. This hands-on experience was convincing: NLWeb closes the gap between the complex promise of AI and the practical need for a fast, controllable, and high-quality search solution that a publisher can actually deploy.</p><h3>How It Works: Powerful Search Out of the Box</h3><p>A key strength of NLWeb is the control it gives publishers over the user experience. Instead of a one-size-fits-all approach, it provides the components to craft different interfaces tailored to specific user needs. The reference implementation enables three distinct operational modes you can deploy immediately:</p><p><strong>Enhanced Document Retrieval:</strong> This mode offers a sophisticated take on traditional search. Users get a ranked list of relevant articles, but each result is enhanced with a relevance score for sorting and an AI-generated explanation for its inclusion. It’s an experience designed for discovery, empowering users to find the best source material for their needs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*h2caK6UVeDxZ5Z4Y" /><figcaption>A list of search results, with relevance scores and explanations generated by NLWeb</figcaption></figure><p><strong>Direct Answer Generation (RAG)</strong>: For users who prefer asking questions over sifting through links, NLWeb offers a retrieval-augmented generation (RAG) mode. It synthesizes information from source articles to provide a direct, conversational answer and remembers context for follow-ups, mimicking a chatbot experience. And it cites its sources, allowing readers to verify the chatbot’s responses.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HHW8lBZyIizmnYds" /><figcaption>A chatbot answer in response to a reader query, grounded in GAIN articles</figcaption></figure><p><strong>Hybrid Summary and Search:</strong> This mode presents a ranked list of articles prefaced by an AI-generated summary of the key takeaways. This gives users a quick cognitive offload — the gist of the information — while still providing easy access to the underlying sources.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*-L1dCjniYPa7RjZY" /><figcaption>A hybrid response, with an LLM-generated summary and individual articles</figcaption></figure><p>Ultimately, this flexibility allows a publisher to choose how to mediate the relationship between their content and their audience — whether to act as a librarian pointing to the right shelf, an expert providing a direct answer, or a curator offering a quick summary.</p><h3>Under the Hood: A Three-Step Pipeline</h3><p>Behind the scenes, NLWeb operates on a straightforward three-stage pipeline designed for simplicity, cost-efficiency, and publisher control.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*q_7DMoovNSMs35Ph" /></figure><h3>Ingest: Standardized Item Collections</h3><p>First, NLWeb ingests your content. To cut down on complexity, it standardizes this process using <a href="http://schema.org">schema.org</a> conventions. For my prototype, I mapped the GAIN blog posts to the <em>NewsArticle</em> schema. This structured data — including content, metadata, and AI-generated embeddings — is then stored in a vector database, creating a standardized, queryable library of your content. I used OpenAI’s <a href="https://platform.openai.com/docs/models/text-embedding-3-small">text-embedding-3-small</a> model to create embeddings, but you can use any embedding model.</p><h3>Process: Efficient, Low-Cost LLM Logic</h3><p>Next, when a user sends a query, NLWeb assesses relevance, scores results, and generates explanations. To directly address the high cost of AI, this entire process is handled by default with parallel calls to a small, fast, and inexpensive model like GPT-4o-mini, ensuring that responses are both high-quality and affordable at scale.</p><h3>Serve: A Flexible, Headless API</h3><p>Finally, NLWeb delivers its results via a flexible API. It is a “headless” system, meaning it has no built-in, mandatory UI. This design choice gives publishers complete control to connect the search backend to any frontend they choose, whether it’s a simple on-site search box, a mobile app, or a custom conversational interface. For my prototype, for example, I built a <a href="https://github.com/NHagar/NLWeb/tree/main/static">simple single-page interface</a> consisting of an HTML document and some Javascript to render search results.</p><h3>An Open Architecture</h3><p>The flexibility of NLWeb points to its ultimate ambition: it’s not just a tool for site search, but <strong>a protocol designed for the open web</strong>. The modular architecture is the foundation for a much broader set of capabilities that go beyond a single website or content type.</p><p>This protocol-driven approach unlocks several powerful use cases:</p><ul><li><strong>Connecting with the Agentic Web:</strong> NLWeb includes a built-in MCP (<a href="https://modelcontextprotocol.io/introduction">Model Context Protocol</a>) server. This provides a dedicated, non-scraping pathway for external AI agents to intelligently query your content, making your site a first-class citizen on the agentic web. For example, <a href="https://claude.ai/share/3d4563de-311a-4489-91bc-5195ac2ed1cb">here’s a query</a> to my GAIN NLWeb prototype sent via the MCP from Claude. Such MCP servers could represent a new product offering for news organizations that might, for instance, enable subscriptions to content so it can be integrated into LLM responses.</li><li><strong>Unified Search Across Offerings:</strong> The protocol is content-agnostic. It can handle news articles, but also recipes, product reviews, or events using their respective schema.org definitions. And while NLWeb is set up for text content out of the box, a newsroom could plug their instance into multimodal embedding and language models to handle podcasts, videos, or non-OCRed documents. This allows a publisher with a diverse bundle of offerings — like The New York Times with News, Cooking, and Wirecutter — to provide a single, unified search interface for their entire ecosystem.</li><li><strong>Personalization Through Memory:</strong> The protocol also includes a reference implementation for storing “memories” about users. With user consent, this allows for deeper personalization. If a reader mentions a dietary restriction or a specific interest, that context can automatically inform all future search results, creating a stickier and more helpful user experience.</li></ul><h3>A Tool for the Newsroom?</h3><p>The utility of semantic search isn’t limited to audience-facing features; it’s also helpful for internal newsroom workflows. While many <a href="https://github.com/jeremybmerrill/meaningfully">open-source</a> <a href="https://github.com/freedmand/semantra">tools</a> excel at exploring unstructured document dumps, NLWeb is uniquely suited for turning a collection of documents (or images, or audio clips) into a durable and searchable newsroom asset.</p><p>The key is NLWeb’s emphasis on structure. It’s not the right tool for pointing at a folder full of messy PDFs to enable search. Instead, it shines when a newsroom wants to build a long-term, queryable database from thousands of standardized records — like police reports or lobbying disclosures — or create a searchable archive of its own past reporting to surface historical context.</p><p>For these use cases, NLWeb offers distinct advantages:</p><ul><li><strong>Privacy and security:</strong> A journalist can easily connect NLWeb to a locally-run LLM, ensuring that sensitive documents in an investigation are never sent to an external provider.</li><li><strong>Verifiability:</strong> Every result is explicitly tied back to a source document, reducing the risk of untraceable AI-generated “hallucinations” and keeping the focus on primary source material.</li><li><strong>Customizable investigation:</strong> A reporter can tailor a specific search implementation for a major project with zero ambiguity about how the ranking or filtering works — a transparent alternative to trusting a third-party black box.</li></ul><h3>Conclusion</h3><p>Publishers are often forced to choose between building costly, complex infrastructure or ceding control to third-party platforms. NLWeb presents a compelling third option. It provides an open framework that transforms a publisher’s core asset into an intelligent, interactive experience they own and control.</p><p>The system is designed to balance the immediate need for a high-quality user experience with the long-term strategic necessity of an adaptable, future-proof tech stack. Rather than operating at the whims of opaque algorithms and AI agents, NLWeb offers a clear blueprint for publishers to flexibly configure and provide natural language interfaces to their content.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8dd345735e27" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/semantic-search-in-a-box-microsofts-nlweb-for-news-publishers-8dd345735e27">Semantic Search in a Box: Microsoft’s NLWeb for News Publishers</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Can AI Triage a Pitch? A Case Study in Editorial Evaluation from Minority Africa ]]></title>
<link>https://generative-ai-newsroom.com/can-ai-triage-a-pitch-a-case-study-in-editorial-evaluation-from-minority-africa-beb864444f9e?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/beb864444f9e</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ editorial-triage ]]></category><category><![CDATA[ evaluation ]]></category>
<dc:creator><![CDATA[ Caleb Okereke ]]></dc:creator>
<pubDate>Wed, 18 Jun 2025 11:01:38 GMT</pubDate>
<atom:updated>2025-06-18T11:01:37.896Z</atom_updated>
<content:encoded><![CDATA[ <h4><em>With too many pitches and too few editors, we built an AI model to help us “decide” what stories to tell. Here’s what we learned and how we’re evaluating it.</em></h4><p>I founded <a href="https://minorityafrica.org/">Minority Africa</a> around a bold premise: that journalism could better reflect the complexity of African identities by centering marginalized voices. That has stayed constant through the years even as so much has changed. When we put out our first call for pitches in 2019, only a handful of pitches came in over six months. We were able to quickly review them and commission some of our very first stories from <a href="https://minorityafrica.org/women-only-parties-nigeria/">Nigeria</a> and <a href="https://minorityafrica.org/uganda-beauty-pageant-helping-to-fight-skin-cancer-among-persons-with-albinism/">Uganda</a>. Fast forward to 2025 and we are receiving 10x more pitches than we did in our early days after launch. For instance, in the first quarter 2025, we received almost 200 pitches, an amount that would easily overwhelm our core text editorial team of three people including myself.</p><p>With a lean editorial team, we faced a challenge familiar to many digital publications: how do you maintain depth, fairness, and attention when the volume scales but the staff doesn’t? To solve this, we’ve turned to a variety of different means, hosting pitch clinics and town halls, moving to a form-only system which means we can better track pitches than on email, and more recently, we’ve experimented with AI.</p><p>The rapid rise of generative AI in journalism has brought significant attention to its applications in <a href="https://doi.org/10.48550/arXiv.2406.13706">content production</a>. These content use cases range from <a href="https://doi.org/10.48550/arXiv.2301.13848">summarizing articles</a> to generating <a href="https://doi.org/10.48550/arXiv.2310.10706">headlines</a>, translating copy, and <a href="https://scholar.harvard.edu/sites/scholar.harvard.edu/files/ggondwezunda/files/exploring_the_multifaceted_nature_of_generative_ai_in_journalism_studies.pdf">drafting full-length stories</a>. While much of the conversation has centered on content-generation tools, we asked a more upstream question: could generative AI meaningfully support editorial triage, the everyday decision-making that determines which ideas move forward and why?</p><p>This curiosity led to the development of “<em>Iraka</em>,” (which means “voice” in <em>Rutooro</em>), a custom internal <a href="https://help.openai.com/en/articles/8554397-creating-a-gpt">GPT tool</a> trained on anonymized past submissions and our own editorial decisions to triage and review incoming pitches. We began building Iraka in February 2025, first by extracting pitch data from our Notion workspace, where submissions and editorial decisions had been tracked. From a corpus of more than 2,500 historical pitches, we selected an initial sample of 120 entries that contained clearly documented editorial outcomes and feedback and that were not for first person stories/essays. The dataset was manually anonymized to remove names, locations, and other identifying details. ChatGPT was then used to standardize the entries for consistency, preserving only three key fields: the pitch text, the editor’s comment, and the final decision (approve, reject, or needs development).</p><p>Using OpenAI’s GPTs platform, we built a custom chatbot and primed it with a carefully constructed prompt reflecting our editorial values, decision criteria for pitch approval, and tone. It was trained to evaluate pitches through the lens of <em>Minority Africa</em>’s mission (prioritizing originality, representation, feasibility, and framing) and to mirror the language and developmental tone found in past editorial comments. The chatbot returns a decision prediction, score out of “5”, and tailored feedback.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*F_4wJuyubSYgqJo-UEZ9Yg.png" /><figcaption><em>AI-Generated Editorial Feedback vs. Final Published Story. This side-by-side comparison shows Iraka’s response to a pitch alongside </em><a href="https://minorityafrica.org/the-country-has-rejected-me-the-nigerians-born-and-bred-in-cameroon-yet-not-cameroonians/"><em>the final story as it appeared on Minority Africa</em></a><em>. The tool accurately identified the pitch as aligned with editorial priorities, emphasizing statelessness, xenophobia, and legal exclusion, and suggested further deepening through human rights framing.</em></figcaption></figure><p>Broadly speaking, one of the tool’s clearest limitations as pointed out by our editors is its tendency to treat the inclusion of a minority group as inherently valuable, without examining how that group is being represented. It frequently assumes that any mention of a marginalized identity signals narrative merit, overlooking whether the framing is reductive, overused, or in need of ethical scrutiny. This results in a pattern where pitches with tokenistic or stereotypical angles receive favorable assessments, while more nuanced editorial concerns, like narrative saturation or extractive storytelling, go unnoticed</p><p>Part of experimenting with the tool has involved determining how to meaningfully capture and evaluate its performance. To that end, we’ve developed a layered evaluation strategy structured around three key components: performance benchmarking, cross-model comparison, and workflow efficiency tracking.</p><h4><strong>Performance Benchmarking</strong></h4><p>In the performance benchmarking aspect of evaluation, we’re testing the model to see how it performs on pitch data it has never encountered before (See figure above). To do this, we isolated a subset of 50 pitches (from 2022 and 2023) that were not part of the knowledge base provided to the GPT. We then ran these through the model to assess: (1) Whether the AI correctly predicted approval or rejection, (2) How well its suggested feedback aligned with the actual editorial comments, and (3) Where and how that feedback diverged, especially in edge cases such as with bold ideas, underdeveloped potential, or emotionally driven narratives.</p><p>The results from this performance benchmarking revealed a nuanced picture. The model was correct with respect to editorial decisions in about 70% of cases, demonstrating a relatively strong ability to reproduce past outcomes in terms of “accept”, “needs development”, or “reject” decisions. However, feedback alignment was significantly lower, with only about 30% of the AI’s decision suggestions mirroring the tone, framing, or developmental style of human editorial comments.</p><p>Part of this may be due to how editorial outcomes were labeled in the training data. Classifying pitches as simply “accepted,” “rejected,” or “needs development” flattens a more complex decision-making process in which many pitches shift categories through discussion, negotiation, or revision. These gray zones are difficult to encode, and the model’s outputs appear to reflect the ambiguity inherent in those editorial thresholds.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*fya5b7ewCjkXPRwKi8RnIQ.png" /><figcaption><em>AI Alignment with Editorial Judgments. This chart compares how often the AI model aligned with human editorial decisions versus editorial feedback. While the model matched editorial decisions in 70% of cases, alignment with the tone and style of human feedback occurred only 30% of the time — highlighting a significant gap between decision replication and editorial nuance.</em></figcaption></figure><p>When examined together, these two dimensions offered more insight: in 50% of cases, the model’s decision matched the editor’s, but the feedback diverged; in 20%, both decision and feedback aligned; in another 20%, both diverged; and in the remaining 10%, the model offered aligned feedback despite making a different decision. These patterns were especially pronounced in edge cases — pitches that were ultimately accepted after significant revision or debate — where the model leaned conservative or offered vague suggestions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*dwGDO6E8G8RY4QXAbdIpkQ.png" /><figcaption><em>Combined Alignment Patterns Between AI and Human Editors. This chart illustrates the distribution of alignment across editorial decisions and feedback. It shows that while the AI frequently matched editorial outcomes, alignment on both decision and feedback occurred in only 20% of cases. The most common scenario — 50% — involved the AI matching the decision but offering divergent feedback.</em></figcaption></figure><p>Building on these insights, one next step in our evaluation is to move beyond binary alignment and adopt a graded feedback alignment scale. Currently, we assess feedback as either aligned or not, but this oversimplifies a spectrum of partial matches, near-misses, and stylistic echoes. For example, the model might capture the core issue an editor noted but miss the tone or context in how it was conveyed. A scaled evaluation would allow us to better differentiate between surface-level mismatches and deeper conceptual gaps, offering a more accurate picture of the tool’s editorial fluency. This approach would also help guide fine-tuning efforts, not just to increase overall alignment, but to calibrate the model’s outputs to match the nuance, intentionality, and voice of human feedback.</p><h4><strong>Cross Model Comparison</strong></h4><p>To further interrogate the model’s performance, our next evaluation step was comparative. We conducted an initial cross-model comparison between OpenAI’s GPT and Google’s Gemini. When tested on the same set of pitches without access to Minority Africa’s internal decision data and with a similar prompt as in GPT, Gemini defaulted more often to rejection, indicating a more risk-averse stance. However, even in cases where its final decision differed, Gemini’s comments frequently mirrored the editorial concerns noted by human editors.</p><p>Interestingly, the two models — GPT and Gemini — aligned on decision outcomes in approximately 73% of the cases, but diverged in tone, confidence, and editorial framing. This suggests that while core judgments may converge, each model exhibits distinct editorial styles of feedback. This distinction becomes especially clear when comparing how each model responded to the same pitch (see next figure), where GPT opted for a developmental path while Gemini recommended rejection, despite identifying similar editorial issues.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*QXCROB1baZT6wsdQBJrcXg.png" /><figcaption><em>Cross-Model Comparison on a Single Pitch. When presented with the same pitch, GPT suggested a “Needs Development” decision with narrative-focused feedback, encouraging reframing around personal experience and social context. Gemini, by contrast, issued a “Reject” decision with a more categorical tone, citing concerns about promotional framing and a lack of systemic analysis. This example illustrates a key finding from our cross-model evaluation: while both models identified similar weaknesses that aligned with editorial feedback, Gemini adopted a more risk-averse and final posture, whereas GPT leaned into developmental possibility.</em></figcaption></figure><h4><strong>Workflow Efficiency</strong></h4><p>Our final evaluation area is workflow efficiency. Here, we’re measuring how the tool impacts the time editors spend triaging pitches compared to our traditional process, with particular attention to whether it reduces redundant back-and-forth communication. Although our commissioning slowed earlier this year, we are regaining momentum and using this transition to more precisely track changes in pitch turnaround time. Currently, our editorial guidelines indicate a turnaround time of about 30 days; one of our goals is to see if the tool can help shorten that window without compromising the quality or fairness of reviews.</p><p>As of now, editors are using <em>Iraka</em> individually to provide a first-pass on submissions, testing its utility alongside regular human review. Every pitch is still manually read, and no editorial decisions are made solely based on the model’s output. This staged integration allows us to observe how the tool fits into existing workflows without disrupting the editorial process.</p><h3>Next Steps</h3><p>Looking ahead, we plan to expand the dataset significantly to improve model performance, moving beyond the initial 120 structured entries to include a broader and more representative sample of past pitches. We’re also exploring the possibility of opening the tool up to writers in a limited capacity, enabling contributors to receive preliminary feedback before formal submission. In parallel, we’ll refine the model’s prompt and behavior using insights from editor evaluations, manual coding of outputs, and ongoing tracking of workflow efficiency.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=beb864444f9e" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/can-ai-triage-a-pitch-a-case-study-in-editorial-evaluation-from-minority-africa-beb864444f9e">Can AI Triage a Pitch? A Case Study in Editorial Evaluation from Minority Africa</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Rethinking Model Collapse and What that Means for the Value of News Data ]]></title>
<link>https://generative-ai-newsroom.com/rethinking-model-collapse-and-what-that-means-for-the-value-of-news-data-7418628bc142?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/7418628bc142</guid>
<category><![CDATA[ model-collapse ]]></category><category><![CDATA[ news ]]></category><category><![CDATA[ genai ]]></category>
<dc:creator><![CDATA[ Nick Diakopoulos ]]></dc:creator>
<pubDate>Mon, 16 Jun 2025 11:01:41 GMT</pubDate>
<atom:updated>2025-06-16T11:41:42.033Z</atom_updated>
<content:encoded><![CDATA[ <p>The term “model collapse” garnered attention starting in 2023 when the pre-print of a <a href="https://www.nature.com/articles/s41586-024-07566-y">Nature paper</a> on the topic was posted online. Briefly, the idea of <em>model collapse is that the output of AI models can degrade over time if each successive version is trained on the last version’s outputs</em>. Collapsed models lose their ability to reflect variability, resulting in less diverse, more homogeneous, or even degenerate or unrecognizable outputs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*CByIVwMC74Z2su1g" /><figcaption>An example of an image generation model that has collapsed and is only able to generate the same similar looking face. Source: <a href="https://arxiv.org/abs/2404.01413">Gerstgrasser et al.</a></figcaption></figure><p><a href="https://www.nytimes.com/interactive/2024/08/26/upshot/ai-synthetic-data.html">Popular</a> <a href="https://gizmodo.com/ai-learning-from-its-own-nonsense-might-just-self-destruct-experts-warn-2000478095">coverage</a> quickly amplified the worry that, as synthetic data multiplies, large language models (LLMs) could progressively “forget” the richness of the real world. Because the stock of high-quality human prose is finite, with <a href="https://arxiv.org/abs/2211.04325">one estimate</a> that its open supply on the web could be exhausted by the end of the decade — it seemed that the news industry was in fact sitting on a treasure trove that the tech companies would need to pay for to keep scaling up their models: fresh, human-authored data.</p><p>The stats check out: model collapse is real. Shumailov and collaborators show in their Nature paper that model collapse can and does occur when each new version of a model is trained exclusively on data that is synthetically generated from the last version of the model. But <a href="https://arxiv.org/abs/2503.03150">newer scholarship</a> has criticized the assumptions in that research from a more pragmatic perspective.</p><p>In particular <a href="https://arxiv.org/abs/2404.01413">Gerstgrasser and colleagues’ work </a>shows that the phenomenon materializes only under the extreme assumption that each generation of a model is trained <em>exclusively</em> on the previous generation’s synthetic output. Once even a modest fraction of genuine human data is re-introduced, the degeneracy abates. Realistic data pipelines interleave curated human data, filtered synthetic data, and — especially at deployment time — retrieval-augmented generation that grounds answers in live external sources. In short, collapse is highly unlikely under real world conditions.</p><p>The upshot is that model developers do need to preserve some stock of human-generated data to sit alongside any synthetic data they use for training. There is indeed <em>some</em> value in news data for training new models. However, we should also put the volume of data needed for pre-training models in context. Meta’s most recent open-weights model, <a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Llama 4</a>, was trained on more than 30 trillion tokens. The New York Times <a href="https://advertising.nytimes.com/products/print/">advertising page</a> says they publish 150 articles a day. If we roughly assume each article is about 1000 tokens (about 750 words), 10 years of the paper would amount to about 550 million tokens, or about 0.0018% of the volume of the training dataset of Llama 4. That’s a really really tiny slice of pie. Despite its high quality, the volume of news data available for training new LLMs, even in aggregate across the entire industry, is paltry. Providing data for pre-training new models is <em>not </em>where the value of news data lies.</p><p>Instead of volume, the value of news data is in its <em>freshness</em>. Generative AI models all go out of date very quickly if they lack current data that keeps up with how the world is changing. This is where news organizations can negotiate with AI platforms based on value. It’s the <em>news gathering</em> function of news organizations that produces freshness: collecting up-to-date information out in the world that computers don’t already have access to (and people want to know about) and which is changing and evolving with the pulse of humanity. News organizations should be leveraging generative AI to enhance this core news gathering function, increasing its scale, scope, and comprehensiveness. Leaning into that defensible and strategically valuable position could just earn news organizations a seat at the table in the future AI-oriented information ecosystem.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7418628bc142" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/rethinking-model-collapse-and-what-that-means-for-the-value-of-news-data-7418628bc142">Rethinking Model Collapse and What that Means for the Value of News Data</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Can Sophina Turn Me Into a TikTok Star? ]]></title>
<link>https://generative-ai-newsroom.com/can-sophina-turn-me-into-a-tiktok-star-930f9c489408?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/930f9c489408</guid>
<category><![CDATA[ content-transformation ]]></category><category><![CDATA[ tik-tok ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Wed, 11 Jun 2025 11:02:36 GMT</pubDate>
<atom:updated>2025-06-11T11:02:36.065Z</atom_updated>
<content:encoded><![CDATA[ <h4>A TikTok influencer has made an app which uses generative AI to help you film videos based on your text articles.</h4><figure><img alt="Clare Spencer reads an autocue from the Sophina app on her phone screen." src="https://cdn-images-1.medium.com/max/869/0*bSPRmNzPLo3Uy4Ow" /></figure><p>Back in April, on the sidelines of the International Journalism Festival, I took part in some user testing that blew me away. Outside a cafe in Perugia, in Italy, <a href="https://www.tiktok.com/@sophiasmithgaler">TikTok star Sophia Smith Galer</a> held out her phone for us to take down her QR code to download the prototype of her new app, Sophina, which will be released more broadly later this month.</p><figure><img alt="Sophie Smith Galer, Right, holds out her phone to four people taking a picture with their phones of her screen on a side street in Perugia, Italy" src="https://cdn-images-1.medium.com/max/1024/0*hHT9iz8eYbf_scBQ" /><figcaption><em>Sophie Smith Galer. Right, shares a QR code to download the Sophina app at an ad hoc user testing session on a side street in Perugia, Italy.</em></figcaption></figure><p>Sophina uses generative AI to help you turn your text article into a script for a short video, for TikTok, Facebook Reels and the like, and then record it by acting as a teleprompter. Less than five minutes after downloading the app, out on the busy street, I — a camera-shy text journalist — had recorded a video based on <a href="https://generative-ai-newsroom.com/is-this-the-beginning-of-the-end-for-ai-summarized-news-196432290e9e">my GAIN article about Apple’s inaccurate news summaries</a>. I wasn’t quite set up to post it so, a month later, back in my kitchen, I decided to spend two hours with the app to see how many short videos I could post.</p><figure><img alt="Sophia Smith Galer’s TikTok profile which shows she has more than 17 million likes and 18 million views. Credit: Clare Spencer" src="https://cdn-images-1.medium.com/max/1024/0*V7K2CUpO-wAAOKiV" /><figcaption>Millions have watched Sophia’s TikTok videos and I hoped her app would give me that huge audience.</figcaption></figure><p>I wasn’t completely new to making TikTok videos. In lockdown I had shared some videos of writing <a href="https://en.wikipedia.org/wiki/Teeline_shorthand">Teeline shorthand</a>. I hadn’t made TikTok video where I talked to the camera before as I was worried I was too stuffy, would stumble, stutter, and forget.</p><p>Thankfully, Sophina gave me what Lydia Chilton refers to as “activation energy”. The Assistant Professor in the Computer Science Department at Columbia University carried out<a href="https://arxiv.org/pdf/2502.05347"> research</a> into student journalists using another AI tool, called ReelFramer, to help convert text articles into TikToks. Sophina, and ReelFramer, reduced the friction involved in moving from writing long text articles to making short videos.</p><figure><img alt="Two screens from the Sophina app, the first shows the interface within which a user pastes their text article and the second says “select hook” and shows a cartoon of a woman with the text “analysing script for the perfect hooks”." src="https://cdn-images-1.medium.com/max/1024/0*WZEWS9QcrxaYD_tk" /><figcaption>Sophina was easy to use and the interface was clean and fun.</figcaption></figure><p>I’m impressed with how Sophia identified a valuable point in the workflow to implement generative AI: the rewriting of a text article into the first draft of a video script. Importantly, Sophina doesn’t attempt to replace the human’s voice and face. This is something I had not managed to do when I was a product manager for a similar prototype last year. My prototype Podmorph took podcasts and, like Sophina, helped turn them into TikTok videos. However, Podmorph made the mistake of focusing on end-to-end automation. It was a technical feat. It meant minimal effort from users who only had to upload audio then select a quote and a picture. We managed to use AI to make a TikTok video by generating pictures based on an AI-driven analysis of AI-generated snippets of a podcast. We were very impressed with what was technically possible. But the output was not very engaging. In contrast, Sophina keeps the end part of the process — the telling of the story — human.</p><h4><strong>Editorial judgement</strong></h4><p>At the same time, I wasn’t impressed with the scripts Sophina generated. Sophia told me that Sophina uses OpenAI’s GPT-4o to rewrite the scripts. The tone seemed a bit too formal and a bit too similar to my articles. The script used “journalese”, the words we would never use when explaining something to a friend but time-pressed journalists use in written news articles. I changed “voiced concerns” to “complained” and “cease” to “scrap”. But I didn’t find a way to see the prompts, adapt them, or try out other models, such as Anthropic’s, which might have improved the scripts.</p><p>As one example of an error in the script-generation process, Sophina attempted to summarise a funny anecdote but took out the punchline. Since I first tested Sophina, it has been updated to add what tone you would like (I chose informal) and a description of your job (I chose journalist). Unfortunately these script problems didn’t go away. The tool killed a joke, again, and — as if to make it worse — subsequently added a clumsy segue “it’s not just a laughing matter”. It felt clichéd and unnatural.</p><p>I also didn’t always agree with the summarisation. Sophina had selected three examples out of six examples I had given. But it chose the least significant and least interesting of the examples. This didn’t surprise me, large language models, like OpenAI’s, predict the most likely next word based on probability, they don’t predict them based on truth, relevance, novelty, significance, or entertainment value, which are some of the reasons journalists might make a selection. Chilton observed the same limitation with ReelFramer: “It doesn’t quite get what’s the important part,” she told me. “We found that the journalists’ training was super valuable in deciding ‘these are the important points’, ‘this is what people need to know’.” She also observed that ReelFramer, whose prompt she said you could adjust, wasn’t very good at correctly judging the audience’s level of knowledge. “You know your audience as the journalist, you would know what needs explanation and how to explain that.” Likewise, I felt I had to add extra information to my story at the beginning of Sophina’s script about Apple’s summarisation feature just for it to make sense for my audience.</p><h4><strong>Automation bias</strong></h4><p>As I used the tool I started questioning my changes to the script — maybe the machine knew more about short video scripts than I did? Chilton observed the same phenomenon in her research participants: “There’s this battle in their heads, who’s the expert here? I’m an expert on journalism, but it’s [the AI tool] more of an expert on TikTok. And so people would actually forget some of their journalism training and sort of give up their agency to the AI.”</p><p>Ultimately though I ended up changing pretty much the whole script before moving on to filming the video.</p><p>Sophina the app is easy to use and the teleprompter feature helped me read the script and record it all in one shot. Although I think the script was a bit low on my screen and my eyes subsequently are looking down a bit rather than directly at the camera.</p><p>Producing my second video, I tried to save time by recording the video without checking the script. Only part way through I noticed a factual error and had to stop.</p><p>The script summarised in such a way that it accused someone of something I couldn’t prove they had done. This also didn’t surprise me. Defamation has been a common risk with generative AI output I found in early prototypes I worked on at the BBC. It doesn’t make a tool like this unusable, it just means that it’s probably wise that the person fact checking the script knows the facts of the story. As Sophina appears to be designed specifically for people who write long text articles to change their own articles into short videos, I think it’s reasonable that it’s the responsibility of the content maker, not the app, to ensure accuracy.</p><p>All errors corrected, I uploaded my video and left the <a href="https://www.tiktok.com/@journoclare/video/7505025468375043350">two</a> <a href="https://www.tiktok.com/@journoclare/video/7505016242894392598">videos</a> to the gods of the TikTok algorithm for a week. I came back to 270 views, no likes, and no comments.</p><figure><img alt="With these pitiful metrics, I am not rushing to give up the day job Alt text: The TikTok profile page for Journo Clare showing 143 plays for one video and 127 plays for another video." src="https://cdn-images-1.medium.com/max/1024/0*IKKZGbRu3eI4AHXz" /></figure><p>So can Sophina turn me into a TikTok star? No. But it got me started. And, for me, that is enough to make it worthwhile.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=930f9c489408" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/can-sophina-turn-me-into-a-tiktok-star-930f9c489408">Can Sophina Turn Me Into a TikTok Star?</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ Personalizing the News: How LLMs Can Adapt the News Experience to Suit You ]]></title>
<link>https://generative-ai-newsroom.com/personalizing-the-news-how-llms-can-adapt-the-news-experience-to-suit-you-23694cafb68f?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/23694cafb68f</guid>
<category><![CDATA[ news ]]></category><category><![CDATA[ personalization ]]></category><category><![CDATA[ generative-ai-use-cases ]]></category>
<dc:creator><![CDATA[ Leo Necheles ]]></dc:creator>
<pubDate>Mon, 09 Jun 2025 12:17:25 GMT</pubDate>
<atom:updated>2025-06-09T12:17:24.988Z</atom_updated>
<content:encoded><![CDATA[ <h4>New AI capabilities can change not just what stories you see, but how those stories are told, paving the way for bespoke news experiences.</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*AL6YbVsncqOQ5_XKrXSfxw.png" /></figure><p>Traditionally, newsrooms have adapted content for a broader audience, curating front pages and other editorial packages to reflect the interests and priorities of their general readership. In recent years, sophisticated recommendation algorithms, like <a href="https://open.nytimes.com/how-the-new-york-times-incorporates-editorial-judgement-in-algorithms-to-curate-home-screen-content-85f48209fdad">those used by the New York Times</a>, have refined this process, using user behavior and editorial judgment to surface relevant stories. However, these systems typically require significant data and engineering resources, placing them out of reach for many smaller newsrooms. Now, rather than focusing solely on recommending existing content, large language models (LLMs) can take it a step further, adjusting how news is written, summarized, and framed to better suit the individual. As readers increasingly seek news that is relevant, digestible, and personally engaging, this level of customization is quickly becoming an expectation. Fortunately, this is now within reach for any newsroom.</p><p>In fact, a <a href="https://reutersinstitute.politics.ox.ac.uk/journalism-media-and-technology-trends-and-predictions-2025">2025 Reuters survey</a> of media leaders found that 80% of respondents believe that using AI to improve personalization and recommendation will be very or somewhat important in the year ahead. Many also reported plans to actively explore features like converting text articles into audio (75%), provide AI summaries at the top of stories (70%), or translate news articles into different languages (65%). These are just some of the many potential use cases where adapting news to specific audience needs could come into play.</p><h3>The Role of LLMs in Personalizing News</h3><p>In a newsroom context, <em>personalization</em> refers to the practice of tailoring news content to match an individual’s interests, reading habits, location, or even values. This has often meant recommending relevant stories based on a reader’s preferences and behaviors, such as surfacing more sports articles after someone reads about their favorite team, or highlighting local news based on a user’s location data. However, LLMs can now adjust not just <em>which</em> stories are shown, but also <em>how</em> they are told, and how they’re framed to draw readers in.</p><p>To understand the full scope of how LLMs are transforming news personalization, it helps to break down the process into three key areas: how stories are told, which stories are shown, and how readers are drawn in. These reflect the evolving notion that AI is shaping not just what we read, but how we engage with the news.</p><h4>1. Content Reframing: Adjusting Tone, Style, and Reading Level</h4><p>One of the most transformative uses of LLMs in newsrooms is their ability to reframe content. By modifying the tone (formal vs. conversational), complexity (expert vs. general reader), or even format (full article vs. summary) of a story, newsrooms can match the way a story is told with the preferences, backgrounds, and expectations of their readers.</p><p>Personalization exists on a spectrum. At one end is the classic one-to-one match between an individual and a piece of content; at the other is cohort-level personalization, where a story is reshaped for an audience segment that shares similar traits. On this blog, Jaemark Tordecilla <a href="https://generative-ai-newsroom.com/transforming-longform-text-into-multimedia-using-generative-ai-5cc50fa9b460">recently laid out</a> how journalists can transform long-form text into multimedia using Generative AI, leveraging a set of widely available no-code tools. For newsrooms attempting to attract younger audiences that prefer short-form video (<a href="https://www.pewresearch.org/short-reads/2024/09/17/more-americans-regularly-get-news-on-tiktok-especially-young-adults/">39% of adults under 30 regularly get their news from TikTok</a>), these techniques offer a way to tell stories in new modalities that the cohort prefers, without the overhead of a full-blown multimedia team.</p><p>Beyond format, other studies have shown how LLMs can influence the emotional framing of news stories. A <a href="https://dl.acm.org/doi/10.1145/3639701.3656308">2024 paper</a> analyzing television dialogue used prompt-engineered LLMs to distinguish between how various stories are framed and shape audience perception. Although the models only matched human judgment 43% of the time, the experiment suggests a future in which newsrooms could adapt not just the tone or structure of an article, but its narrative frame. More recently, <a href="https://ceur-ws.org/Vol-3929/paper4.pdf">researchers tested</a> how GPT-4 could actively reframe headlines in a news recommender system to elicit emotional responses. In this user study, negative framing generated significantly stronger emotional reactions and even increased participants’ willingness to pay for the news service. These findings suggest that LLMs could soon be used not just to deliver content in new formats, but to strategically shape how stories feel to different audiences.</p><p>Newsrooms are already starting to put variations of these techniques into practice. <em>Time</em> recently launched <a href="https://time.com/7201556/generative-ai-time-journalism/?utm_source=chatgpt.com">TIME AI</a>, a tool that allows readers to choose how they want to engage with a story. With the power to view a single article in its full-length version, a short summary, a narrated audio clip, or even an interactive Q&amp;A, Time is empowering its readers to access the same core information in the way that suits them best. In addition, ESPN is looking to create <a href="https://www.inc.com/associated-press/espn-wants-ai-to-generate-personalized-sportscenter-feeds-for-users.html">personalized “SportsCenter” shows</a> for subscribers in Fall 2025, using voice-overs from its host to create highlights packages tailored to individual interests.</p><h4>2. User-Centric News Recommendations: AI-Powered Summaries &amp; Personalized Feeds</h4><p>While content reframing personalizes how a single story is told, another emerging use of LLMs is determining which stories a reader sees in the first place. This can be done through personalized summaries, feeds, and recommendations, all adapting in real time to user preferences.</p><p><a href="https://ieeexplore.ieee.org/document/10367969">Research has shown</a> that GPT-3 can correctly sort news stories into high-level IPTC topics with about 80% accuracy. The researchers also discovered that when having the model classify articles in steps (identifying a broad category, then narrowing down to subtopics), it made fewer mistakes and avoided confusing or made-up results. When combined with techniques like <a href="https://dl.acm.org/doi/10.1145/3626772.3657783">retrieval-augmented generation</a>, these findings suggest that GPT-powered systems can not only recommend relevant stories, but also organize them more reliably than traditional systems, making personalized news feeds more accurate and engaging.</p><p>LLMs also offer the promise of personalized news feeds that are more responsive to real-time developments and more targeted to individual readers. For example, <em>The Times of India</em> built <a href="https://journalists.org/resources/case-study-how-the-times-of-india-brings-real-time-personalization-to-1500-daily-news-stories/">Signals</a>, an advanced recommendation system that analyzes user behavior across the platform in real time, rather than relying on static content labels or tags. And by strategically pushing archived stories to readers during low-volume periods, this system has also developed new ways to surface evergreen coverage. Signals has <a href="https://journalists.org/resources/case-study-how-the-times-of-india-brings-real-time-personalization-to-1500-daily-news-stories/">drastically increased reader engagement</a> — since launching the system, The Times of India has seen an 85% improvement in website click-through rates and an over 30% increase in app engagement.</p><h4>3. Headline and Newsletter Generation: AI-Optimized Engagement</h4><p>LLMs also offer advantages for editorial tasks geared toward audience engagement, such as headline testing and search engine optimization.</p><p><a href="https://generative-ai-newsroom.com/personalizing-newsletter-headlines-testing-llms-against-human-editors-6f14135089da">One recent study</a> found that readers preferred AI-generated newsletter headlines 62% of the time over those that were written by journalists. The study tested both AI and human-written headlines across a range of topics and reading profiles, showing that LLMs were especially effective when the headlines matched the reader’s prior preferences. By having LLMs write custom headlines tailored to user interests, the headline itself becomes a personalized entry point into the news experience.</p><p>At <em>Bloomberg, </em><a href="https://aclanthology.org/2022.acl-long.446.pdf">AI engineers used LLMs</a> to revise headlines based on real-time changes in news stories. Rather than rewriting headlines from scratch, the model modifies previous headlines to reflect what’s new, helping maintain continuity for readers following an evolving story.</p><p><a href="https://pressgazette.co.uk/news/times-titles-halve-digital-subscriber-churn-with-tailored-emails-from-ai-named-james/">Several years ago</a>, <em>The Times of London </em>and<em> Sunday Times </em>implemented a tool called “JAMES” to address their dwindling newsletter subscribers. JAMES, an AI “digital butler,” sent individualized newsletter emails by learning each subscriber’s content preferences, reading habits, and optimal engagement times. During a year-long trial, the tool reached over 100,000 subscribers and helped reduce churn by 49% among selected cohorts, with over 70% of exposed users engaging with the emails, particularly those who were previously low-engagement users.</p><h3>Comparing Two Personalized News Experiences</h3><p>To make news personalization more tangible, I conducted a review of two distinct approaches: <a href="https://chat.openai.com/">ChatGPT</a> (using the gpt-4o model), a chatbot repurposed as a makeshift news curator, and <a href="https://particle.news/">Particle</a>, a dedicated AI news app. I wanted to see whether ChatGPT, despite not being designed for news personalization, could function as a reliable space for daily news consumption, and how that might compare to a custom-designed tool such as Particle.</p><h3>ChatGPT</h3><p>To establish a baseline, I first prompted ChatGPT for a generic weekly news breakdown, without providing any user-specific information. This offered a useful starting point for evaluating how well it performs at giving generic news briefs compared to a purpose-built platform like Particle.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*UHgdo0fvPzhSt18_" /></figure><p>This initial briefing was promising, utilizing up-to-date news and explicit sources for each headline. It was now time to feed ChatGPT the user-specific information. In principle, ChatGPT’s “<a href="https://openai.com/index/memory-and-new-controls-for-chatgpt/">saved memories</a>” should allow this information to be referenced anytime I ask for a news briefing going forward.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*og3PJph6zcIS_t9V" /></figure><p>I fed ChatGPT several pieces of information relevant to my news consumption habits, including where I live, my news interests, and outlets I frequent. I even listed Particle to see if ChatGPT would attempt formatting its briefing in a similar layout to their app. Because this was now stored for future reference in ChatGPT’s saved memory, I could prompt the model for an updated version of the top stories to know about this week. Outside of which specific stories ChatGPT listed, I was curious to see if the format the content was presented changed as well.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*qp608_fu_epVMORc" /></figure><p>After once again prompting for a weekly news briefing, I was able to see ChatGPT’s first attempt at a personalized news breakdown for my specific interests. While it did accurately compile my interests, each piece of news contained only one individual source and only one or two sentences of information. There wasn’t much depth to any of the content.</p><p>Next, I was curious to see if I could prompt ChatGPT to update my content preferences based on stories I like or don’t like.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*A-oklSzlJLZOvO3G" /></figure><p>I prompted the model to act as a news curator and feed me five stories that I would then respond to with a rating. This would theoretically allow the model to adjust its recommendations for me accordingly and provide more accurate news briefs for my taste.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*xoRZXTwiALYFXb1f" /></figure><p>After collecting my input on various news stories, I re-prompted the model one last time for my weekly news briefing. To my dismay, nothing changed with the output. I was still looking at the exact same format and text. ChatGPT had failed to adjust its content based on my explicit preferences.</p><h3>Particle</h3><p>As a dedicated AI news aggregator, Particle is able to specifically target the needs of news consumers. This allows for an app with comprehensive UI and UX features that are specific to readers’ needs.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/936/1*t97QtUHCb7lmoTT8VAKOSw.png" /></figure><p>While starting from scratch on Particle’s app, I was prompted through an interactive screen to show which categories of news I prefer to consume; I swiped through a Tinder-esque interface of news content to establish my personal tastes. Next, it was time to consume some content and compare the experience.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/962/1*4VQnGqhqTmMbJVLKvBO3Ng.png" /></figure><p>While on the story’s page, Particle provides various formats you can choose to read any given story in. This format allows Particle to provide several options depending on how I, the consumer, want to read my news.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/924/1*8UCOva8u13QknY18O6IZ9w.png" /></figure><p>Additionally, reading through story content comes with many other engaging features such as relevant quotes and articles, community questions, and hyperlinks with brief pop-ups of topic summaries (sort of like a built-in Wikipedia). All of these tools help make news consumption a truly personalized experience rather than just a read-through.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/906/1*-EQSOiyGW_cVywxX-LuUyw.png" /></figure><p>Even after the initial calibration, I can go to Particle’s content preference page to adjust topic frequency. Their infrastructure makes it easy for users to identify which beats they would like to see more or less of.</p><h3>Take-Aways</h3><p>All of this goes to show that while ChatGPT’s new saved memory features show promise in personalized daily/weekly content across separate chats, newsrooms still have a lot of power and opportunity to release their own features that customers can enjoy. Not only that, but newsrooms can also prioritize building out strong user experiences in their apps where personal user profiles are input via an interactive interface. Apps like Particle offer engaging and customizable experiences that general-purpose LLMs like ChatGPT fail to match. This includes adapting to users’ needs by using in-app data and analytics to see which content succeeds for certain users and should be pushed more. It remains difficult for news consumers to automate their own highly personalized and interactive news briefings, a gap that newsrooms could attempt to fill.</p><h3>In Closing</h3><p>Powerful as they are, LLMs still make mistakes and can fabricate details, meaning that to preserve factual integrity, human editors must remain in the loop — a clear tension with trying to create a personalized version for every user, though perhaps still doable for audience-segment-based approaches. Additionally, personalization engines that maximize engagement risk trapping users in echo chambers that amplify familiar perspectives while filtering out dissenting or diverse viewpoints; tools like Particle’s “Opposite Sides” specifically aim to break users’ filter bubbles by offering different viewpoints from the same story. Readers would also benefit from a clear disclosure label explaining how algorithms influence their feeds, especially given <a href="https://reutersinstitute.politics.ox.ac.uk/sites/default/files/2024-06/RISJ_DNR_2024_Digital_v10%20lr.pdf">survey data</a> showing a majority remain uneasy about AI-generated news.</p><p>Nonetheless, what once seemed futuristic is now within reach: a news experience shaped not just by what we read, but how we prefer to read it. With the rise of LLMs, even smaller newsrooms can now meet the growing demand for personalized, engaging content, not just by recommending stories, but by reshaping how those stories are told. As these tools become more accessible, the next challenge lies in using them creatively and responsibly to serve diverse audiences without sacrificing editorial integrity.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=23694cafb68f" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/personalizing-the-news-how-llms-can-adapt-the-news-experience-to-suit-you-23694cafb68f">Personalizing the News: How LLMs Can Adapt the News Experience to Suit You</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[ How a Danish News Service Made a Profit with its Transcription Tool ]]></title>
<link>https://generative-ai-newsroom.com/how-a-danish-news-service-made-a-profit-with-its-transcription-tool-285bc05b7cf9?source=rss----df04cefca135---4</link>
<guid isPermaLink="false">https://medium.com/p/285bc05b7cf9</guid>
<category><![CDATA[ generative-ai-use-cases ]]></category><category><![CDATA[ transcription ]]></category>
<dc:creator><![CDATA[ Clare Spencer ]]></dc:creator>
<pubDate>Mon, 02 Jun 2025 15:53:29 GMT</pubDate>
<atom:updated>2025-06-02T15:53:29.021Z</atom_updated>
<content:encoded><![CDATA[ <h4>A software engineer at Zetland created the transcription tool Good Tape, which has brought in millions of dollars.</h4><p>An evergreen question that perpetually irks me, and apparently many others in the industry, is whether there is a sustainable business model for news. At the International Journalism Festival this year, it felt like the mood was that AI had made this existential crisis worse. So when Der Spiegel’s head of AI, Ole Reissmann, told me that a Danish news service has been making money from their transcription tool — all made possible thanks to AI — I was intrigued.</p><p>The CEO of Zetland <a href="https://www.linkedin.com/in/tavklitgaard">Tav Klitgaard</a> told me that their transcription tool <a href="https://goodtape.io/">Good Tape</a> has transcribed more than 10 million audio files and has more than 2.5 million users worldwide. After its establishment, Good Tape went on to become a subsidiary company of Zetland and today it has roughly $3 million in annual recurring revenue (ARR), he said.</p><p>The story of the founding of Good Tape starts with software engineer Jakob Steinn. “I scroll GitHub like other people scroll Instagram” he told me with pride. This regular checking for updates of coding projects others had shared on GitHub led Jakob to discover early on that OpenAI had shared <a href="https://openai.com/index/whisper/">Whisper</a> — an <a href="https://github.com/openai/whisper">open source (and open weights)</a> generative AI model for transcription and translation.</p><figure><img alt="Jakob Steinn, Tav Klitgaard and Lasse Finderup pose together" src="https://cdn-images-1.medium.com/max/1024/0*H99m1gMqt9jlMQ-M" /><figcaption>Tav Klitgaard, centre, decided to develop software engineer Jakob Stein’s transcription prototype which became Good Tape, of which Lasse Finderup, right, became the CEO.</figcaption></figure><p>He tried Whisper out on an <a href="http://kremlin.ru/events/president/transcripts/24034">infamous speech Russian President Vladmir Putin made in Munich in 2007</a>. Important to the test was that the speech was in Russian, was 32 minutes long, and had already been translated manually. “I generated [English] subtitles from the original Russian video and then I took a dubbed video and put my subtitles on top so I could hear the English words and read the subtitles at the same time to see how well they matched.” His test proved Whisper could take a language and both translate and transcribe it to English at the same time.</p><p>He thought this translation was the obvious next step but when he shared his work with colleagues in the newsroom he found that what they really needed was a tool for transcription in <em>Danish</em>. So he got to work testing Whisper on Danish YouTube videos. At this point, he was accessing Whisper through the command line on his mac. To test whether Whisper was good enough for their newsroom, he asked journalists to send him audio of their interviews, which he ran through the model and sent back to them transcribed.</p><p>Tav wasn’t aware of Jakob’s experiment at this point. But he was about to find out in an unforgettable way. “The first time I heard about it was one morning, I very vividly remember someone comes running into the office almost screaming, ‘allocate everything you have to this project that Jakob is doing because it’s magic, it works!’”. Tav said he could sense this was worth pursuing, “which wasn’t really that hard because I saw a professional person almost crying because he could see that his professional life would change because of technology.”</p><p>It made sense to Tav for Jakob to spend time building a tool for his colleagues to upload audio to transcribe. “A back of the napkin calculation was: ‘that’s just hours saved next week for us so let’s spend some time building’” Tav told me.</p><figure><img alt="Tav Klitgaard and Lasse Finderup sit at an office desk" src="https://cdn-images-1.medium.com/max/1024/0*l_S4KHbRCJEDO3rO" /><figcaption>Caption: The CEO of Zetland, Tav Klitgaard, left, appointed a CEO of Good Tape, Lasse Finderup, early on in the journey.</figcaption></figure><p>Next, Jakob shared a Google Colab notebook (like a Google doc but where you can write code and execute it) with the journalists in the newsroom, empowering them to process the audio themselves. Feedback from the journalists made him realise that it was really important to them to see the progress of the transcription.</p><p>Jakob went on to make a web interface and test the tool with the wider Danish journalism community. “People were just writing back to us saying, ‘oh my god, now my work life will change’. And so then the ball just kept rolling and we decided to really invest in building it,” said Tav. Zetland launched a Beta version of the product in January 2023.</p><h3>Differentiating the Product</h3><p>For their public product they added user logins, subscription, data privacy, sped up the processing and improved the accuracy. Jakob says Good Tape is much more than just an interface allowing journalists who can’t code to use Whisper. “From the beginning we were not a wrapper around the APIs because we’ve always run it on our own hardware.” He said the team added in vital building blocks to make Good Tape <em>faster</em>, <em>safer</em> with data, and more <em>accurate</em> than a wrapper around Whisper’s API would be.</p><h4><strong>Speed</strong></h4><p>For speed, they implemented the model on TPUs rather than GPUs. GPUs are graphical processing units which process many small tasks simultaneously. On the command line interface OpenAI had provided, they used Nvidia’s GPUs. Jakob replaced these with Google’s tensor processing units (TPUs). He says that, with TPUs, Good Tape can process an hour of audio in 45 seconds.</p><p>This created a lot of extra behind-the-scenes work. “We had to scrap that interface and build our entire own engine to run on TPUs because OpenAI does not support that”. This was not the only modification they made: “We had to rebuild the transcription engine, kind of from scratch, to be able to control every single piece and layer to make improvements to them”. But he said the work was worth it: “What differentiates us from other Whisper users is that we have so much control.”</p><p>It’s worth noting that this speed advantage is only passed on to paying customers — as I discovered when I tried out Good Tape on the interviews for this article. After I uploaded the audio, Good Tape told me I would have to wait nearly an hour for my transcription unless I paid.</p><figure><img alt="This shows a screen on Good tape saying “your file is now in a queue… Waiting time is currently about 57 minutes… Hate to wait? Skip the line now Upgrade here.”" src="https://cdn-images-1.medium.com/max/937/0*JeDA_mU2YgwBmAhh" /><figcaption>Speedy transcription is only available to paying customers — this test shows a customer using the free service may have to wait for almost an hour.</figcaption></figure><h4><strong>Data security</strong></h4><p>Jakob explained to me that Good Tape is also built with data protection in mind. “All processing and all storing is happening on our own hardware. We’re not using any external APIs. We are running the transcription engine and everything is within the EU.”</p><p>He said this is important for journalists when you consider data sovereignty — that data is subject to the laws of the country where it is collected or stored. “In the beginning we had a surge of Taiwanese users because Taiwan does not want to use mainland [China] transcription services for their journalistic freedom.”</p><h4><strong>Accuracy</strong></h4><p>Jakob has also changed the default settings, such as temperature, which — if it’s too high — can allow Whisper to generate more “creative” or random text not appropriate for professionals who need to maintain factual accuracy.</p><p>“It’s all a probability game. When Whisper sees the next tokens come in it would generate five different possibilities and then it would rank them and pick one that has the best [probability]. We have changed the way that that logic works which makes it more predictable so that it’s more consistent.” With these settings and adjustments, “It doesn’t sometimes just go completely bananas and hallucinate.”</p><h3><strong>A Market of Customers Within and Beyond Journalism</strong></h3><p>“The last deal breaker was: is there a willingness to pay?” Tav told me. They launched the pro version on March 15, 2023. “We got our first paying customers within seconds,” said Tav.</p><p>They asked users in other small languages — from Finnish to Croatian to Hebrew — to test their output. “People wrote back in all caps saying, ‘What is going on here? This is magic,’” said Tav.</p><p>They discovered other small languages had the same problem the Danes experienced — that previous transcription services hadn’t been accurate enough to be helpful. “If you put all of those languages together, that’s a very very big market,” Tav told me.</p><p>When I tried Good Tape as a free customer, there were multiple points when I was strongly encouraged to upgrade. After I uploaded my hour-long interview with Tav, I discovered that I would have to pay to get the second part of the transcription:</p><figure><img alt="This shows the Good Tape interface with timed transcriptions followed by a dialogue box which says “Whoa. That’s a lot of tape. Have you considered upgrading to Good Tape Pro?”" src="https://cdn-images-1.medium.com/max/1024/0*7QaTCefFiKYAb_q_" /><figcaption>Good Tape offers free transcriptions up to a limit — it gave me 30 minutes of my hour long interview with Tav before asking me to pay</figcaption></figure><p>This approach appears to be paying off. “We have managed to make a profit and have some of the largest newsrooms in Europe as customers” boasts Tav, while pointing out a lot of customers are not journalists. “We believe it’s about 30 to 35% of our customers that are within journalism. Then there’s business consultants, UX designers, health professionals — people who do qualitative interviews and need to work professionally with them … The growth in Good Tape now is stronger — and has been for the past few years — than in Zetland,” he said.</p><p>From my point of view as a journalist, automated transcription really has changed my working life for the better. Things have come a long way since I learned shorthand in the early 2000s, to spending hours manually transcribing in the 2010s to now. Thanks to transcription tools, I have saved roughly four hours writing this article — plenty of time left over to start dreaming about the next useful tool for journalists.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=285bc05b7cf9" width="1" height="1" alt=""><hr><p><a href="https://generative-ai-newsroom.com/how-a-danish-news-service-made-a-profit-with-its-transcription-tool-285bc05b7cf9">How a Danish News Service Made a Profit with its Transcription Tool</a> was originally published in <a href="https://generative-ai-newsroom.com">Generative AI in the Newsroom</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p> ]]></content:encoded>
</item><item>
<title><![CDATA[Uncovering Hidden APIs on the Web]]></title>
<link>https://generative-ai-newsroom.com/uncovering-hidden-apis-on-the-web-db9ec01eb2e0?source=collection_home_page----df04cefca135-----0-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Evan Chen]]></dc:creator>
<pubDate>2025-05-19</pubDate>
<atom:updated>2025-05-19</atom:updated>
<content:encoded><![CDATA[Uncovering Hidden APIs on the Web
Using generative AI to support investigative journalism by 
helping collect data from websites that don’t want to share it
Evan Chen
Follow
6 min read
·
May 19, 2025
--
1
Listen
Share
Modern websites are full of valuable information for data-driven reporting, but actually collecting that information can be a slog. Journalists often face websites that are scraping-resistant, load data dynamically with JavaScript, or bury key details behind layers of interaction. Manually collecting data from these sources is time-consuming and technically demanding.
That’s where 
undocumented APIs
 come in. These are hidden data sources that power websites under the hood but aren’t officially documented or publicly promoted. When discovered, they can provide direct access to structured data — clean, fast, and without the mess of parsing HTML. In fact, journalists have used them to break major stories, like when 
Amazon was found leveraging internal APIs to favor its private-label products
..
To recap: in 2021, Amazon faced backlash when reports surfaced that it was using undocumented APIs and internal data to give its private-label products an unfair advantage over third-party sellers. The controversy escalated when investigative reports revealed that Amazon allegedly accessed non-public sales data from competing brands on its platform, allowing it to identify popular products, create alternatives under its private labels (such as Amazon Basics), and manipulate search-engine optimization (SEO) rankings to favor them. This raised serious concerns about anti-competitive behavior and abuse of platform power.
A key point in the story comes from when a hidden Amazon API was discovered, revealing internal product ranking adjustments that prioritized Amazon-branded products over independent sellers. This showed how Amazon’s algorithm nudged its own brands to appear more prominently in search results, even when third-party alternatives had better reviews or sales performance. The investigation ultimately forced Amazon to defend its practices and eventually make changes to its ranking systems.
So the journalistic value of undocumented APIs is well-established. The problem is that 
finding undocumented APIs is really tedious
, and it may not always pay off. It often involves combing through browser network logs, interpreting cryptic request patterns, and reverse-engineering authentication. It’s powerful, but inaccessible for all but the most dedicated.
To expand journalists’ ability to find new undocumented APIs I 
have been developing a tool
 that automates this process with the help of LLMs. It captures and filters network traffic, uses AI to identify promising endpoints, and extracts the headers needed to make real API requests. The end result is that it’s easier for journalists and developers to uncover and use hidden APIs for their reporting.
How It Works: A Step-by-Step Walkthrough
Let’s say a journalist is investigating Ticketmaster, following public complaints about dynamic pricing, resale markups, and lack of transparency during high-demand ticket sales (like for Doja Cat or Sabrina Carpenter). The company claims ticket prices are adjusted based on demand — but the exact mechanisms are hidden from users.
The goal is to uncover how pricing is determined, whether verified resale listings are prioritized, and whether the platform gives preferential placement to certain vendors. Here’s how the tool I built can help.
Step 1: Capturing Network Traffic
First, we visit Ticketmaster’s website when tickets for a popular concert go on sale. With browser network logging enabled, we capture the activity of one of the concert listing pages. This generates a .har (HTTP Archive) file capturing every background request the page makes, including hidden APIs used to load ticket availability and pricing in real time.
My tool parses this .har file and filters out noise like image files and tracking scripts, leaving just the structured API requests. Even then, there may be hundreds of endpoints triggered by seat maps, availability checks, pricing refreshes, and promotions.
I group similar requests together to reduce redundancy while preserving unique patterns that may hint at internal pricing logic.
Step 2: Identifying Valuable Endpoints Using AI
Next, my tool sends the cleaned list of API requests to OpenAI’s API with a prompt asking which of the endpoints are likely to contain structured pricing, availability, or prioritization data (for the full prompt see our 
repo online
). The model scores each endpoint from 0 to 100. For our Ticketmaster example, high-ranking results include:
ticketmaster.com/api/page/view
ticketmaster.com/api/spotlights/presale-events
identity.ticketmaster.com/signed-in
promoted.ticketmaster.com/home-global
These are endpoints that may contain not just raw prices but indicators of internal prioritization logic, surge multipliers, or resale relationships.
Step 3: Extracting Headers for a Successful Request
Once valuable potentially newsworthy endpoints are found, we want to actually make requests to them. But many require headers for authentication or session state, otherwise they return errors.
The tool compares successful vs. failed requests and uses 
Playwright
, a website testing tool, to simulate requests with different header combinations. For example, it might discover that a request to the api/v2/dynamic-pricing endpoint only works if you include a header like the following:
{
  "accept": "*/*",
  "content-type": "application/json",
  "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
}
This helps journalists replicate the API call and access the same structured data that Ticketmaster’s frontend uses, without having to rely on manual site interaction.
Step 4: Producing a Structured API Schema
Finally, my tool packages each detected API endpoint into a structured format. Here’s an example we found from Ticketmaster:
{
  "url": "https://analytics.ticketmaster.com/api/page/view",
  "description": "This endpoint tracks page views, which is crucial for understanding user engagement and behavior on the Ticketmaster platform. It provides insights into how users interact with the site, which pages are most popular, and can inform marketing and content strategies.",
  "usefulness_score": 85,
  "method": "GET",
  "required_headers": {
    "accept": "*/*",
    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)
  },
}
This gives journalists a portable, readable snapshot of a hidden API, allowing them to track how prices change in real time, compare across vendors, or even monitor for bias in resale listing prioritization. This is all displayed in a dashboard that provides users with the API endpoint, its required headers, and a CURL example, should they want to replicate the call. See below:
Future Enhancements
While the API engine can help uncover valuable endpoints and automate large parts of the discovery process, there are still important limitations to be aware of, especially for journalists hoping to use this tool in the field.
Rate Limits and Anti-Bot Detection
Many sites, including Ticketmaster, have rate limits or bot detection systems that throttle or block repeated API calls. During testing, I found that even modest automated traffic could trigger temporary bans or 403 errors (i.e. “Forbidden”). While I’ve added delays between requests, this can significantly slow down analysis, so you’ll need to be patient. In the future I’d like to include throttling detection to be able to dynamically adjust request frequency and simulate more realistic human browsing behavior with Playwright.
Authentication Challenges
Some APIs require tokens or cookies that are tied to logged-in sessions, device IDs, or other hidden variables. If these credentials expire or are device-bound, it becomes harder to consistently access the data. In the future we may be able to get around this by integrating login workflows or session preservation tools so users can authenticate manually once, and let the tool carry the session forward.
AI Misclassification
The AI model that ranks endpoint usefulness isn’t perfect. Sometimes it labels seemingly-random endpoints as high-value or misses subtle but important ones. There’s a tradeoff between precision (strict filtering) and recall (casting a wide net). In the future I’d like to let users set how aggressive the AI should be, or even re-rank endpoints based on what kind of investigation they’re doing (pricing, visibility, location data, etc.).
Scalability Bottlenecks
Processing hundreds of endpoints with multiple verification steps (AI scoring, header testing, schema output) can be slow and resource-intensive, especially for large sites. To address this we’re considering refactoring the tool to run in parallel across multiple threads or containers, and allowing journalists to run only selected parts of the pipeline (e.g., just header extraction or AI scoring) to save time.
Final Thoughts
Undocumented APIs are often the hidden backbone of the modern web — quietly powering dynamic content, pricing engines, and user personalization. For journalists, they represent a largely untapped source of structured, real-time data that can reveal how platforms operate behind the scenes. But uncovering them has traditionally been a slow, manual, and technically demanding process.
I see this project as a starting point for a broader ecosystem of tools that give reporters deeper access to the data flowing just beneath the surface of the web. If you’re a journalist, technologist, or just curious about what websites aren’t showing you, I’d love to hear your thoughts!]]></content:encoded>
</item><item>
<title><![CDATA[Keeping Track of AI Use Cases in the Newsroom]]></title>
<link>https://generative-ai-newsroom.com/keeping-track-of-ai-use-cases-in-the-newsroom-1811b8cb606f?source=collection_home_page----df04cefca135-----1-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[AI]]></category><category><![CDATA[Newsroom]]></category>
<dc:creator><![CDATA[Ethan Silver]]></dc:creator>
<pubDate>2025-05-07</pubDate>
<atom:updated>2025-05-07</atom:updated>
<content:encoded><![CDATA[Keeping Track of AI Use Cases in the Newsroom
A practical guide to how journalists are using generative AI and how to keep up
Ethan Silver
Follow
10 min read
·
May 7, 2025
--
1
Listen
Share
Image generated by OpenAI’s ChatGPT-4o using DALL·E
Newsrooms are implementing new AI tools at a rapid pace, experimenting with everything from automated fact-checking to AI-assisted document processing. The speed of these developments, where new articles, papers, and videos discussing newsroom AI crop up practically every week, makes it challenging to keep up with how exactly AI is transforming the news media. As the deployment of artificial intelligence in the newsroom becomes more widespread, the need to track and collate common use cases becomes more pressing. Knowing what’s been done and what’s working across the industry stands to help guide development and innovation in the future.
To address this need, in this article we both outline a snapshot of generative AI use cases in the newsroom, and offer a set of resources for finding new AI tools and staying on top of the continuous flow of advancements in the field.
How We Curated Use Cases
In constructing the following list of use cases, I scoured published research, academic journals, online articles, and the news organizations themselves. I conducted the search from January to April of 2025, reviewing a 
total of 64 sources
 — from the Online News Association’s 
case study series
 to the Columbia Journalism Review’s 
report on artificial intelligence.
Using the information from these sources, I organized the use cases into a list of broad categories:
News
 
Discovery
News
 
Gathering
News
 
Production
Content
 
Transformation
Content
 
Management
Content
 
Distribution
User
 
Interaction
Each section of this article will explore one of these categories. I’ll define the category, describe its general use cases, and then provide concrete examples of how it shows up in newsrooms. It is important to remember that many of the use cases I describe in this article can be (and are) used in multiple aspects of the newsmaking process — most do not fit into just one category.
News Discovery
Discovery
 encompasses tools that help journalists monitor breaking news and find story ideas. Often this means speeding up the process of reviewing documents, datasets, or information streams, helping journalists find the needle in the haystack.
Using generative AI, the Norwegian newspaper iTromsø significantly reduced the time journalists spent digging through data. They developed a tool called 
Djinn
, which can search and summarize thousands of municipal documents, identifying key information journalists may want to investigate further. Another example is the 
COA Beat Assistant
, developed to review government audit reports for watchdog journalists. And in 2023, a 
BBC World Services investigative team
 used AI to sift through tens of thousands of social media posts about the war in Ukraine.
In these cases, GenAI can act as an initial filter, flagging potential stories that might otherwise go unnoticed in the sheer volume of documents and data.
In summary, AI can be used for news discovery by:
Searching social media for breaking stories
Reading government reports or other documents
Analyzing patterns in large datasets
News Gathering
During the 
newsgathering 
phase, journalists collect background information about potential stories. This process often involves several key tasks: data analysis, background research, transcription, and fact-checking.
AI 
data analysis
 tools allow journalists to extract key information and trends from relevant datasets. For example, 
The Colonist Report,
 an independent Nigerian newsroom, used ChatGPT and Gemini to analyze and visualize data from over 3,000 government documents for a flooding investigation.
GenAI has also shown promise as a tool for efficient 
background research
. Some newsrooms, such as 
Bonnier News
, develop their own AI research tools. For newsrooms that lack the resources to create in-house solutions, the release of 
web search tools
 and Deep Research products from 
OpenAI
, 
Google
, and 
Perplexity
 also offers a potential solution.
AI is also useful for 
interview transcription
. Tools that provide audio transcription can be found in nearly every newsroom, including the BBC, NBC, and AP. Once a transcription is generated, journalists can quickly extract key quotes or generate summaries. For journalists working with unfamiliar languages, AI can also be used for translating these transcribed interviews.
Finally, 
fact-checking
 and 
source verification
 are essential parts of the newsgathering process. Newsrooms can use AI to monitor and flag potential misinformation. One example is 
Project VERDAD
, which tracks misinformation from radio broadcasts in the U.S. Another is a tool developed by the 
Spiegel Group
, which scores claims based on accuracy before being sent for human review. The development of these tools tends to be slower, with newsrooms carefully implementing them with human oversight and verification. Using AI to assess the truthfulness of claims — rather than simply flagging them — can be controversial: 
Full Fact’s health misinformation detection tool
 stresses the use of AI for “triage” rather than actual fact checking. These automated systems require guardrails to make sure they accurately review information and maintain newsroom trust. Another 
GAIN
 article provides more information on the promise and challenges of these kinds of tools.
In summary, AI can be used for news gathering by:
Analyzing data, including visualization for exploratory analysis
Assisting with background research
Transcribing and translating audio from interviews
Detecting potential misinformation & verifying claims
News Production
News production is all about the actual creation of news content: writing articles, recording podcasts, or filming videos. Organizations must intentionally decide how and where AI can be used to make content — and the line between 
what is and isn’t acceptable is sometimes blurry
. Some newsrooms have already 
given the reins
 to generative AI, but most won’t allow LLM content to be published without a human monitoring every step of the process.
Generative AI’s arguably most recognizable feature is its ability to write, but few organizations allow journalists to draft entire articles using AI. Some, however, allow journalists to work 
with
 GenAI as they write. The Scandinavian newspaper 
Aftonbladet
, for example, provides journalists with “Buddy Reader”, an AI tool used for proofreading and feedback. After articles are drafted, the AI can offer suggestions for headlines and a title. Organizations such as the AP and the 
New York Times
 are among many organizations that allow this use-case, albeit with 
strict human review and oversight
.
AI can also be used to generate data visualizations or 
article images
. For video journalism, it can be used to generate
 rough cuts of videos
 or even 
short clips
. And AI generated media and headlines tend to perform well: the images generated by Bonnier News’ BonsAI 
improved CTR in some cases by 100%
.
Just as journalists can use AI for summarizing piles of documents and reports during the news gathering process, journalists can use AI to summarize their own articles. 
Major organizations
 have begun to implement AI-generated summarization into their news production process. These summaries 
aren’t perfect
, but are generally framed as being a beneficial time-saver for journalists. Summaries can also be generated for articles published in the past, which helps make older articles more accessible.
In some cases, new organizations are looking towards generative AI to improve automatically written articles. These types of articles — which have been published long before generative AI — usually deal in data-rich domains such as in finance for stock market updates or local sports 
game recaps
. Compared to prior methods, generative AI can offer more natural language and thorough explanations for these pieces.
In summary, AI can be used for news production by:
Outlining, drafting, editing, or proofreading articles
Suggesting headlines or article titles
Generating visual material such as data visualizations and images
Creating background music or b-roll clips for video journalism
Summarizing article and other news content
Improving fully-automated articles in data-rich domains
Content Transformation
Generative AI improves the accuracy and accessibility of tools like 
translation
 and text-to-speech for news organizations. Text-to-speech features for articles are becoming more mainstream, such as 
AI-generated audio of newsletters
. NBC even 
recreated Al Michaels’ voice
 for Summer Olympics recaps. These use-cases exemplify how AI can be used to make the news more dynamic, accessible, and fun.
Transformation can also involve revising the writing style of articles for different audiences. An article’s language could be altered depending on whether it is being read by a researcher or a child. 
Particle News
, a news curation app, allows users to pick the style of the article summary, such as comparing and contrasting opposing viewpoints or explaining to the reader like they’re five.
An exciting new application for AI is multimedia transformation. Articles can be turned into listenable podcasts (or vice versa). The GAIN blog released
 an article
 explaining how AI tools can help adapt an article into a video. These transformations can occur between any and all mediums. For instance, the BBC 
is currently working on a project to “reformat” existing content
, like turning live sports broadcasts into article recaps.
In summary, AI can be used for content transformation by:
Translating articles and enabling text-to-speech
Revising writing style to better suit the reader
Adapting content to different types of media
Content Management
Internally, newsrooms have found GenAI valuable for automated tagging, keyword suggestions, and metadata organization. Take 
Reuters
, which developed an AI system that finds and tags public figures in photos and videos so journalists can find them later on.
Other use cases in this category include search engine optimization (SEO), 
automated time-stamping
 of videos and podcasts, and other 
internal analysis of news coverage
. The 
Aftonbladet
 team, for example, used AI to analyze 100,000 articles to reveal a significant gender gap in the content they produced. Applications of AI for managing content can thus take different forms, whether organizing internal archives or analyzing content to shape editorial decisions.
In summary, AI can be used for content management by:
Providing tags for articles
Generating keywords and metadata for images and articles
Organizing and categorizing new and pre-existing content
Time-stamping for video and audio content
Improving SEO strategies
Identifying potential areas of improvement or 
gaps in perspectives
Content Distribution
On the business side of news organizations, GenAI has been effective for optimizing existing practices like advertising, paywalls, and recommendations.
Using data from user interaction on the platform, AI algorithms are able to provide better recommendations and user experiences. 
Forbes.cz developed a recommendation tool
 that analyzes which content is consumed and how a user scrolls through it. The developers saw improvements over manual recommendations, increasing the number of page views by 20–25%.
Dynamic paywalls offer another level of site personalization, allowing organizations to intelligently choose who sees a paywall and when. These decisions could be based on user engagement, number of page views, how the user is navigating the site, and many other data-driven factors.
Organizations are also using AI for advertising and marketing. Generative AI offers new ways to analyze user data and provide more targeted recommendations. The New York Times launched 
BrandMatch
, which offers more specific targeting criteria than traditional approaches. 
Non-profit news organizations
 have also used AI to craft marketing and fundraising campaigns — optimizing administrative tasks, email outreach, and curating automated newsletters.
In summary, AI can be used for content distribution by:
Recommending content to users
Implementing dynamic paywalls
Advertising & marketing content
Creating and distributing newsletters
Aggregating news content and highlighting key information
Content Interaction
The final category of use-cases for generative AI in the newsroom is content interaction — how organizations allow audiences to engage with their coverage. These cases tend to be relatively newer.
The first use-case is semantic site search, allowing users to use natural language and unspecific keywords to access articles. 
Forbes Adelade
 is a search tool that also provides a summary for quick answers. One step further than simple semantic search is an AI chatbot, such as the 
Washington Post’s Ask Post AI
 and the 
TIME person-of-the-year chatbot
. These chatbots plug into a newsroom’s database of information and articles, allowing them to provide a brief answer, related information, and recommendations for further reading on site.
The Washington Post is also testing 
AI-moderated comment sections
 next to articles. These potentially foster a more productive discussion space, summarizing the main conversation and highlighting comments that are particularly of note.
Organizations can also provide users AI-driven analysis on the political bias of news coverage and opinion pieces, such as 
L.A. Times Insights
. This analysis is run independently of the L.A. Times newsroom and is powered by Particle News.
In summary, AI can be used for content interaction by:
Allowing users to semantically search news sites
Powering a chatbot that the user can directly converse with
Moderating comment sections
Analyzing and communicating the political lean of news coverage
Conclusion
This article only scratches the surface of generative AI’s use cases in the newsroom. Organizations are constantly implementing AI in new and creative ways. Right now, many projects are focused on optimizing pre-established workflows such gathering, production, and distribution. These use-cases showcase AI’s ability to derive signals from noise — expediting the traditionally tedious “needle-in-a-haystack” tasks for journalists, for example. However, this is changing. Generative AI is giving users new ways to consume the news: chatbots, customizable article lengths and styles, and cross-medium transformations. For the foreseeable future, many of these use-cases will require a human in the loop, but that doesn’t mean we shouldn’t use AI to push at the boundaries of what journalism can be.
Here’s the 
summary table of all the use cases that informed 
this article. We’ve found the below sources helpful for staying on top of new developments and which can maybe inspire the next new use-case for AI in journalism:
News and Newsletters
Artificial intelligence | MIT News
AI News
Simon Willison’s Weblog
AI in the News | Newsletter by Florent Daudens
AI in Action | Newsletter by IBM
Google News Initiative
Ben’s Bites
The Rundown AI
The Batch | DeepLearning.AI | AI News & Insights
Every
AI Weekly
Tools & Case Studies
LSE Journalism AI Case Study Table
LIVE TRACKER: GenAI in media — 90+ tools and products from around the world (in English and in Finnish) | Numeroiden takaa
Journalist’s Toolbox of New AI Tools
AI Tools for Local Newsrooms
AI in the Newsroom: Case Study Series — ONA Resources Center
Simon Willison: Project]]></content:encoded>
</item><item>
<title><![CDATA[How Full Fact Uses Generative AI to Find Harmful Health Advice]]></title>
<link>https://generative-ai-newsroom.com/how-full-fact-uses-generative-ai-to-find-harmful-health-advice-b0e05382ab90?source=collection_home_page----df04cefca135-----2-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Misinformation]]></category><category><![CDATA[Fact Checking]]></category><category><![CDATA[Multimodal Ai]]></category><category><![CDATA[Gemini]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Clare Spencer]]></dc:creator>
<pubDate>2025-05-01</pubDate>
<atom:updated>2025-05-01</atom:updated>
<content:encoded><![CDATA[How Full Fact Uses Generative AI to Find Harmful Health Advice
From multimodal misinformation detection to harm rating and ranking
Clare Spencer
Follow
8 min read
·
May 1, 2025
--
1
Listen
Share
Influencer Belle Gibson was depicted in a Netflix drama as telling a lie that a tapeworm came out of her mouth after she drank apple cider vinegar.
The Netflix series Apple Cider Vinegar has brought back into public consciousness the threat of spreading health disinformation and made me curious about how, in 2025, we can use generative AI to identify influencers’ dangerous advice. The drama is loosely based on a real life newspaper investigation in the early 2010s into Instagram influencer Belle Gibson. She said she had terminal brain cancer, had rejected conventional treatments and had healed herself partly through nutritious meals. Only, she never had brain cancer and her advice was potentially fatal. The journalists Beau Donelly and Nick Toscano 
reportedly
 exposed Belle’s lies after they were tipped off by her friend who knew an editor at the Australian newspaper The Age.
Fast forward to 2025 and tip offs can be much more high-tech. For example, the fact checking organisation 
Full Fact 
recently integrated Google’s multimodal large language model Gemini into a prototype system that can help identify potentially harmful health claims. It’s a compelling case of how using generative AI can help deal with the gargantuan task of monitoring an overwhelming quantity of content. Just maybe the future will have less Apple Cider Vinegar-type drama.
Full Fact’s fact checker Leo Benedictus describes his work as “at the extreme end of the thoroughness scale”. Good for facts, bad for speed. One of the first times he tried out the new system, code-named “Raphael”, was in July 2024 when he needed to quickly identify dangerous health claims in a two-hour+ interview. For his hugely popular podcast Diary of a CEO, Steven Bartlett had interviewed Dr. Aseem Malhotra who had become infamous during the pandemic for spreading misinformation about Covid vaccines. Leo described it as a “classic dilemma for fact checkers” in that within the interview there were too many claims for him to be able to assess.
In this case Leo used Raphael to pull out the main claims and accelerate his work. For instance, the system helped him identify statements from Dr. Malhotra such as: “I have come to the conclusion that the Covid vaccine introduction has had a catastrophic net negative effect on the population and society”. Leo said he saved a lot of time because Raphael linked back to the original content so he could go through and listen to the original. “When I see those claims I can then click on them and it will play the exact moment of the video. So I’m never relying on it. I’m using it to say ‘how about this?’ and then I listen. That’s a quick easy process, I don’t have to open the video and find the time stamp or anything.” Shortly after Steven Bartlett shared the podcast, 
Full Fact published an article
 disputing Dr. Malhotra’s claim and pointing to evidence that Covid-19 vaccines saved far more lives than they cost.
This one interview is not even a drop in the ocean. “The scale of content on YouTube alone of health videos is enormous. There is no way that human fact checkers can monitor it,” Kate Wilkinson, an AI product manager for Full Fact told me.
Multimodal is the Future
Kate is particularly excited about possibilities brought about by technological advances in multimodal generative AI which they use to process information from different types of input such as text, images, audio, and video and combine them to generate one summary.
“What is harder, especially now with the rise of short form video, is the fact that misinformation is often not explicit, it’s inferred. Often misinformation is comprised of different mediums in one piece of content,” she said. In short videos, like those on TikTok, it is common for it to include a video, different audio and separate text on screen. “Individually they may not constitute misinformation. But when viewed together as a whole, there is an inference that is actually misinformation.”
To illustrate the challenge of multiple different media in one piece of content she uses the example of a TikTok video of someone falling over with text on screen saying “me in 5 years after taking that Covid vaccine”.
Full Fact’s use of multimodal generative AI can detect visual, text and audio and put them together to detect inferred messages
“We’re able to find potential health misinformation that isn’t explicit… We’ve got a video where someone is doing something. We’ve got text on screen, which is ‘me in 5 years after taking that Covid vaccine’. And then we’ve got a caption which says, ‘Just jokes, I hope.’ So, I think there’s a bit of tongue and cheek stuff happening here. But the model is able to infer that what is actually being suggested here is that if you take the Covid vaccine, you could have long-term health problems.”
“It’s a huge step for us and we hope our users are going to be able to monitor subtle implied suggested misinformation that we haven’t been able to historically detect,” Kate told me.
How Full Fact Built Raphael
To start building Raphael, in 2024, Full Fact used expert health fact checkers to annotate around 500 claims. The annotators’ metrics included if the health information was clear, the type of health misinformation such as if claims were about health, treatments, diseases, or symptoms, and the severity of the potential harm. The categories of severity of harm were:
High harm
: Believing this claim is likely to directly cause serious medical harm to individuals or groups.
Some harm
: Believing this claim is likely to directly cause a limited amount of harm to some individuals.
Low harm
: Believing this claim is likely to directly cause only a small amount of harm.
Indirect harm
: Believing this claim is likely to cause harm by leading people to avoid legitimate medical advice or treatment.
Harmless
: Believing this claim is unlikely to cause any harm to an individual.
Can’t tell
: The potential harm cannot be determined based on the available information.
They used that information in their prompt for Google’s multimodal large language model Gemini. “Because the context window is so large, we just threw that annotated data into the prompt every time that we processed a video and said here are examples of claims,” said Kate. They used Gemini 1.5-flash-001 and intend to replace that with the 2.0 Flash model because 
Google has announced that
 1.5 is being discontinued in May.
In 2024, 
Full Fact joined Google.org’s generative AI accelerator
 where Google provided mentors to advise the project and Cloud credits to support the computing expenses. This made choosing the Gemini model the obvious choice for the project.
She warns that generative AI presents some specific challenges: “You can run a video 10 times and it can give you 10 different responses.” Humans are similar insofar as different people may exhibit subjective variation in labeling claims too. Despite this she said the feedback she received from testing it with around 60 fact checking organizations is that the tool helps with scale and speed of monitoring.
Use GenAI for Triage Not Checking Facts
The benefit of scale is important to Full Fact’s head of AI Andy Dudfield.
“There are so many more things that we can fact check than we have the ability to fact check. We are detecting maybe half a million things we can fact check each day, Full Fact’s quite a big Fact Checker and we have 12 fact checkers,” 
he told the audience of the International Journalism Festival
 in Perugia.
The generative AI prototype Raphael builds on Full Fact’s use of natural language processing (NLP). Andy describes this as “not the most cutting edge stuff” but “boring magic that makes our lives easier and this is where we find the most useful use of technology”.
More specifically, the “boring magic” helps Full Fact “enrich” sentences. “We take a look at who said it, where did they say it, what were they talking about, was there a number involved. So we can start to break down the content by patterns that sit inside it.” These legacy AI systems relied mainly on keyword and topic detection. Now with generative AI Full Fact can rank health-related claims by potential harm.
Andy shared the Full Fact’s list, generated with the help of generative AI, of the top ten most harmful looking health claims on the day.
Full fact uses AI to identify the most harmful looking health claims
They also use AI to search through and find repeats of what Full Fact have already fact checked and then automatically alert fact checkers. This is something they could do before with NLP and are planning to replace with generative AI. “A fact checker can come in in the morning and see a page that roughly looks like this [below], the bold is stuff we previously fact checked, the stuff underneath it is potential repeats. And they can just work their way through and say ‘ok on this podcast, I didn’t have to listen to, I’ve been alerted by technology that someone has said something that we have already fact checked’.”
Full Fact automated monitoring of repeats of what they had already fact checked
AI helps “in the process of identifying what we should fact check each day. It’s really good at that. We use it to find repeats of things that we previously fact checked because we want to understand our impact, we want to understand the media information ecosystem”.
Using AI for triage rather than fact checking is a key distinction which defines this use of AI for fact checking. “We do not use AI to do fact checking because I do not think it is currently a good idea. AI is developing very very fast but right now I do not think that would be a sensible thing for us to do”, Andy said in Perugia.
Andy Dudfield told the audience of the International Journalism Festival that Full Fact identifies around half a million claims to check every day
Andy warns against the dangers of using generative AI to scrutinize claims. “Even if you train something on the best data and still it’s a generative model, it’s going to make something up,” he told me.
Full Fact are currently using Raphael internally and intend to incorporate it into their main suite of tools for clients, including other fact checking organizations, later in 2025.]]></content:encoded>
</item><item>
<title><![CDATA[The Osmotic Approach to News Recommendation]]></title>
<link>https://generative-ai-newsroom.com/the-osmotic-approach-to-news-recommendation-b0a6d0784360?source=collection_home_page----df04cefca135-----3-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Recommender Systems]]></category>
<dc:creator><![CDATA[Rupert Kiddle]]></dc:creator>
<pubDate>2025-04-29</pubDate>
<atom:updated>2025-04-29</atom:updated>
<content:encoded><![CDATA[The Osmotic Approach to News Recommendation
Balancing Novelty and Familiarity with Text Embeddings
Rupert Kiddle
Follow
7 min read
·
Apr 29, 2025
--
1
Listen
Share
Promoting exploratory pathways through news— Image generated with DALL·E
This post is part of a series based on research presented at the 
workshop on News Futures at the CHI 2025 conference
.
News organizations face a fundamental challenge with recommendation systems: How do they provide valuable content suggestions without extensive knowledge about their individual readers? The majority of state-of-the-art recommendation techniques share a common requirement: they rely on 
extensive user behaviour
 to function effectively. This is commonly referred to as the “cold start” problem. For most news organizations, this requirement presents both practical and ethical challenges.
Practically speaking, a significant portion of news audiences browse anonymously, and individual users may be difficult to track consistently between sessions. Ethically, news organizations face increasingly stringent regulatory hurdles around the collection and use of personal data, such as those imposed by the 
Digital Services Act
 in Europe. This restricts those organizations’ abilities to implement traditional recommendation systems focused on personalizing news articles to individual users’ tastes.
In this context, text similarity-based approaches offer a promising alternative. The premise is simple: identify semantically related content to offer users 
navigable pathways
 through journalistic coverage via “next item” or “more like this” recommendations. By focusing on 
content-content
 relationships rather than 
content-user
 relationships, recommendation can guide readers in their exploration of daily news coverage while decreasing reliance on their personal data, potentially limited to just a few clicks from a single anonymous browsing session.
However, implementing effective text-similarity-based news recommendation is difficult to get right. 
Research reveals
 what has been characterized as the “
Goldilocks zone
.” Recommendations that are too similar can lead to 
filter bubbles
, lack of exploration, and user boredom. Conversely, recommendations that are too different can be perceived as 
disconnected or chaotic
, resulting in poor engagement. The ideal approach must walk a fine line — 
familiar
 enough to maintain relevance while introducing sufficient 
novelty
 in recommendations to support 
serendipitous discovery
 of journalistic content.
In our work, we navigate these tensions by engineering a text-based recommendation system that operates within this Goldilocks zone while maintaining minimal data requirements. We use text embeddings (vector-based representations) to represent the 
subjects
 (the people, organizations, and locations) and 
contexts
 (the broader themes) of news articles and leverage these to make strategic suggestions (“pivots”) that either focus on the same context but different subject, or the same subject in different contexts. Through these strategic pivots, we aim to balance familiarity with novelty in the recommendations made to users. A user study with 732 news readers in the Netherlands demonstrates that this approach can support diverse content exploration without compromising user experience.
Bisociation in Information Behaviour
To address the Goldilocks challenge identified above, we draw on the concept of “
bisociation
.” This is a cognitive mechanism first described by Arthur Koestler where meaningful insights emerge from perceiving phenomena through multiple frames of reference simultaneously. Rather than simply finding similar content, we strategically create connections between related but distinct knowledge domains.
This approach aligns with natural human information-seeking behavior. Studies of large-scale Wikipedia browsing patterns 
have revealed
 that users naturally engage in “dancer-like” exploration — making purposeful leaps between seemingly disconnected topics that can potentiate deeper, 
more integrated learning
.
We can strategically support this natural bisociative behavior in news recommendation through the use of 
text embeddings
. These are mathematical representations that capture semantic relationships between texts. Unlike simpler alternatives such as keyword matching, embeddings encode deeper latent connections that exist beneath surface-level similarity.
We call our approach “osmotic recommendation” because it increases permeability between different knowledge domains within journalism. Our system identifies semantic bridges between distant topic areas, revealing natural pathways users might otherwise miss. This enables readers to make valuable bisociative connections while maintaining relevance to their current context — balancing familiarity with novelty to find 
just the right
 degree of news article similarity.
Implementing the Osmotic Approach
To implement our osmotic approach in practice, we decompose article similarity into two fundamental dimensions:
A
 subject dimension
: which captures the named entities (e.g., people, organizations, locations)
A
 context dimension
 which describes the broader themes (e.g., phrases such as “environmental regulations” or “gender equality”)
Since our implementation focused on Dutch news articles, we utilized the 
RobBERT-v2-Dutch-Base
 embedding model from Hugging Face to generate two vector representations for each article: a 
subject embedding
 and a 
context embedding
, using the salience-weighted terms associated with each dimension.
This decomposition allows us to offer readers two recommendation pathways:
 subject pivots
 and 
context pivots
. These pathways maximize similarity along one dimension while minimizing it along the other, creating two types of 
bisociative bridges
 that users can take between news articles.
An example of osmotic news recommendation in action: we first break down a reference news article into subject and context embeddings, and then recommend “pivoted” news articles to read next by holding one dimension constant and seeking novelty in the other.
The figure above demonstrates this process with an example: a reader engaged with an article about Mexico’s first female president taking office. Our system extracted both subject and context embeddings from this input text, then generated two types of recommendations:
A 
subject pivot
 to an article titled “Mexico’s Drug Cartels Appear to Play Greater Role in Elections than Ever Before” (maintaining focus on the Mexican political entities, but shifting to themes of organized crime).
A 
context pivot
 to an article titled “Diane Abbott says she intends to ‘run and win’ as Labour candidate” (maintaining thematic focus on female political leadership, but in an entirely different geographic and political setting).
By offering these alternative pathways, we increase the likelihood that at least one recommendation will strike the right balance between familiarity and novelty, falling within the Goldilocks zone that promotes both engagement and exploration.
Research Evidence: Diversity Without the Cost
We tested this approach with 732 participants in The Netherlands. The experiment presented previous days’ news articles in a controlled setting. Participants first selected a headline of interest, then engaged in two sequential recommendation rounds. During each round, they evaluated four types of recommendations (subject pivots, context pivots, most-similar, and random) on relevance, surprise, and satisfaction, then they selected their preferred article, read it, and proceeded to the next round. The session concluded with participants rating to what extent they would value similar recommendations to those that they chose, on their favorite news website.
Our study revealed three key findings:
Higher “forward flow”
: pivot recommendations sustained twice the semantic distance between articles compared to the similarity-based approach. This “
forward flow
,” which measures the conceptual distance traveled during a reading session, demonstrated the method’s ability to effectively bridge different knowledge domains.
Equivalent satisfaction
: despite this increased distance, pivot recommendations maintained user satisfaction levels equivalent to similarity-based approaches, suggesting they found the right balance of novelty and familiarity, addressing the 
typical trade-off
 between recommendation diversity and user satisfaction.
Evaluation vs. Selection Behavior
: While selection of similarity-based recommendations depended heavily on high item rankings, users chose pivot recommendations particularly when these were only ranked moderately across evaluation dimensions. This disconnect between subjective evaluation metrics and actual selection behavior highlights an interesting gap between how users rate content and what they ultimately choose to engage with.
Our findings demonstrate that carefully engineered connections between news stories can help readers explore diverse stories without compromising user satisfaction (e.g., due to disinterest, unfamiliarity), providing an algorithmic approach to exploiting the “Goldilocks zone” of text similarity.
Future Directions: Osmotic Network Discovery
While our research focused on subject and context dimensions, the osmotic approach opens possibilities for a rich ecosystem of semantic bridges between news articles. For example:
Temporal pivots
 connecting stories across different time periods
Scale pivots
 linking local and global manifestations of phenomena
Perspective pivots
 bridging different viewpoints on similar topics
Causal pivots
 connecting stories sharing similar underlying mechanisms
Beyond these prescribed connections, news organizations could discover organic pivot dimensions through privacy-conscious analysis of aggregated clickstream data. By examining patterns in how readers naturally navigate between semantically diverse articles, publishers can identify which bisociative bridges resonate most with their specific audiences. These discovered pathways could then be engineered into recommendation systems, creating an evolving 
osmotic network
 that adapts to audience exploration preferences while maintaining the crucial balance between familiarity and novelty required for serendipitous discovery.
Our work demonstrates that text embeddings, when thoughtfully decomposed, can serve both journalistic values and audience engagement in news recommendation. The osmotic approach offers a compelling solution to a fundamental challenge in digital news: how to promote content diversity without compromising user experience or privacy. By creating permeable boundaries between knowledge domains, we assist readers to move fluidly across the information landscape, discovering unexpected connections that enrich their understanding of complex issues. In an era of increasing media fragmentation, this lightweight yet powerful approach serves both readers seeking broader perspectives and publishers navigating an increasingly challenging digital ecosystem.
This blog post is based on research co-authored by Rupert Kiddle (Vrije University Amsterdam) Anne Kroon (University of Amsterdam), Kasper Welbers (Vrije University Amsterdam), and Damian Trilling (Vrije University Amsterdam).
This research is part of a project that has received funding from the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (Grant agreement №947695).]]></content:encoded>
</item><item>
<title><![CDATA[What’s Political on TikTok? A User-Centered Approach]]></title>
<link>https://generative-ai-newsroom.com/whats-political-on-tiktok-a-user-centered-approach-7d965dabfb27?source=collection_home_page----df04cefca135-----4-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Politics]]></category><category><![CDATA[News]]></category><category><![CDATA[Llms]]></category><category><![CDATA[Hci]]></category><category><![CDATA[Tiktok App]]></category>
<dc:creator><![CDATA[Stephanie Wang]]></dc:creator>
<pubDate>2025-04-24</pubDate>
<atom:updated>2025-04-24</atom:updated>
<content:encoded><![CDATA[What’s Political on TikTok? A User-Centered Approach
Using audience perspectives and LLMs to map out the political landscape on TikTok.
Stephanie Wang
Follow
7 min read
·
Apr 24, 2025
--
Listen
Share
By 
Stephanie Wang
, Jason Greenfield & 
Danaé Metaxa
.
This post is part of a series based on research being presented at the 
workshop on News Futures at the CHI 2025 conference
.
With over 
1.5 billion users worldwide
, TikTok is rapidly growing in popularity. While 
a third of U.S. adults now use the platform
, we still know little about the nature of political content on TikTok, or how users interpret it. At the same time, U.S. lawmakers have voiced 
growing concerns
 that TikTok poses a national security risk due to its ownership by the Chinese company ByteDance. But without independent research, it’s hard to assess whether fears about political extremism or polarization are justified. We begin with a couple of fundamental questions: what do users actually see, and what do they consider political on TikTok?
Towards answering these, we introduce a user-centered approach to understanding political content on TikTok — one that foregrounds users’ own perceptions of what constitutes political content, rather than relying on top-down definitions. We surveyed 368 U.S.-based TikTok users about their experiences with political content on the platform, and collected annotations and video data from their personalized “For You” feeds using a custom browser extension. Based on the patterns in these survey responses, we created a taxonomy of different categories to describe the data. We applied this taxonomy to both user annotations and video transcripts. This data allows us to understand what kinds of content users considered political on TikTok, and what specific videos they labeled as political, and why.
Our study shows that users identify more issue-based content (like race and gender rights) as being political, rather than formal political topics in their feeds. Augmenting human experts with an LLM to analyze TikTok videos helped us efficiently map this landscape, offering a model for researchers and newsrooms who aim to understand and audit platform content. However, we discovered that LLMs also struggle to identify news content without prominent visual cues, which has implications for the development of content analysis pipelines for TikTok or other kinds of short-form video.
Studying Political Content on TikTok From the User Perspective
Screenshot of the web tool running on Tiktok. Users can rate the video with one of the three radio buttons, explain their choice, and submit the answer. Additionally, they can see how many videos they rated with the progress bar to the right of the Tiktok logo and scroll through the Frequently Asked Questions section on the bottom right of the page.
We recruited a politically balanced and demographically diverse sample of 368 TikTok users based in the United States. Participants first responded to an open-ended survey prompt: 
How would you describe the kinds of TikTok content you consider to be political, and what makes it political to you? 
Then, using a 
custom browser extension
, participants logged into their personal TikTok accounts, scrolled through their “For You” feeds, and annotated 40 consecutive videos. For each video, they indicated whether or not they perceived it as political and briefly explained why. To validate this self-reported data, we randomly sampled one political and one non-political annotation per participant and manually assessed whether their description aligned with the video content.
We then developed a set of 31 categories based on patterns we found in survey responses. We used these categories to label data from the survey responses (N=368), user annotations of political videos (N=2,171), and the video transcripts themselves (N=1,115). For instance, we labelled whether responses pertained to topics such as “healthcare” or “housing”, or specific government institutions such as “SCOTUS”.
Scaling Data Analysis with LLMs
To scale our analysis, we combined expert human coding with a large language model (LLM). We used OpenAI’s GPT-4o model via the Chat Completions API, with temperature set to 0 to ensure consistent and deterministic outputs in our classification task. The LLM was prompted with the following, where “{codebook}” is a dictionary mapping the abbrevaited category name to its description, and “{text}” is the user description (replaced with “{transcript}” when we prompted the LLM to code the video transcripts):
Given the following predefined codes for political content: {codebook}, classify the following text into one or more of these categories: “{text}”. Respond with ONLY the category name (e.g., TRUMP, RACE, EDU, IMG).
We refined our set of categories by clarifying ambiguous definitions, reducing overlap among categories and reducing vagueness by adding examples. To illustrate, the category “Education” was amended from “Any mention of education”, to “Any mention of schools, education policy, or student loans, including references to Affirmative Action” to better capture the breadth of education-related political discourse, particularly in the context of 
contemporaneous events
.
User Perceptions of Political Content
Percentage of all codes in participant survey responses to what they find to be political on TikTok. The highest percentage codes suggest that participants use general language to describe political content instead of references to specific policy issues.
Our study found that TikTok users identified an average of 17.3% of videos in their feeds as political content. The content that participants considered political covered a broad range of topics, from formal political entities like politicians, legislation, and elections to issue-based themes like healthcare, abortion, and COVID-19.
A Disconnect Between Survey Responses and Real Feed Annotations
Example survey responses and user annotations. Each row also includes how the participant response was coded in our data and whether those codes are formal political, issue-specific, or both.
Analyzing survey responses, we found that participants most frequently mentioned formal entities such as Politicians, Elections, and Government, while specific policy issues (e.g., Economy, Abortion, Immigration) were less frequently mentioned. However, when users labelled the individual TikTok videos in their feeds, a different pattern emerged. The most common themes in these annotations centered on social issues such as Gender and Sexuality Rights (“LGBT person being discriminated against at work”), Race (“This is about systemic racism”), Police (“Police brutality”), and Economy (“This video speaks of the financial strain of inflation in America on young people.”).
This shift suggests a contrast between how people conceptualize political content in the abstract versus what they actually encounter in their daily TikTok experience — suggesting a form of saliency bias in how users recall political content.
Where is the news?
Given TikTok’s rising 
popularity as a news source
 for U.S. adults, we aimed to identify news content in our data but encountered challenges in reliably classifying it. Our LLM-based pipeline showed low agreement with human experts when identifying news content in video transcripts. For instance, a clip from 
The Rachel Maddow Show
 might not be labeled as news if the transcript lacked key visual or branding cues. The LLM-based pipeline used in the study performed poorly at labeling such content based on video transcripts. In future work we plan to incorporate visual inputs to the LLMs to better classify news content.
Turning instead to our manual review of videos, we observed few political videos posted by official news organizations. Instead, TikTok videos were likely to feature clips from news programs reposted by influencers or unaffiliated users. We hypothesize that traditional news media accounts have yet to fully breach TikTok, despite 
52% of American TikTok users reporting they regularly get news from the platform
 and the News category appearing in 16% of user survey responses. As news content is commonly used as a proxy for political content in social media research, our finding suggests that studies that solely measure user exposure news to approximate exposure to political content may come up short and miss the bigger political landscape when studying TikTok.
Implications
This study contributes a new user-centered method for understanding political content on TikTok, one that centers real-world exposure and user perception over traditional approaches that rely on political keywords, public figures, or news content.
Our findings suggest several important implications for future research, as well as for practitioners looking to understand their audiences or use LLMs as a tool in this process.
First, because TikTok’s algorithm is highly personalized and user-level data is difficult to access, surveys are a common method for studying political content on the platform. Our findings suggest that survey-based measures can provide a useful high-level view of what TikTok users deem to be “political” content. However, we also observed a disconnect: in surveys, participants emphasized formal political content (Politicians, Elections), but when reviewing their actual feeds, they labeled more issue-based content (Race, Gender/Sexuality Rights, Police). This highlights the added value of in-situ feed data in offering a more precise view of political content on the platform.
Second, analyses focusing on traditional news may not be optimal for studying TikTok. In both user annotations and our manual review of videos, we saw few official news outlets. Instead, news content frequently appeared in repurposed formats, such as clips from TV segments shared by influencers, making detection based on standard news categories unreliable.
Finally, our exploratory work using a codebook + LLM approach suggests a promising direction for scalable analysis of political content. While strong human-LLM agreement was not achieved for all categories, performance was adequate for some codes. For other categories, like News, researchers could also try to leverage the full multimodal nature of TikTok videos, incorporating visual features into our audio analysis pipeline to improve detection.]]></content:encoded>
</item><item>
<title><![CDATA[Building Cross-Functional AI Teams in Newsrooms: Navigating Challenges and Opportunities]]></title>
<link>https://generative-ai-newsroom.com/building-cross-functional-ai-teams-in-newsrooms-navigating-challenges-and-opportunities-a7997a7d81f9?source=collection_home_page----df04cefca135-----5-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[News]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Media]]></category><category><![CDATA[Human Computer Design]]></category>
<dc:creator><![CDATA[Qing Xiao]]></dc:creator>
<pubDate>2025-04-23</pubDate>
<atom:updated>2025-04-23</atom:updated>
<content:encoded><![CDATA[Building Cross-Functional AI Teams in Newsrooms: Navigating Challenges and Opportunities
Lessons from the field on bridging editorial and technical divides
Qing Xiao
Follow
8 min read
·
Apr 23, 2025
--
Listen
Share
This post is part of a series based on research being presented at the 
workshop on News Futures at the CHI 2025 conference
.
Examples of the use of AI in Chinese news can be observed in cases such as AI-generated footage about (a) how migratory birds navigate their routes, and (b) how to protect oneself during severe convective weather. All AI-generated videos were labeled as ”AI Creation” in the top right corner of the screen.
Newsrooms are no longer only populated by editors, journalists, or designers. Walk into a modern news organization, and you might find a new group of professionals within: AI practitioners, and even data annotators, roles that previously had little connection to journalism.
This shift has created opportunities for news innovation but also introduced significant challenges, as journalists and AI professionals often operate with different goals and values. While journalists typically emphasize editorial values such as accountability and public service, AI practitioners may focus on system performance and technical feasibility. These priorities do not always align, and the divide is further deepened by mutual unfamiliarity: AI practitioners often lack awareness of newsroom norms, and journalists struggle to keep pace with AI’s rapid development. As cross-functional collaboration becomes a daily necessity rather than a buzzword, a central question emerges: how can two professions with such different languages and mindsets build a shared future in newsmaking?
To better understand the motivations, practices, and tensions behind this kind of collaboration, we conducted 
a study in partnership with several leading Chinese news organizations
. We interviewed 26 professionals, including 17 journalists, six AI technologists, and three data annotators, and hosted collaborative workshops to understand what more effective, and more equitable, newsroom collaboration for building AI tools could look like.
We found that, as Chinese newsrooms begin to build internal AI expertise, they face significant challenges in integrating AI into journalistic workflows due to steep learning curves of AI and the limitations of traditional adaptation strategies. Our research further reveals deep cultural divides between journalists and technologists, leading to misaligned expectations, underused tools. Crucial AI workers like data annotators are also excluded from communication. To move forward, we highlight the promise of shared language, co-design practices, and more equitable collaboration models that center both journalistic values and technical insight in future.
The Challenge of Building In-house Expertise for AI
Our first finding is that many leading Chinese news organizations have deliberately adopted a strategy of building cross-functional teams within newsrooms, bringing in AI engineers, algorithm researchers, and data annotators to work directly alongside journalists.However, in practice, journalists find that 
the strategies they previously used to adapt to emerging technologies such as data visualization are no longer effective with newer AI technologies
.
In the past, when technologies like data visualization or digital news platforms emerged, journalists either learned the relevant skills to harness these technologies themselves, often with support from journalism schools, or worked with external technical partners for short-term collaborations, such as those seen in VR news projects. These approaches worked reasonably well when the technologies were discrete, learnable, or limited to a few projects. But AI is different. Its learning curve is steep, involving complex subfields like machine learning, and its integration into the newsroom can be deep and expansive, touching everything from content generation and audience analysis to workflow automation.
Journalists find it increasingly difficult to gain enough expertise to use these tools 
meaningfully
, and short-term outsourcing models — which had worked for previous technologies — fail to keep pace with the scale and speed of AI’s influence. For a time, many newsrooms sought support through collaborations with tech companies, but these partnerships often came with trade-offs: journalists reported a lack of communication, low flexibility, and reduced autonomy in editorial decisions. In response, in addition to partnering with tech companies, some Chinese newsrooms have shifted toward building internal, cross-functional AI teams, allowing for more direct control over how AI is used in news production and enabling closer alignment between technological development and journalistic needs.
Cultural Divides Between Journalists and Technologists in the Newsroom
Our interviews revealed that collaboration between journalists and AI technologists in Chinese newsrooms typically follows a recognizable arc, though it doesn’t always involve building a full-fledged product. Sometimes, it’s simply about completing a computational task: running an analysis or extracting information.
These collaborations often begin with senior journalists identifying broad editorial goals, such as improving content recommendations or understanding audience behavior. From there, AI technologists step in to assess whether and how those goals can be addressed through computational methods. But this handoff is rarely seamless. Journalists are often vague about the technical specifics, while technologists may not fully understand the practical pressures of news production. As a result, aligning expectations requires rounds of back-and-forth translation between journalistic intentions and engineering feasibility.
Once the task is defined, AI professionals usually take the lead on execution, while journalists shift into a monitoring or advisory role. This division of labor is meant to ensure technical rigor, but it can limit opportunities for meaningful feedback. In cases where the task involves building a tool, journalists are typically involved in early planning and later evaluation, but in tasks that are purely analytical, their involvement may be minimal.
Regardless of the form it takes, a common tension runs through these collaborations: 
the tools or outputs are not always usable in practice
. Even when a computational task is successfully completed, it may not fit neatly into the daily workflows of journalists or meet their evolving needs. We also found that AI workers, such as data annotators, were almost completely excluded from cross-functional communication, even though the data they labeled was particularly important for the development of AI in China’s journalism. Our interviewees claimed that they had almost never participated in any training on journalism and lacked a clear promotion path, despite the fact that they all joined the industry with journalistic aspirations.
Implications for Improving Collaboration
We also brought together journalists, AI technologists, and AI workers to discuss the potential opportunities in future collaboration. One of the key insights that emerged was the importance of using clear, shared language across these different professions. Our participants emphasized that many misunderstandings stem from vague or overly technical terms that mean different things to different people. They suggested using “bridge words” or familiar analogies to help translate ideas across roles. For example, journalist participants shared that when working with AI-generated text, they often approached it as if they were editing a rough draft written by a human. This framing allowed them to focus on refining the final story to meet editorial standards, rather than getting bogged down in how the AI produced the content. On the other hand, AI developers explained that when journalists talked about concepts like “news fairness” or “editorial neutrality,” they had to interpret these ideas in computational terms, for instance, as objectives that could be encoded in natural language processing models. Our workshops revealed the need for more intentional communication tools and shared conceptual frameworks.
Another promising approach discussed during our workshops is co-design and prototyping, that is, having journalists and AI professionals work side-by-side to design tools and build early versions (or “prototypes”) of them together. Although none of the newsrooms we interviewed had formally adopted this method yet, many participants agreed that it held significant potential. Often, journalists find it hard to describe exactly what kind of AI support they need, especially when they are unfamiliar with what the technology can actually do. At the same time, AI developers may struggle to understand newsroom routines or editorial goals if they are only receiving abstract descriptions. Co-design offers a practical middle ground. By building simple prototypes, that are early, rough versions of a tool, or reading the initial result of computational analysis, journalists can see and touch the possibilities. Rather than trying to figure everything out through long meetings or vague written requests, co-design can create a shared space where both sides can learn from each other. It turns the development process into a conversation, one built not only with words, but with working examples.
In our workshops, participants described how co-design might unfold in practice. A typical process might begin with a brainstorming session where journalists explain a specific editorial challenge, such as monitoring social media for misinformation. Based on this input, AI developers would quickly build a low-fidelity prototype, for example a simple interface that clusters related comments or highlights named entities from documents. Journalists could then try out the prototype and respond with immediate reactions, what’s helpful, what’s confusing, and what’s missing. This direct feedback helps clarify journalists’ real needs and prevents developers from spending time building features that won’t be used.
Importantly, these sessions are not one-offs. Instead, participants imagined an iterative process where a prototype is updated, re-tested, and discussed again in short cycles. Each round helps both sides refine not just the tool, but their shared understanding of the problem. Over time, this back-and-forth builds trust and helps establish a common vocabulary that technical terms become clearer for journalists, and newsroom priorities become more tangible for developers. Co-design, in this way, doesn’t just produce better tools or solve tasks; it helps teams work better together.
Our participants also raised concerns about power dynamics within newsrooms, particularly in relation to how AI professionals and AI workers, such as data annotators, are positioned in the organizational hierarchy. Although these individuals play a crucial role in developing and maintaining the AI tools that journalists increasingly rely on, they are often placed in marginal or subordinate roles, with limited influence over key decisions. For example, AI engineers may be brought in only after editorial goals have already been set, reducing their contributions to mere execution rather than meaningful collaboration. Similarly, AI workers tasked with annotating data are typically excluded from broader discussions. Without more equitable participation, the design and use of AI systems risk being driven solely by managerial priorities, rather than by the shared values and needs of the entire newsroom team.
A Path Forward for Better Collaboration
To summarize, our research sheds light on how journalists, AI engineers, and data workers are beginning to engage in cross-functional collaboration inside newsrooms — something that’s still quite new and evolving. For both researchers and news practitioners, this opens up important questions: How do these different roles work together when they often have different goals, skills, and ways of thinking? What challenges come up when trying to build AI tools or conduct computational analysis that actually help journalism, rather than add more confusion or complexity? And where might these collaborations lead in the future? For journalists and media leaders, our findings offer a path toward answering these questions by rethinking team structures, training approaches, and workflow design for more inclusive AI collaborations.
Is your newsroom navigating these cross-disciplinary collaborations? We’d love to hear about your experiences, challenges, and successes in integrating AI and journalism.]]></content:encoded>
</item><item>
<title><![CDATA[Tackling the Transparency Puzzle]]></title>
<link>https://generative-ai-newsroom.com/tackling-the-transparency-puzzle-0969b3bcc489?source=collection_home_page----df04cefca135-----6-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Democracy]]></category><category><![CDATA[Transparency]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Hannes Cools]]></dc:creator>
<pubDate>2025-04-22</pubDate>
<atom:updated>2025-04-22</atom:updated>
<content:encoded><![CDATA[Tackling the Transparency Puzzle
Five Perspectives from AI Disclosure Research in News
Hannes Cools
Follow
9 min read
·
Apr 22, 2025
--
2
Listen
Share
Hanna Barakat & Cambridge Diversity Fund / 
https://betterimagesofai.org
 / 
https://creativecommons.org/licenses/by/4.0/
By 
Hannes Cools
, 
Claes de Vreese
, 
Abdo El Ali
, 
Natali Helberger
, 
Pooja Prajod
, 
Nicolas Mattis
, 
Sophie Morosoli
, 
Laurens Naudts
 & 
Teresa Weikmann
.
The question of whether and how to disclose the use of AI in news and journalism remains a complex and unresolved issue. While transparency is often seen as a key principle of journalism, the effectiveness of AI disclosures is far from clear. Research on disclosure effectiveness shows mixed results and remains inconclusive to a certain extent. Some studies indicate that audiences appreciate transparency, while others show that disclosures may have little impact on trust or may even reduce credibility if users perceive AI-generated content as less reliable. Furthermore, different stakeholders such as news organizations, policymakers, or audiences, may have conflicting expectations.
This raises some fundamental questions
:
What do we know about AI disclosures and the research conducted about them?
How do AI disclosures in news articles influence audience perceptions of credibility and trust?
Which types of audiences notice these AI disclosures, which groups does it benefit, and in what ways?
Without clear evidence of their effectiveness, AI disclosures risk becoming symbolic gestures rather than meaningful interventions. Complicating matters further, existing knowledge about AI disclosures in journalism is fragmented across multiple fields, leading to knowledge silos. For example, regulatory frameworks focus on legal obligations and ethical standards, while user experience (UX) research examines how design elements influence reader perception. At the same time, studies on audience preferences explore whether users actually want AI disclosures and how they interpret them.
In short, these often-siloed areas of research often operate independently, leading to gaps in our understanding of AI disclosures. In this article, we want to highlight AI disclosures in news organizations from the different disciplines and expertise present in the 
AI, Media & Democracy Lab 
where we work
.
 In doing so, we hope to contribute to solving the inherent complex puzzle that AI disclosures pose for news organizations and society. In addition, we offer some recommendations and considerations for further research.
Insights from News Organizations: Principles & Practices
The introduction of generative AI tools in late 2022 forced newsrooms to rapidly develop charters, principles, 
and guidelines addressing AI transparency and ethics
. Relatedly, one of the central questions that are put forward from the organizational level is:
How can news organizations implement coherent and consistent AI disclosure practices based on the guidelines they put forward without losing the audience’s trust?
The development of these principles aligns with 
journalism’s longstanding tradition of self-regulation
, where news organizations rely on press councils and internal guidelines rather than government oversight. These AI guidelines across news organizations have shown variability and ambiguity, particularly in the operationalization of core principles like human oversight, transparency, and bias mitigation.
While transparency is a widely endorsed principle, its practical implementation in the form of disclosures varies significantly across organizations. For instance, the Dutch Press Agency (ANP) adheres to a strict human>machine>human production chain, ensuring that humans oversee AI processes at every stage. Similarly, Nucleo, a Brazilian digital-native outlet, asserts that they will never publish AI-generated content without human review nor allow AI to serve as the final editor. Meanwhile, The Guardian emphasizes openness with readers, and CBC follows a “no surprises” rule, ensuring full disclosure whenever AI-generated content is used.
This lack of consistency in AI disclosure policies presents a major challenge for audience engagement. Ambiguous or inconsistent guidelines between outlets make it difficult for news organizations to communicate clearly about their AI use, which could lead to audience confusion and skepticism. This credibility gap underscores the need for a more coherent and standardized approach to AI disclosures in journalism.
Insights from the Audience Perspective: Perceptions & Concerns
Understanding how audiences perceive AI disclosures is crucial to evaluating their effectiveness. One of the central questions from the audience perspective on AI disclosures is:
How do audiences think about AI use in news and what are their information needs when it comes to disclosing AI use in news?
Group interviews with 
Dutch audience members have revealed a strong desire for transparency, particularly with an emphasis on what data AI relies on
, and how it generally influences the news they consume. Audiences expressed strong concerns about privacy and ethical AI use, wanting to be assured that news organizations are using AI responsibly.
Interestingly, the group interviews revealed a clear paradox:
 While readers desire absolute transparency, many also trust established news organizations to use AI responsibly without needing to know the specific details on a more granular level. For example, some interviewees argued that a more general statement on the use of AI on a trusted news organization’s website is sufficient, rather than having explanations on the level of every article. When asked about where this trust comes from, explanations reveal that it is often based on the reputation of the news organization rather than a deep understanding of the AI technologies at play.
The challenge for news organizations is to navigate this paradox by implementing disclosures in a more nuanced way, and by understanding better what audiences’ information needs are. For example, it might well be that for less impactful AI tasks, such as spell-checking or translation, detailed disclosures may not be necessary. On the contrary, more significant tasks like generating headlines or writing articles might require more explicit disclosures. By offering different levels of transparency based on the AI’s role in the news production process, news organizations can build trust while addressing readers’ concerns.
Hanna Barakat & Cambridge Diversity Fund / 
https://betterimagesofai.org
 / 
https://creativecommons.org/licenses/by/4.0/
Insights from User & Design Research
Encounters with AI-generated content shape not only what audiences consume, but how they interpret the broader relationship between humans and machines in journalism. One of the central questions that arises from user & design research is:
How to translate evolving transparency obligations into disclosures that are both legally compliant and user-friendly?
Drawing on expertise in human-computer interaction (HCI), communication, law, and design can help in designing disclosures that are meaningful, accessible, and context-sensitive. Last year, 
workshops on user and design conducted by Abdo El Ali and colleagues
 brought together professionals from diverse fields to explore the implications of AI disclosure mandates in journalism. A recurring theme was the tension between the need for informative disclosures and the risk of information overload, where users may be overwhelmed with too much detail. Concepts like personalization, user experience, and user cognitive load emerged as critical to understanding how both journalists and audiences engage with AI disclosures.
The findings from the workshops echo concerns in the literature, particularly the 
concept of contextual integrity in the paper proposed by Nissenbaum
 — underscoring that effective transparency must balance relevance with simplicity. Design research suggests that not all disclosures should be treated equally. While users want to know if AI played a role in content creation, they are less concerned with technical specifics than with understanding the nature of the exact human-AI collaboration.
Other 
researchers have highlighted
 that concise, layered disclosures can reduce cognitive load while preserving informative value. Human-computer interaction research points toward adaptive systems that tailor 
the depth and timing of disclosures to user preferences or behaviors.
 Likewise, visual and interactive techniques could help illustrate AI’s contribution to a news story, enabling audiences to grasp how journalistic judgment and machine input coexisted in the final product. These user-centric solutions of a layered form of disclosures could offer a promising path toward intelligent systems capable of providing transparency that informs without alienating users.
Insights from Communication Effect Studies regarding AI Disclosures
The disclosure of AI involvement in news articles has been shown to affect perceptions of credibility and trustworthiness. One of the central questions is therefore:
How do AI disclosures in news articles influence audience perceptions of credibility and trust, and under what conditions do these effects vary across different audiences?
One of the primary concerns with AI disclosures is whether audiences even notice them. Research indicates that disclosure labels need to be prominently displayed and carefully worded to ensure they are not overlooked or misinterpreted. The design and wording of these labels are crucial, as terms like ‘AI-generated’ are more easily understood than more ambiguous terms like ‘artificial’ or ‘manipulated,’ which could imply that the content is false or unreliable.
Experimental research
 also shows that, in many cases, AI disclosures lead to negative effects on the perceived credibility and trustworthiness of news. This effect is particularly pronounced when generic labels are used, as audiences may interpret these labels as 
signaling full automation, even when human oversight is involved
. Yet, these effects may not be universal. For instance, the exact impact of AI disclosures may vary depending on individual factors such as whether audiences perceive AI as particularly objective and transparent iii. Thus it is clear that news organizations face a delicate balance between maintaining journalistic transparency and protecting their commercial interests.
Hanna Barakat & Archival Images of AI + AIxDESIGN / 
https://betterimagesofai.org
 / 
https://creativecommons.org/licenses/by/4.0/
Ethical and Legal Perspectives on AI Disclosures
The ethical and legal challenges of AI disclosures are significant. Generative AI technologies have the potential to distort people’s information environments by producing biased, false, or misleading content. One of the central questions is therefore:
How can the law help citizens and society better understand the informational risks associated with AI-systems and what legal rights and remedies should citizens have to effectively combat these risks?
The European Union’s AI Act attempts to address these concerns by imposing transparency and disclosure obligations on AI providers and deployers. In recent work however, two scholars from the lab critique the EU’s overreliance on 
transparency as its core empowerment mechanism
. For one, the AI Act only obliges that citizens are made aware they interact with AI, nothing more. People are not informed about the risks associated with AI exposure. Second, the AI Act offers little actionable citizen rights that give people control over their informational environment. In fact, where AI is used by media organizations, there might not be any transparency obligation to begin with! While pictures and audio-visual media are placed at heightened scrutiny, 
no transparency duties in the AI Act apply for textual content over which editorial responsibility and control has been exercised.
 Finally, labels signaling AI-interaction do not act as truth-labels, they give little indication concerning the reliability of the content consumed: AI-generated news content can be factually correct.
While transparency is important, it does not necessarily guarantee that citizens are informed about the risks associated with AI-generated content. Furthermore, the AI Act does not require transparency for all types of content, particularly in cases where editorial responsibility has been exercised. The legal framework’s failure to address these nuances highlights the need for a more robust approach to AI disclosures that goes beyond mere labelling.
The ongoing ethical and legal debate around AI disclosures must focus on the broader questions of when and why disclosure matters. It is not enough to simply inform citizens that they are interacting with AI; they must also be empowered with the tools to understand and control the information they encounter. This could include the ability for citizens to filter, contest, and report biased or misleading AI-generated content. By providing citizens with more agency, lawmakers can help protect public trust in the media and democratic deliberation and participation.
Ways Forward: Towards Meaningful AI Disclosures in News
As work on AI disclosures continues to evolve, several knowledge gaps remain. There is limited understanding of the long-term impact of AI disclosures on audience trust and perception. Research so far has focused primarily on Western democracies, leaving open questions about the effectiveness of AI disclosures in non-Western media systems and different legal contexts. Additionally, the effectiveness of different transparency strategies, whether through content labels, interactive explanations, or editorial statements, remains unclear. Longitudinal studies are needed to assess how AI disclosures affect audiences over time. Moreover, there are potential unintended side effects of AI disclosures that have not yet been fully explored. For example, how might disclosures influence people’s behaviour or decision-making in the long term? These questions will require ongoing research and reflection as AI continues to play a larger role in journalism.
In conclusion, the question of how to disclose AI in journalism remains complex and multifaceted. It requires balancing transparency, usability, and the practical needs of news organizations. By adopting a more integrated approach that considers user experience, legal and ethical considerations, and audience preferences, news organizations can create more effective AI disclosure practices. As AI continues to shape the future of journalism, it is crucial that transparency not only informs audiences but also builds trust in the media’s ability to use AI responsibly and ethically.
More information on the current projects of the AI, Media, and Democracy lab can be found 
here
.]]></content:encoded>
</item><item>
<title><![CDATA[A Feature, Not a Bug: What Newsrooms Need to Know About the Uncertainty of LLM Responses]]></title>
<link>https://generative-ai-newsroom.com/a-feature-not-a-bug-what-newsrooms-need-to-know-about-the-uncertainty-of-llm-responses-a794bc75d787?source=collection_home_page----df04cefca135-----7-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News]]></category><category><![CDATA[Uncertainty]]></category><category><![CDATA[Prompt Engineering]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Sachita Nishal]]></dc:creator>
<pubDate>2025-04-02</pubDate>
<atom:updated>2025-04-02</atom:updated>
<content:encoded><![CDATA[A Feature, Not a Bug: What Newsrooms Need to Know About the Uncertainty of LLM Responses
Prompting LLMs to express uncertainty can help strategize the process of verifying their outputs
Sachita Nishal
Follow
5 min read
·
Apr 2, 2025
--
Listen
Share
A key appeal of generative AI tools in the newsroom is their potential for streamlining time-intensive yet mundane tasks. LLMs can be useful for everything from 
finding claims that are worth fact-checking
 and 
extracting metadata from unstructured data for analysis
 to 
classifying past coverage to inform future reporting
. But they can still generate inaccurate or false responses, and so require 
extra verification work
 from an attentive person. What if that verification work could be eased by LLMs that signal their own reliability in a response though? How could we then design LLM-based tools to communicate uncertainty, so that journalists could easily judge when or how an LLM’s response needed extra vetting?
A recent study from researchers Autumn Toney-Wails and Lisa Singh at Georgetown University 
investigated how leading AI models express uncertainty
, in the context of fact-checking different types of claims, and whether these expressions can clue users to the actual accuracy of the LLMs’ responses themselves. Their findings reveal that explicitly asking LLM systems to say when they’re unsure about their outputs can significantly improve accuracy. However, they also discovered a concerning pattern: when LLMs rate their own confidence on a scale (like 7/10), these self-ratings rarely align with the actual accuracy of their outputs. Different AI models also show different tendencies toward admitting uncertainty, with Claude 3 Haiku’s outputs readily communicating doubt (“I’m not sure but …”) while GPT-4 tends to project confidence regardless of its accuracy.
While this study focused specifically on fact-checking as a use-case, its findings about uncertainty expressions could extend to other types of labels, summaries, or answers that LLMs are used to generate in the newsroom. Future experiments might explore how LLMs express uncertainty across these different types of outputs. What makes this study especially relevant is its focus on natural language expressions of uncertainty from LLMs — phrases like “I’m not entirely certain” or “I don’t have sufficient information” — rather than technical metrics that aren’t as easily interpretable, such as probability scores for generated tokens.
In the rest of this blogpost I expand on the setup and findings of this study in detail and then use that to suggest implications for how journalists might more effectively design, prompt, and interpret AI systems in their workflows.
Different models, prompts, and domains
The researchers tested two leading AI models — GPT-4 and Claude 3 Haiku — for their ability to fact-check a diverse set of approximately 2,500 claims spanning three categories: scientific facts, common knowledge, and misinformation. To evaluate how these models express uncertainty, the researchers employed several prompting strategies. They used basic prompts like “Is it true that…?” and “True or False: [claim].”, and then modified these with explicit instructions for expressing uncertainty: “If you are unsure, please answer with unsure.” In some variations, they also requested confidence ratings on a scale of 1 to 10 from the LLM. They ultimately tested different combinations of these approaches as well.
While framed around fact-checking, the study’s takeaways for journalists aren’t about whether LLMs can label novel claims as true or false. As the findings below will show, current systems aren’t reliable enough for this task. Instead, the study’s significance lies in revealing how LLMs express uncertainty across different knowledge domains, and how these expressions correlate with actual accuracy. For newsrooms implementing generative AI in various workflows, understanding these uncertainty patterns may offer a practical advantage: knowing when system outputs may be less reliable can help orient limited human attention to those bits most in need of verification.
LLM uncertainty improves accuracy, but is not the default
Based on the different prompt configurations they tested, the authors found different patterns in (1) the accuracy of LLM responses, (2) the tendency of LLMs to express uncertainty, (3) the relationship between expressed uncertainty and response accuracy.
First, all models struggled most with fact-checking misinformation claims, achieving at best 81% accuracy (GPT-4, when prompted to express uncertainty) and as low as 48% (Claude Haiku, without prompting for uncertainty). This high error rate shows why LLMs alone shouldn’t be the final arbiter in fact-checking workflows. 
Without specific prompting, both models also rarely expressed uncertainty in their responses
. Claude Haiku was more likely to do so than GPT-4 in the context of labelling misinformation claims (36% vs. 8% uncertainty expression rate), showing that this model is a bit more sensitive to the domain, and adapts its responses accordingly.
Both LLMs also responded very differently to explicit prompts for expressing uncertainty. Claude 3 Haiku frequently adhered to this prompting — doing so in half or more of its fact-checks for common knowledge and misinformation claims. GPT-4, by contrast, rarely expressed uncertainty except when evaluating misinformation (19–23% of cases), 
tending to provide definitive answers even when incorrect
. Interestingly, the highest accuracy for both models came when they were explicitly prompted to both express uncertainty in natural language and provide a confidence rating.
Figure from Toney-Wails and Singh (2024), demonstrating the increased accuracy from prompting models to express uncertainty in their responses.
The resulting disconnect — between how rarely models expressed uncertainty without prompting, and the accuracy when uncertainty was expressed — suggests that developers and end-users need deliberate strategies to elicit useful uncertainty signals. “Useful” is the key word here, because the actual value of the confidence ratings provided by these models were poor indicators of accuracy. GPT-4 assigned the highest possible confidence score (10/10) to 87% of all its responses, including many that were incorrect. This suggests that when an LLM confidently states “I’m certain that…” or provides a high numerical confidence score, this signal alone shouldn’t be trusted.
Implications for news practitioners and researchers
For developers working in newsrooms, this study offers clear guidance: explicitly prompting LLMs to express uncertainty can improve response accuracy. Model selection should be strategic as well. Claude offers uncertain responses for a wider range of domains and prompt formulations, and so may be better suited for certain tasks. GPT-4 might suffice for less difficult domains like common knowledge questions,such as 
labeling well-known entities in unstructured data
. The research does raise concerns about confidence scores, which tend to skew high regardless of accuracy. Instead of relying on these potentially misleading numbers, it could be more practical to trigger alerts based on natural language uncertainty expressions in LLM responses.
For journalists and editors, recognizing the patterns of AI certainty and uncertainty would be useful, even in the case of ad-hoc usage. When LLMs express doubt — particularly models like GPT-4 that rarely seem to do so in this study — this should immediately signal the need for additional verification.
For researchers in this field, while this study provides a foundation for understanding how uncertainty is expressed in LLM outputs, the follow-up question of how journalists might actually interpret and respond to these uncertainty signals remains unexplored. How would 
you
 use uncertainty signals in your journalistic work?]]></content:encoded>
</item><item>
<title><![CDATA[AI Overviews, Chatbots, and News Traffic: The Story So Far]]></title>
<link>https://generative-ai-newsroom.com/ai-overviews-chatbots-and-news-traffic-the-story-so-far-c010b3bf53cb?source=collection_home_page----df04cefca135-----8-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Web Analytics]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Google Ai Overview]]></category><category><![CDATA[Audience]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2025-03-26</pubDate>
<atom:updated>2025-03-26</atom:updated>
<content:encoded><![CDATA[AI Overviews, Chatbots, and News Traffic: The Story So Far
Analyzing Referral Traffic in the Early Days of AI-Powered Search
Nick Hagar
Follow
13 min read
·
Mar 26, 2025
--
Listen
Share
By 
Nick Hagar
 and 
Nick Diakopoulos
As Google bakes AI more deeply into search, will publishers suffer?
In early March, Google rolled out an early preview of “AI Mode,” a product that changes how search works. 
AI Mode
 navigates the web independently, synthesizing answers with embedded citations to sources users never directly visit.
This experiment is a significant step toward Google’s vision of
 doing the searching for you
. It also raises important questions about publishers’ future: if Google handles the searching, will users still visit news sites?
AI Mode is just one example of the challenges news publishers face with generative AI and web traffic. 
Several
 
GenAI
 
firms
, most prominently 
OpenAI
, have built their own search products, in an effort to become destinations for timely information. And while many worry these tools will redirect traffic away from news sites, there’s been little concrete data to understand what’s actually happening.
This piece is Part 2 of our series analyzing Comscore traffic data to assess the real-world impact of generative AI on news publishers. In 
Part 1
, we examined where ChatGPT and Perplexity send web traffic. Now, we zoom in on referrals to 10 prominent news sites, asking: Are Google’s AI Overviews significantly changing search traffic, and do publisher strategies like blocking OpenAI’s crawler or forming partnerships actually affect referral patterns? We analyze data surrounding key AI Overviews rollout events and compare outcomes for publishers with different OpenAI policies.
Referral Traffic Upheavals
This analysis takes place against a backdrop of shifting referral traffic for publishers. As many outlets have noted, especially smaller outlets, Google traffic was 
far from stable
 even before AI Overviews — the 
September 2023 “helpful content” update
 started a downward trend for many. Social referrals have also proven unreliable in recent years, with X 
throttling access to some news sites
 and Facebook 
deprioritizing news
.
In the midst of these upheavals, generative AI is something of a wildcard. Top-level data around GenAI search tools are discouraging, as industry research shows
 severely curtailed clickthrough rates on AI Overviews
 and
 largely incorrect citations for news articles
. At the same time, as evidenced in 
Part 1 of our analysis
 and in other 
analyses
 and 
reporting
, AI tools haven’t moved the needle on referral traffic. This would seem to indicate that audiences 
aren’t
 shifting their news reading habits
 over to generative AI chatbots (yet, at least), even in the case of AI Overviews, where newsroom directors have reported 
little concern
 around potential traffic impacts.
The contrast between reported low clickthrough rates on AI Overviews and the observed stability in overall search traffic to news media, coupled with the largely unknown effects of direct AI interactions like those with ChatGPT, demands a more rigorous investigation. One plausible explanation is that Google’s initial rollout of AI Overviews may have been followed by a 
reduction in their appearance on news-related queries
, mitigating potential traffic impacts. To better understand these dynamics, we analyzed referral data across major news publishers. This analysis examines how Google’s AI Overviews expansions impact traffic, and how publisher decisions correlate with referral traffic from ChatGPT.
Our Analysis Approach
We analyzed inbound traffic to 10 major news organizations: The Associated Press, The Atlantic, CNN, Fox News, The New York Times, Politico, Time, USA Today, The Wall Street Journal, and The Washington Post. We selected these outlets to represent a diverse cross-section of the news ecosystem, including both legacy publishers and digital-first operations with varying business models and audience sizes.
We sourced referral data from Comscore over a 5-month period from July to November 2024. For this timeframe, Comscore’s panel contains approximately 364,000 active U.S. desktop users. We filtered out programmatic traffic to focus exclusively on visits indicating a person actually viewed the referred site. This left us with a sample of 152,700 visits to these 10 news websites from 14,703 unique users.
With this dataset, we investigated three key questions about generative AI’s impact on news traffic:
How did Google’s AI Overviews rollout affect search traffic to news sites?
 We examined two major expansion events in Google’s rollout timeline.
Do different robots.txt policies affect referral traffic from ChatGPT?
 We compared traffic patterns between publishers that block OpenAI’s crawler and those that don’t.
Do formal partnerships with OpenAI translate to more referral traffic from ChatGPT?
 We analyzed whether having a content licensing deal with OpenAI affected traffic patterns.
For our Google analysis, we focused on two significant milestones in AI Overviews’ evolution: First, the 
August 15, 2024 expansion
 to six additional countries, which also introduced a crucial design change — displaying links more prominently within the AI Overviews panel. Second, the 
October 28, 2024 worldwide rollout
 across more than 100 countries that also expanded language support within the U.S., making AI Overviews accessible to non-English speakers domestically. While our dataset captures only U.S. users, both launches represented substantial changes in how AI Overviews functioned and who could access them. The enhanced link visibility from the first update and the expanded language support from the second could alter user interaction patterns that impact referrals.
To isolate the impact of these rollouts from normal traffic fluctuations and major news events (such as Election Day and President Biden’s withdrawal from the presidential race), we employed a 
difference-in-differences regression approach
 to model the share of web traffic coming from Google search (see Appendix A for methodological details). This allowed us to compare how Google traffic changed over time relative to non-Google referrals, helping identify effects specific to AI Overviews rather than general news consumption trends.
For our ChatGPT analyses, we first identified which publishers had formal partnerships with OpenAI (via Pete Brown’s 
AI partnership tracker
) and which blocked 
OpenAI’s ChatGPT-User crawler 
in their robots.txt files (by using the historical archive of robots.txt policies 
maintained by Ben Welsh
). We chose to focus on the ChatGPT-User crawler since, of the 3 crawlers that OpenAI maintains, it is the one directly linked to user behavior as per the OpenAI documentation: “When users ask ChatGPT or a CustomGPT a question, it may visit a web page with a ChatGPT-User agent”. Five publishers in our sample have OpenAI partnerships, while four block the crawler. We conducted Mann-Whitney U tests on traffic to publishers in each condition for November 2024, after ChatGPT Search had launched.
60% of our sample allows OpenAI’s ChatGPT-User crawler, and half have a partnership of some kind with the company.
What We Found
Google’s AI Overviews Show No Measurable Impact on News Traffic
After isolating the impact of the two AI Overviews rollouts on the share of referrals coming from Google, we found no statistically significant effect. While our regression model indicated slight positive coefficients for both launches, neither reached statistical significance (p=0.10 for the August expansion and p=0.07 for the October worldwide rollout). Even with major news events, such as the Presidential election, that drove large amounts of search traffic, the share of referrals coming from Google only moved by a few percentage points — 38% before the August rollout, 40% after that August rollout, and 43% after the worldwide rollout.
This finding suggests that despite the increased link salience and accessibility of AI Overviews, they neither boosted nor cannibalized Google’s referral traffic to news sites relative to other sources. For publishers concerned about AI summaries redirecting users away from their content, this represents cautiously good news — at least for now.
An important consideration in interpreting these results is the extent to which AI Overviews appear on news-related queries in the first place. For example, in mid-2024 
Ziff Davis reported
 that only 8% of queries driving their core traffic triggered AI Overviews. If this proportion is consistent across news publishers, then a vast majority of search queries related to news content may not include an AI Overview at all.
Our findings, therefore, may reflect Google’s cautious approach to summarizing content for news-related queries with AI rather than indicating how users behave when they actually encounter AI Overviews. This selective deployment could explain why we’re not seeing dramatic shifts in traffic patterns despite the feature’s expanding presence. This could of course shift in the future, as Google “aim[s] to show an AI-powered response as much as possible” in its new 
AI Mode
 product.
Blocking OpenAI’s Web Crawler: A Futile Defense Strategy?
For the month of November 2024 (after ChatGPT Search launched), we found that publishers’ policies toward OpenAI’s ChatGPT-User crawler had virtually no effect on the amount of non-404 referral traffic they received from chatgpt.com (i.e. traffic to valid content-bearing URLs on the site). Our Mann-Whitney U test comparing outlets that block the crawler versus those that don’t revealed no significant difference (U=13.5, p=0.83).
The median outlet that blocked OpenAI’s ChatGPT-User crawler in its robots.txt files received 7 referrals, while those that did not received 5 (with averages of 8 referrals for blocking sites and 12 for non-blocking sites). The traffic to blocked sites largely goes to the New York Times and CNN:
Despite blocking OpenAI’s ChatGPT-User crawler, the New York Times receives the second-most referrals from ChatGPT in our sample.
This suggests that robots.txt policies targeting the ChatGPT-User crawler may have little impact on how AI search tools like ChatGPT direct traffic. These systems may either disregard crawler restrictions or reference blocked publisher content through alternative means, such as training data or search indices built by crawlers that publishers haven’t blocked. Microsoft’s Bingbot is a likely candidate, given OpenAI’s partnership with Microsoft. OpenAI also operates its own crawler, OAI-SearchBot, focused on content indexing. However, when we similarly analyzed this bot, we found similar results.
OpenAI Partnerships Fail to Deliver Traffic Advantages
Similarly, for November 2024, we found that having a formal partnership with OpenAI had virtually no effect on the amount of non-404 referral traffic news outlets received from chatgpt.com. Our Mann-Whitney U test again found no significant difference (U=15.5, p=0.60), with traffic being remarkably similar — 6 median referrals for partners, and 4 for non-partners (14 mean referrals for partners, and 6 for non-partners). Even roughly extrapolating the Comscore sample out to the US internet population would still only create a gap of a few thousand referrals per month, a small difference.
This finding reinforces what we observed in 
Part 1
 of our analysis, where we found a similar lack of traffic differentiation for a different set of outlets. The consistency of this pattern across multiple analyses suggests that content licensing deals with OpenAI are not currently translating into meaningful referral traffic benefits.
This raises important questions for publishers considering such partnerships. While these agreements may provide other benefits — such as compensation for content use or influence over how content is represented — they do not appear to create significant new traffic pathways back to news sites. It suggests that user behavior with ChatGPT Search may be different from traditional search engines, with users potentially viewing the AI’s summary as sufficient rather than clicking through to original sources, regardless of whether those sources have formal relationships with OpenAI.
ChatGPT’s Citation Problem: High Rate of Links to Nonexistent Pages
Expanding on a concerning pattern we identified in our 
first analysis
, we found that ChatGPT stands as an extreme outlier in the proportion of its referrals that result in 404 errors (i.e. page not found). We examined all referrers in our dataset with at least 100 visits to news sites and referrals to more than one publisher. Of those, almost all return a very small proportion of 404s. ChatGPT, however, sends users to nonexistent pages 30% of the time when considering our whole 5-month timeframe. This rate did drop after the launch of ChatGPT Search — 404 errors went from 58% of ChatGPT referrals before this point, down to 11% after. However, that rate is still higher than all other referrers meeting our criteria in November 2024:
Only 7 of the analyzed referrers sent any 404s to our sample of news publishers in November 2024. Of those, ChatGPT is an outlier.
This high error rate provides evidence of hallucinations in ChatGPT’s responses, an issue that 
others have also noted
. When the model cites news sources, it may create URLs that look plausible but don’t actually exist. Most troubling is the pattern of which outlets receive these erroneous links — all but one of the 404s in our sample point to The New York Times, with the remaining one directing to The Washington Post. These links almost all follow the pattern of a New York Times article page (e.g., 
https://www.nytimes.com/2021/05/03/technology/privacy-risks-digital-surveillance.html
, 
https://www.nytimes.com/2023/06/30/us/politics/supreme-court-epa-limits.html
). However, these pages do not exist, and searches for them on Google and the Internet Archive’s Wayback Machine return no results.
This concentration of errors among some of the most authoritative news sources suggests ChatGPT may be overconfident in attributing information to prestigious outlets, perhaps reflecting biases in its training data or its understanding of which sources users would find credible.
Limitations
This analysis has several important limitations. First, these data only capture desktop users. Mobile traffic to news websites from Google may follow different patterns. Second, our data do not capture the period before Google’s 
initial rollout of AI Overviews
 in May 2024. While the subsequent expansions identified in our sample represent meaningful shifts in AI Overviews’ prevalence and functionality, it is also possible that only the initial rollout caused a major disruption in search traffic. Finally, our data capture a particularly active news cycle in the lead up to the 2024 U.S. Presidential election. While we control for major news events in our regression analysis of search traffic, and there is no one “normal” news cycle, this period may be anomalous in the level of news reading activity we see from users.
Generative AI Hasn’t Disrupted News Referrals (yet)
Our analysis yields several key findings about generative AI’s impact on news traffic so far.
First, we found no observable impact on news traffic from shifts in Google’s AI Overviews feature introductions in our time frame, even after controlling for major news event spikes. There are at least a couple potential interpretations of this result. As Google 
claims
, users may indeed be clicking on links within AI Overviews, preserving referral traffic despite the new interface. However, another explanation is that Google is being cautious in its rollout of AI Overviews for news-related queries. Given high-profile AI hallucinations and inaccuracies — 
like those seen with Apple Intelligence
 and the AI-generated 
LA Times opinion counterpoints
 — Google could be deliberately limiting AI summaries on news results to avoid similar pitfalls.
This second interpretation suggests an important caveat: the lack of disruption doesn’t mean news publishers are immune to changing search modalities. Recent data from 
Tollbit
 suggests that general-purpose AI chatbots like ChatGPT drive traffic at a substantially lower rate than traditional Google search. As 
academic publishers have now noticed
, when AI tools are used, they can fundamentally alter how users interact with search results. Together with our findings, this suggests that AI summaries may not yet be widely deployed for news content, creating a temporary period before these lower referral rates may begin to affect publishers more broadly as Google expands.
Our second major finding reinforces what we observed in our previous analysis — OpenAI traffic patterns show little consistency beyond remaining consistently low. Whether a news outlet blocks OpenAI’s crawler in their robots.txt file or not makes little difference to referral traffic. This aligns with another point from Tollbit’s 
analysis
, highlighting the growth in AI crawlers that bypass robots.txt. We also find that having an official partnership with OpenAI doesn’t drive meaningful traffic advantages. This suggests that the current integration of news content into ChatGPT Search is not creating referral pathways that drive significant traffic back to publishers, regardless of their relationship with OpenAI.
Finally, we continue to see evidence of hallucinations in AI tools pointing to prominent news organizations. ChatGPT sends a disproportionate number of users to non-existent pages, with 30% of its referrals resulting in 404 errors — dramatically higher than almost any other referrer. Most strikingly, these errors predominantly affect a single outlet: The New York Times. This pattern raises questions about how these AI systems represent and cite content from high-authority news sources, potentially creating a 
misleading impression of attribution
 while linking to content that doesn’t exist.
Taken together, these findings present a nuanced picture: generative AI tools have yet to significantly disrupt news referral patterns, but the underlying data shows concerning signals. The considerable error rate in ChatGPT’s citations, combined with Tollbit’s finding of substantially lower clickthrough rates from AI interfaces, suggests that when AI mediates content discovery, both accuracy and publisher traffic suffer substantially. Google’s seeming caution with news content provides temporary shelter, but as AI search capabilities advance, publishers face an inevitable reckoning. News organizations must use this grace period strategically — not merely to diversify traffic sources, but to fundamentally rethink their position in an information ecosystem where AI increasingly stands between them and their audience.
Appendix A: Difference-in-differences implementation
To isolate the impact of Google’s AI Overviews on news website traffic, we used a difference-in-differences (DiD) regression analysis. This approach compares changes in Google referral traffic (the “treatment” group) to changes in traffic from all other sources (the “control” group) before and after two key AI Overviews rollout dates: August 15, 2024 (a limited expansion), and October 28, 2024 (the worldwide rollout). We divided our data into three time periods: before the expansion of AI Overviews beyond the U.S., after that first expansion, and after the worldwide rollout. Our core dataset, sourced from Comscore, included daily referral traffic counts for each of the 10 news outlets, separated by traffic source (Google vs. other).
The key variable in our analysis was an interaction term that combined whether the traffic came from Google and which time period it was. This interaction allowed us to estimate the difference in traffic changes between Google and other sources specifically due to the AI Overviews expansions. A positive coefficient on this interaction term would suggest AI Overviews increased Google’s share of traffic, while a negative coefficient would suggest a decrease.
Importantly, we controlled for several other factors that could influence traffic patterns. We included controls for the day of the week and the specific news outlet, accounting for typical daily variations and inherent differences between sites. We also accounted for major news events that could affect all web traffic around that time — namely, a period around election day, a period when Joe Biden dropped out of the Presidential race, and a period at the beginning of the Democratic National Convention. We included these as both general controls and as interactions with Google traffic, to allow for the possibility that these events might impact Google traffic differently than other traffic. Finally, because we were interested in Google’s 
share
 of traffic, we accounted for the total traffic to each site on each day. We used a statistical model appropriate for count data (a Negative Binomial regression) and adjusted our calculations to account for the fact that traffic to the same news outlet over time is likely to be related.]]></content:encoded>
</item><item>
<title><![CDATA[Why Story Discovery is the Killer App for Generative AI in Journalism]]></title>
<link>https://generative-ai-newsroom.com/why-story-discovery-is-the-killer-app-for-generative-ai-in-journalism-ff49ba2ddef0?source=collection_home_page----df04cefca135-----9-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Story Discovery]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Clare Spencer]]></dc:creator>
<pubDate>2025-03-20</pubDate>
<atom:updated>2025-03-20</atom:updated>
<content:encoded><![CDATA[Why Story Discovery is the Killer App for Generative AI in Journalism
An argument for why story discovery, rather than story telling is a better use of GenAI’s capabilities and reflection of its limitations
Clare Spencer
Follow
7 min read
·
Mar 20, 2025
--
2
Listen
Share
The Generative AI in the Newsroom’s aim is to collaboratively figure out when — and when not — to use generative AI in news production. As a writer for the site, and having worked on applying generative AI to a variety of different newsroom tasks over the last three years, I am becoming increasingly more comfortable with recommending one use beyond any other: 
using generative AI to surface story ideas
.
This is where the exciting — and most journalistically valuable — stuff is happening. Take my coursemate at the 
AI Journalism Lab at Newmark J-Schoo
l, Rune Ytrberg, who impressed me with his use of large language models (LLMs) to 
uncover a health scandal in his local hospital
. He asked LLMs to find the most severe cases of neglect from more than 1000 pages of Norwegian hospital documents. This unearthed the case of a radiologist who had been giving suspected cancer patients’ X-Rays a fleeting glance, to devastating consequences.
Let me elaborate my argument for why newsrooms should prioritise tasks to help with finding stories when using generative AI. The crux of it boils down to this core limitation of the technology: Generative AI often makes errors, or “hallucinations” as the technologists like to say. This affects 
everything
 for the news industry where a fundamental goal is striving for accuracy. We have already seen factual errors creep into the news, thanks to misjudged uses of generative AI that don’t appear to prioritize accuracy at all. 
I wrote previously
 on this site about Apple withdrawing its Apple Intelligence news summaries after multiple inaccuracies. For instance, Apple Intelligence wrongly summarized that the man charged with the murder of UnitedHealthcare CEO Brian Thompson, Luigi Mangione, had shot himself outside court when the BBC had reported he Mr Mangione shouted “completely unjust” while on his way to court.
Apple Intelligence’s inaccurate news summaries have earned their place on the generative AI wall of shame
Journalist Karen Hao’s simple framework has helped me work out a safe use of generative AI. She has two questions:
Does the task that you’re trying to perform with AI need high accuracy, or does it not need low accuracy?
Will your output be internally facing for your own research, or will it be audience-facing as a final product?
“If it’s low accuracy, and it’s going to be internally facing research, then it’s much safer to adopt Gen AI” she says 
in a Reuters Institute article
.
Using generative AI to get tips for potential stories fits into Karen’s framework for a safe use of generative AI since it is an internally facing activity and doesn’t absolutely require high accuracy since someone will check the lead before advancing it. I split the news-making process broadly into two main blocks: story finding and storytelling. Story finding is the furthest away, in the production process, from the audience with the most opportunities for journalists to catch any mistakes before an audience member sees.
If you use generative -AI for storytelling, such as reformatting, summarizing, or wording for a younger age range then, as a responsible news organization, you will then have to check that copy for any factual errors the model may have introduced. I know what this fact checking looks like up close. I used to manage a team of 15 reporters at the UK publisher Newsquest. Their job was to check the AI-generated first draft of a news story. Some called our team the humans in the loop. The task of spotting the factual errors AI introduces is subtly different to a traditional subeditor. In very non-technical terms, it was as if a little AI pixie would dance around the copy sprinkling falsehoods such as mixing up names, making up dates and, now and again, changing a fundamental fact in the story.
An illustration of a hypothetical AI pixie adding in errors to content.
It was the AI-assisted reporters’ job to catch the pixie’s mischief by reading the original input, then the AI generated copy and then weed out the differences. They completed this task multiple times a day and largely succeeded in producing stories quicker than other reporters.
Another experience, however, leads me to argue that newsrooms should instead focus on applying generative AI to transformative tasks rather than efficiencies. By that I mean, focus on tasks you didn’t or couldn’t do before rather than on finding ways to make the current tasks you do quicker and/or cheaper. That’s because efficiency can be an illusion when you don’t have access to details on all the costs. As a product manager for a small audio production company, Overcoat Media, I have just finished making a prototype for a generative AI tool called PodMorph which reformats podcast content for various platforms.
We integrated multiple generative AI tools into PodMorph to automatically convert long-form audio into a vertical video for TikTok, square picture for the Instagram grid, and other formats.
One of the things I loved about this project was that I had rare access to a fuller picture of the costs. I was struck that it 
would have been entirely possible to spend more using generative AI than the savings you might make on labor
. There are a lot of variables and increasingly more ways of doing things. But some costs for us included the obvious generative AI tools as well as previously hidden costs of cloud databases and server costs. We also spent money on the user research, the user testing (both of which used the content makers’ time) and we spent money on a software engineer. Also, I was aware that we took up the time of some of perhaps the most expensive people at Overcoat Media — the directors — because bringing in generative AI forces you to ask a profound question about a company’s aims. I also found that when I analysed one particular manual task we were part-automating (generating a first draft of a text programme description) the difference between the cost of the maintenance of our tool and the cost of the time we were saving were almost identical. For all these reasons I believe it is better to aim to use generative AI to provide your audience with something you previously couldn’t, like uncovering more original and impactful stories.
In sum, using generative AI to tell stories (1) risks lower quality output, (2) can increase the proportion of repetitive work needed for checking outputs before publication, and (3) may not even be cheaper than your previous way of working. Instead, I argue, newsrooms should consider the opportunities in using generative AI for 
finding
 stories. That’s what my coursemate Rune did to uncover a scandal putting lives at risk.
Rune is the head of the data journalism lab at the northern Norwegian newspaper iTromsø. Through the second half of 2024, his team published a series of articles which revealed a radiologist had rushed his job checking for tumours on X-rays. Rune says he wouldn’t have found this story if he hadn’t used generative AI.
Rune’s team didn’t know of the existence of this story when they started looking into shortages at the University Hospital in Tromsø. They had first started looking into whether understaffing at Tromsø’s hospital was costing lives after 
Renate and Anders lost their son
, Erik, in childbirth in July 2023. A year after Erik’s death, 
iTromsø reported
 that the state administrator concluded that the hospital had violated the requirement of a duty to provide proper health services. On reading this story, another patient
 contacted iTromsø and told them
 that he had been sent home with unbearable back pain after analysis of an X-ray didn’t detect a cyst which was then found a year later. This led Rune to do a wider search on hospital documents. Using Norway’s freedom of information law, he requested documents regarding the X-ray department sent to the state administrator and the health inspectorate. He received almost 1000 pages. He says this was too big to search in the traditional ways. This is where he started experimenting with generative AI.
By using LLMs from both OpenAI and Anthropic plugged into 
Anything LLM
 (an interface which allows users to, among other things, upload documents and get generative AI to search within them) he was able to interrogate the documents without knowing exactly what he was looking for. The breakthrough was when he asked for the five most severe cases from all the documents. This brought up documents regarding a substitute doctor and he requested more documents regarding a substitute doctor.
From this he discovered that a temporary doctor had been allowed to work from home on his own computer with little control from the hospital. With coordination from other newspapers, they worked out that the doctor had actually been working across Norway, Denmark and Sweden, which don’t coordinate work schedules so could have doubled up on shifts. In one instance, a document revealed that he had spent no more than 11 seconds looking at an X-ray. They found that five patients died under his watch.
Rune, sixth from the right, and his team at iTromsø won the Data-skup award in 2024 for their use of generative AI in investigations.
CREDIT: Magnus Eriksen
Rune and his team went on to be recognised for their work by 
winning the Data-SKUP award 
(an award for data journalism given by the SKUP investigative journalism association in Norway) in October 2024 and were commended by the jury for making it “possible to absorb large amounts of unstructured information and give journalists the opportunity to find narratives and stories”. In other words, they had applied generative AI to find a needle in a haystack.
Rune’s metaphorical haystack was documents he obtained through freedom of information requests. Journalists are starting to experiment with using generative AI to surface stories from other heaps of information too. These range from US education publication
 Chalkbeat monitoring public meetings
 to Filipino journalist 
Jaemark Tordecilla interrogating
 his government budget. What other stories are waiting to be found in various haystacks of documents?]]></content:encoded>
</item><item>
<title><![CDATA[EarthCheckr: Leveraging Generative AI for Environmental Journalism Fact-Checking]]></title>
<link>https://generative-ai-newsroom.com/earthcheckr-leveraging-generative-ai-for-environmental-journalism-fact-checking-00a7f82fb0a3?source=collection_home_page----df04cefca135-----10-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[News]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Environment]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Sam Schramski]]></dc:creator>
<pubDate>2025-03-12</pubDate>
<atom:updated>2025-03-12</atom:updated>
<content:encoded><![CDATA[EarthCheckr: Leveraging Generative AI for Environmental Journalism Fact-Checking
How the Earth Journalism Network developed a tool to support the fact-checking process
Sam Schramski
Follow
7 min read
·
Mar 12, 2025
--
Listen
Share
Environmental journalism operates at the nexus of scientific inquiry, political discourse, and public engagement. The verification of factual claims in this field is both essential and labor-intensive, often requiring reporters to cross-reference scientific literature, policy documents, and firsthand testimonies. EarthCheckr, a GenAI-enhanced fact-checking tool developed by the 
Earth Journalism Network
 (EJN), offers a novel approach to assisting editors in the validation of environmental reporting.
Earth Journalism Network (EJN), established in 2004, is a global network of 25,000+ members dedicated to strengthening environmental and climate change reporting by providing resources and support to journalists worldwide, with an emphasis on the Global South. Through small grants, training opportunities and mentorship, EJN aims to improve the capacity of local journalists and independent newsrooms to inform communities and policymakers, hold power accountable, and drive action on critical environmental issues, even as they strive to counter dis- and misinformation and make the most of limited resources.
Dashboard interface for EarthCheckr.
The Role of Generative AI in Fact-Checking
EarthCheckr leverages a large language model (LLM) to facilitate the identification and verification of factual claims in journalistic drafts. It runs on the Open AI API using the gpt-4o model. Unlike conventional AI writing assistants, EarthCheckr does not generate content or suggest sources. Instead, it systematically extracts “verifiable claims” — statements that require attribution or further scrutiny — highlighting areas that necessitate verification. This is a form of 
ante hoc 
(or “beforehand”) fact-checking core to the iterative process of research, writing, follow-up interviews, rewriting, and editing that most journalists are familiar with before content is available to the public. Most non-journalists, however, are more familiar with 
post hoc
 (“afterwards”) fact-checking, perhaps made most famous by PolitiFact and crowdsourced stand-ins, such as “Community Notes” on social media.
At EJN, rather than serving as a research proxy, we view EarthCheckr as an analytical tool that enhances or supplements editorial oversight.
Upon submission of a journalist’s draft, and with their consent, a small cadre of EJN editors and journalism mentors run the draft through EarthCheckr. The tool isolates key factual statements, and compiles them into a structured document (see figure with example output above). This document is then reviewed and annotated by the journalist, who cross-checks each assertion against primary sources such as peer-reviewed studies, official reports, and interview transcripts. The verified claims are subsequently archived in EarthCheckr’s growing database — with an eye toward an eventual repository of rigorously vetted environmental information.
Addressing the Challenge
The proliferation of generative AI tools in journalism has intensified concerns regarding misinformation. While some GenAI applications inadvertently propagate inaccuracies, EarthCheckr adopts a fundamentally different paradigm — prioritizing editorial validation over autonomous content generation. By integrating AI-assisted claim identification with human oversight, the tool strengthens journalistic accuracy rather than replacing critical editorial judgment. Basically, it just makes the fact-checker’s job a bit easier.
Environmental reporting is particularly vulnerable to misinformation, whether through misrepresentation of scientific data, climate change denialism, or industrial lobbying efforts. EarthCheckr provides journalists with a systematic mechanism for helping to distinguish credible information from misleading narratives, reinforcing the integrity of their work.
The initial impetus for this project was a draft of a story we received from a grantee that was laden with inaccurate information. The journalist, a freelancer, was told by his outlet’s editor that they would be unable to provide fact-checking. EJN provides scores of opportunities and does secondary editing of much of our grantees’ work at any given moment, so we must depend on originating newsrooms to furnish our editors with fact-checked stories — we simply do not have the capacity to fact-check all the journalists and media houses we support. In the case of the wayward reporter, we were able to provide a human fact-check; however, it became clear to us that for small newsrooms and freelancers throughout the Global South, fact-checking is considered a massive luxury.
So, we turned to generative AI to try to help lower the bar and support journalists in factchecking their work.
How EarthCheckr Works: A Structured Approach
Editors and mentors utilize EarthCheckr through a well-defined process:
Gaining access
 — EJN editors and mentors obtain unique login credentials from EJN.
Ensuring journalist consent
 — Before analysis, journalists must formally agree to EarthCheckr’s consent policy, fostering transparency.
Uploading the Draft
 — Editors submit the complete draft of their story, script, or visual cutlines into EarthCheckr’s interface.
Reviewing extracted claims
 — EarthCheckr isolates factual assertions, structuring them into a reviewable format. This is based on one very long and complex prompt.
Journalist verification
 — The journalist systematically verifies claims and submits a revised document with source confirmations as internal comments, typically as annotations.
Fact archiving
 — Verified claims are integrated into EarthCheckr’s database, which will eventually be used for reference purposes and future refinement.
Unlike 
automated fact-checking tools
 that seek to validate claims independently, EJN does not claim that EarthCheckr’s results are final — or even medial. The tool ensures that human expertise remains central to the verification process, preserving journalistic autonomy.
An example of the EarthCheckr consent form EJN editors and mentors provide the journalists we support with prior to publication.
How It Works in Practice: A Brief Example
Consider this hypothetical paragraph:
“The Amazon rainforest lost over 5 million hectares of forest cover in 2023, representing the highest deforestation rate in a decade. Studies indicate this has reduced rainfall in neighboring agricultural regions by 30%, while carbon emissions from the cleared areas have increased by 25% year-over-year.”
When processed through EarthCheckr, the tool identifies the following verifiable claims:
“The Amazon rainforest lost over 5 million hectares of forest cover in 2023”
“This represents the highest deforestation rate in a decade”
“Studies indicate this has reduced rainfall in neighboring agricultural regions by 30%”
“Carbon emissions from the cleared areas have increased by 25% year-over-year”
The journalist would then verify each claim with appropriate sources, such as:
Official deforestation monitoring data from INPE (Brazilian National Institute for Space Research)
Peer-reviewed rainfall pattern studies
Carbon emissions data from scientific institutions
After verification, the journalist would correct claim #1 to “4.28 million hectares” based on the latest INPE data and provide proper attribution for each factual statement.
Challenges
A tool like this is not without some challenges. Adoption by members of the EJN team has taken some time out of concern that it adds yet another task to the often-overwhelming workload shared by editors, mentors and, of course, journalists. Individuals who have not gone through a rigorous fact-checking process before are often surprised by the meticulousness of it. Obviously, not all journalists are comfortable having their unpublished drafts run through OpenAI’s API model, although there are 
explicit safeguards
 against stories being used as training data by the company. We at EJN do not have the capacity to peruse the source material that journalists provide as proof of any factual assertions they make in support of their stories after they’ve been run through EarthCheckr (think 
New Yorker 
fact-checker combing through every word of a transcript or public document referenced).
For us it’s still untenable to use an archive of verified claims from previous stories and drafts and then call a GenAI tool, say through retrieval-augmented generation (RAG), to check the veracity of a claim on a new draft. While models can be accurate with RAG most of the time, “most of the time” is not good enough when the threat of hallucinations persist with LLMs. And even if such an archive could be used effectively, building such an archive with our corpus of stories going back more than 20 years would be nearly impossible given that we’ve long lost contact with many of our previous grantees and partners who would need to opt in.
There are also copyright and legal considerations that render this nearly impossible. Finally, we can only hope these stories were previously fact-checked in a thorough manner, although we can’t assume this completely. It’s possible that part of this corpus dating back many years has not been fact-checked.
AI as an Editorial Enhancement Tool
With all these caveats noted, we still believe that EarthCheckr exemplifies how GenAI can be strategically deployed in journalism — not as a substitute for investigative rigor but as a means of optimizing verification workflows. While the role of AI in newsrooms continues to evolve, EarthCheckr presents a compelling case for its application in strengthening factual accuracy without undermining journalistic agency.
Reporters and editors require advanced tools to uphold factual integrity to deal with the flood of factually inaccurate information, and this is no less a problem in environmental journalism (e.g., misrepresentation of scientific data, climate change denialism, or industrial lobbying efforts). EarthCheckr represents a step forward in GenAI-assisted verification, offering a model in which AI augments, rather than replaces, the essential human elements of reporting.
While EarthCheckr is currently only available for internal use by Earth Journalism Network staff, we are hopeful to collaborate with partners to be able to bring it to a global audience of environmental reporters and editors in the future.]]></content:encoded>
</item><item>
<title><![CDATA[GenAI as News Gatekeeper? What Traffic Data Shows]]></title>
<link>https://generative-ai-newsroom.com/genai-as-news-gatekeeper-what-traffic-data-shows-783795ba2103?source=collection_home_page----df04cefca135-----11-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Web Analytics]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[Audience]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2025-03-06</pubDate>
<atom:updated>2025-03-06</atom:updated>
<content:encoded><![CDATA[GenAI as News Gatekeeper? What Traffic Data Shows
Analysis of Comscore data shows that GenAI tools like ChatGPT and Perplexity send little traffic to news publishers.
Nick Hagar
Follow
10 min read
·
Mar 6, 2025
--
Listen
Share
By 
Nick Hagar
 and 
Nick Diakopoulos
Generative AI has upended search, shifting it from something that directs users to outside sources into something that extracts and summarizes information on its own, a so-called “answer engine.” This change threatens news publishers, who have historically relied on search engines for a 
large share
 of their traffic. The tech companies providing these new interfaces 
continue
 to 
partner
 with news organizations, and Google 
claims
 that people continue to click on links on its AI overviews. But it’s still early days, and the question looms large for how these interfaces will impact the attention market: Will these new user portals result in any meaningful referral traffic for news publishers?
This post is the first in a series analyzing 
Comscore
 data to understand how generative AI tools interact with news publishers. Today, we explore the overall traffic patterns from ChatGPT and Perplexity and what they mean for news organizations. In future posts, we’ll extend the analysis to examine specific referral patterns to news publishers and provide a more detailed assessment of generative AI’s impact on their traffic.
The Stakes: Why ChatGPT and Perplexity Matter
While incumbents such as Google and Microsoft are also integrating generative features into the search experience, for this analysis, we focus on ChatGPT and Perplexity. Both tools represent serious efforts by new generative AI companies to enter the search space. 
ChatGPT integrated search
 in October 2024, and 
Perplexity
, an AI-powered search engine, launched in October 2022. These tools are also widely used — while Perplexity doesn’t report user numbers, one 
estimate
 puts it around 15M users, and 
ChatGPT
 has recently reported more than 400 million weekly users.
The ChatGPT Search user interface, showing a query response that integrates both news and non-news sources that users can click to read more.
News organizations have responded to the rapid growth of these tools in varied ways: Some, like the 
Associated Press
 and 
Axel Springer
, have negotiated licensing deals with OpenAI. Others, like the 
New York Times
, have pursued legal action around the use of their content in training LLMs. These decisions reflect how publishers weigh the 
risk of tech platform partnerships
 against their potential impact on referral traffic. By analyzing actual traffic patterns between AI tools and news websites, we can provide publishers with empirical evidence to inform their strategies as they navigate this technological shift.
Our Analysis Approach
To understand this relationship, we analyzed data from Comscore, a firm specializing in media measurement. We analyzed traffic data for the 5 month period from July to November 2024, for about 364,000 active U.S. desktop users in Comscore’s panel of people whose internet activity they track. This time period encompasses several major launches and updates for both ChatGPT and Perplexity — including the launch of ChatGPT Search — that may have impacted traffic to news sites.
Major GenAI product launches and updates during our study period (July-November 2024), including ChatGPT Search, which launched on October 31
Comscore’s dataset provides visit-level data for all referrals from chatgpt.com and perplexity.ai within our 5-month period. Before running our analysis, we transformed this data in two ways.
First, we removed any visits that met one of the following criteria:
Self-referrals (e.g., perplexity.ai to perplexity.ai, chatgpt.com to chatgpt.com or openai.com). This traffic signals that the browser is communicating back to the website for various functionality while the user is on the page.
Programmatic traffic, defined as any visit with a MIME type of application/*, or a blank MIME type. This traffic typically indicates the browser receiving some data from the site.
Redirect traffic (e.g., Google sign-in pages, static file serving) and tool-based traffic (e.g., browser extensions and AI plugins), defined via manual review of website domains.
Second, to identify where traffic flows from these services, we manually categorized the top 1,000 websites by traffic for each. We bucketed sites into one of 22 categories, which emerged through a bottom-up qualitative coding of each website. Major categories included education websites, news outlets, and programming tools (see Appendix A for a full breakdown of the categories). This helps us see what 
kinds
 of sources are getting linked to by ChatGPT and Perplexity and how traffic is flowing to those different sources. In our findings, we combined any categories that make up less than 1% of traffic for either service into an “Other” category.
These criteria allow us to focus on referral traffic that reflects a user taking action to navigate from one of these generative AI tools to visit an external website. In other words, we focus on traffic indicating a person actually visited the referred site. After filtering, our sample contains 145,065 visits (106,435 to ChatGPT; 38,630 to Perplexity) from 23,118 unique users (19,876 to ChatGPT; 3,977 to Perplexity), which form the basis for the findings we present.
Where Do ChatGPT and Perplexity Send Traffic?
Education sites receive 30% of Perplexity’s traffic, while technical resources receive 18% of ChatGPT referrals. News sites receive just 3% of referrals from ChatGPT and 7% from Perplexity.
Academic journals and publishers are a major referral category for both services,
 
with about 16% of traffic from ChatGPT and 22% from Perplexity
.
 Both services seem to take advantage of open access scientific resources, such as on PubMed, Semantic Scholar, and ResearchGate, and these types of links are often followed by users, suggesting an interest in seeing the details from these sources.
Both Perplexity and ChatGPT refer users to a range of scientific research sources.
Our analysis also reveals distinct referral patterns for both platforms, both in the types of sources and the specifics of exactly which sources are clicked. For instance, Perplexity directs roughly 30% of its traffic to educational resources while ChatGPT sends less than 5% of traffic to these types of sources. They also differ in the details of 
which 
education
 
sites they refer to with Perplexity leading to sites such as 
scribd.com
 and 
studocu.com
 along with university domains (e.g., 
ugm.ac.id
, 
unirioja.es
), while ChatGPT drives traffic towards sites such as 
texthelp.com
, 
coursera.org
, and 
khanacademy.org
. Another example: ChatGPT sends about 18% of its traffic to technical sites, including tech companies (e.g., documentation and product pages on 
microsoft.com
 and 
apple.com
), developer tools (
github.com
), and programming documentation (
nodejs.org
), whereas Perplexity only sends about 4% to such sites.
Notably, while social media platforms and Wikipedia 
dominate web traffic
 writ large, both make up relatively small amounts of referral traffic from these tools. Among social platforms, YouTube receives the most traffic from ChatGPT (8% of referrals). Wikipedia receives just 1% of ChatGPT referrals. In addition, referrals to Google products from ChatGPT are large enough to break out into their own bucket (6% of traffic). These referrals go to a range of services — most prominently, Search and Google Scholar, but also Docs, Maps, and other products.
News Sites Receive Negligible GenAI Traffic
News publishers receive minimal traffic from these AI tools: just 3.2% of ChatGPT’s filtered traffic (7% of unique users — approximately 1,400 of 20,000) and 7.4% of Perplexity’s filtered traffic (21% of unique users — approximately 800 of 4,000). Among the 143 news websites in our sample, only 5 received 100 or more unique visitors during the five-month period. In comparing Perplexity and ChatGPT, news appears to be more of a driver for Perplexity, ranking as the 3rd largest category, after education and academic journals. Perplexity linked to 110 unique news sources in comparison to ChatGPT’s 69, and only 37 of those overlapped between the two, indicating that each service is fairly distinct in the news sources it refers to.
Within news referrals, we also analyzed URLs that returned 404 (page not found) errors. These are significant because they represent potential model hallucinations, an issue 
reported on previously by Nieman Lab
, wherein an LLM fabricates a link to a news source as part of its answer. While we found only 21 such URLs in the Perplexity data, accounting for fewer than 50 referrals, missing links from ChatGPT were more common: We identified 205 such URLs. Almost all of these links go to prominent English-language outlets, with 197 of the missing URLs pointing to either 
hbr.org
, 
nytimes.com
, 
theguardian.com
, 
nationalgeographic.com
, or 
bbc.com
.
While we cannot say definitively whether these referrals are hallucinated links, our analysis provides a couple data points in favor of this argument. First, none of the URLs to those five news sites are archived on the 
Wayback Machine
, which is unusual for large news outlets (
nationalgeographic.com
, for instance, has 
over 10,000 URLs archived
). Second, manual review of individual cases reveal URLs that don’t seem to correspond to any live stories on these sites. For example, the ChatGPT URL 
www.nytimes.com/2023/10/15/us/biden-greenhouse-gas-emissions-court.html
 doesn’t line up with any stories published by the New York Times during October 2023 — the closest match is 
an article about the EPA
 from the following year. Similarly, this ChatGPT URL linking to The Guardian doesn’t line up with any coverage in May 2023: 
www.theguardian.com/technology/2023/may/26/ai-jobs-climate-bill-gates.
 However, it does have a close match in this 
June 2024
 story. These factors indicate that, while it’s possible ChatGPT is referencing web pages that either changed their URLs or were taken down, it is also likely that the model has hallucinated plausible-looking links to authoritative news outlets when responding to user queries.
After filtering out 404 errors, we found that international outlets dominate the top-referred news sites, with only 
hbr.org
 and 
bbc.com
 primarily publishing in English. In the longer tail we do find the likes of 
apnews.com
, 
sciencedaily.com
, 
reuters.com
, 
ft.com
 and other brand name publishers, but the traffic to each is minimal. Perplexity’s top news sources are exclusively Indonesian — possibly reflecting its 
popularity in Indonesia
 extending to Indonesian communities or connections in the U.S. panel we analyzed, driving these referral patterns. For ChatGPT, Spanish-language publications account for three of its five most-referenced sites. Notably, hbr.org receives ChatGPT referrals despite 
explicitly prohibiting
 
OpenAI crawlers
 in its robots.txt file.
Top-referred news sites show surprising regional diversity, with international outlets dominating referrals from both platforms despite the focus here on Comscore’s US panel.
OpenAI Partnerships Have a Small Effect
Within our sample of news websites, we identified six that had a publicly reported partnership with OpenAI, all announced before the start of our analysis period:
apnews.com
nypost.com
 (News Corp)
ft.com
lemonde.fr
elpais.com
 (Prisa Media)
businessinsider.com
 (Axel Springer)
Statistically, OpenAI’s partner sites don’t receive significantly more traffic than non-partners (Mann-Whitney U = 496.5, p=0.08). The raw numbers are quite small: partner sites averaged just 35 visitors compared to 17 for non-partners — a mere 18 additional visitors over five months. The optimistic interpretation of these findings is that, while the difference isn’t statistically significant, partner sites do receive roughly double the number of visitors as non-partner sites. However, as we discuss below, many publishers 
report getting negligible traffic
 from GenAI search tools. This, combined with the low percentage of traffic we see in this sample, suggest that these high-profile content deals primarily serve as licensing arrangements rather than meaningful audience development strategies.
Limitations
Our analysis has a couple key limitations. First, we examined only desktop traffic, excluding mobile usage, which represents a significant portion of how users access both AI tools and news. Although 
desktop usage appears to dominate mobile in the US
, including mobile traffic could shift the picture since mobile user behavior may differ in clickthrough and source needs. Second, our focus on U.S. users may not reflect global patterns, particularly given the international news sites appearing in our top referrals. These limitations suggest opportunities for future research to develop a more comprehensive picture of how generative AI affects publisher traffic across platforms and regions. In addition, future research should examine the impact of integrating generative AI in traditional search engines such as Google and Bing.
News Remains Peripheral in the GenAI Ecosystem
These findings paint a stark picture for generative AI tools as sources of news traffic. They’re also consistent with broader industry data. A recent 
analysis
 found that generative AI tools contribute less than 0.1% of referral traffic to 14 top news publishers. And while some news outlets 
say
 that they’re seeing growth in ChatGPT referrals, many see virtually no traffic from the platform. This mirrors our earlier analysis, which found that 
less than 2% of queries to LLMs were news-related
.
Together, these findings suggest that, to date, generative AI has not become a meaningful intermediary for news traffic. While GenAI search tools send referral traffic to resources associated with other kinds of information seeking, such as educational and technical materials, they are not connecting users to authoritative news outlets. Moreover, ChatGPT and Perplexity tend to direct users to somewhat different types of sources and well as different specific news sources.
Our analysis cannot speak to 
why
 news referral traffic is low. It’s possible that readers are satisfied with the information they get from generated responses and don’t click through to sources. It’s also possible that, in line with our 
prior findings
, GenAI users don’t leverage these tools for news reading, and are still doing traditional web searches, browsing news homepages, and engaging in other deeply entrenched news consumption habits. In either case, from a traffic or audience development perspective, the value to a news publisher of having their links included in a GenAI search tool’s response appears low.
Appendix A: Full category list
These are the 22 categories that we used to label referral destination sites.
Academic journals:
 Peer-reviewed journals, or the publishers of those journals
Blog:
 Personal or topic-specific blogs
Business and legal:
 Businesses, business consultants, legal services
Career:
 Job- and career-oriented resources
Design:
 Repositories of design resources (images, videos), design tools
ECommerce:
 Online shopping
Education:
 Educational resources (online courses, study guides), school websites
Finance:
 Financial information, tools, and institutions
Google products: 
Search, Scholar, Docs, etc.
Government:
 Official government sources
Health and science:
 Healthcare, health research, and scientific information
Interests and hobbies:
 Recreational and intellectual pursuits
Malicious or inactive:
 Spam sites, malware, pornography, offline or otherwise inaccessible
Marketing:
 SEO and other marketing resources
Music and streaming:
 Streaming services
News:
 News outlets
NGO:
 Non-Governmental Organizations, professional associations, think tanks
Publishing and culture:
 Book publishing, literature, cultural institutions
Reference:
 Dictionaries and encyclopedias
Search:
 Search engines
Social:
 Social media platforms
Technical:
 Documentation for software/programming tools, websites for software/tech companies]]></content:encoded>
</item><item>
<title><![CDATA[The Often Overlooked Water Footprint of AI Models]]></title>
<link>https://generative-ai-newsroom.com/the-often-overlooked-water-footprint-of-ai-models-46991e3094b6?source=collection_home_page----df04cefca135-----12-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Environmental Impact]]></category><category><![CDATA[Waterfootprint]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Julia Barnett]]></dc:creator>
<pubDate>2025-02-25</pubDate>
<atom:updated>2025-02-25</atom:updated>
<content:encoded><![CDATA[The Often Overlooked Water Footprint of AI Models
… and what that means for individuals, newsrooms, and journalists
Julia Barnett
Follow
7 min read
·
Feb 25, 2025
--
Listen
Share
Generative AI models continue to grow larger — not only in terms of tokens they output (
words
, 
pixels
, or 
minutes of music
) but also in terms of proliferation of both users and creators. This means that with each passing day there are more models being trained and more models being used. Both training and usage have associated costs beyond the financial ones: an often forgotten factor is the environmental impact of these models. The 
carbon footprint of these models
 has rightfully received 
public
 and 
academic
 attention, but even less discussed is the 
water footprint
 of these models. In a world where 
clean water is increasingly becoming a finite resource
, it is essential to also evaluate and be cognizant of the water usage of these models.
This post provides an overview of the findings from a 
recent pre-print paper by Li et al.
 which examines the water footprint of these models and encourages model creators to be more transparent about their water usage. Drawing heavily from that work, in this post I am first going to contextualize the problem in terms we can understand, explain how AI uses water, and then call attention to how this impacts the media ecosystem and what journalists covering generative AI can do.
Some Big Numbers
First I’m going to present some jarringly large numbers about the water footprint without any context:
GPT-3
 (OpenAI’s 
now outdated and surpassed model
 from 2020) training evaporated 
700,000 liters
 of fresh water.
Google’s data centers in 2023 
withdrew 
29 billion liters
 of fresh water for on-site cooling.
The global AI demand in 2027 is 
projected to account for
 
4.2–6.6 trillion liters 
of water.
You might be thinking, “huh, that sounds like a lot.” But most likely, you have no reference for what that actually means. Now let me give you a frame of reference to understand these numbers. The 
Philadelphia Water Department
 estimates the average American’s daily water usage amounts to 384 liters, assuming the following activities take place:
🛁 One full bathtub: 136 liters
🚽 6–8 flushes per day: 68–91 liters
🚿 10 minute shower: 76 liters
👕 Washing machine: 57 liters
🍽️ Dishwasher load: 15–38 liters
🧼 Personal hygiene: 9.5 liters
🥛 8 glasses of water: 4 liters
Now let’s revisit the first numbers discussed above:
GPT-3
 training accounted for 
5 people’s yearly
 water consumption (1823 days of an individual’s water consumption).
Google’s data centers in 2023 accounted for
 206,906 people’s yearly
 water consumption (75,520,833 days).
The global AI demand in 2027 is projected to account for 
30–47 million people’s yearly
 water consumption (about the population of Canada).
It is also important to note that these estimates are low; 
Google’s water consumption
 grew 20% in 2022, 17% in 2023, and continues to grow now. 
Microsoft saw similar growth
, and those are just two of many 
players in this field
.
How does AI use water?
Much like the cloud, it can often be difficult to conceptualize the tangible physical impact of these AI models. AI uses water because the data centers that power these models generate electricity, and that produces heat. In order to cool that heat, they use water.
Two important definitions to encompass 
water usage
:
Water withdrawal
: freshwater taken from the ground or surface water sources, then used for agricultural, industrial, or municipal uses.
Water consumption
: “water withdrawal minus water that can be recycled”; amount of water consumed, evaporated, transpired, incorporated into products or crops, or otherwise removed from the water environment.
There are three primary scopes to define AI water usage:
Scope 1: Direct usage for on-site data center cooling
Scope 2: Off-site water for electricity generation
Scope 3: Supply chain water for server manufacturing
In this post we will focus on scope 1 because it is the most well-documented, but the other two combine to form a much larger indirect impact than just on-site cooling.
Taken directly from 
Li et al.
, an illustration of how a data center can use water.
In the figure above you can see the illustrated process of how AI uses water. The data centers generate heat, and in order to cool that down a cooling tower evaporates water to dissipate the heat. Non-evaporated water can only be recycled a few times, and roughly 80% of this water is consumed (not just withdrawn). This is all 
clean water
, and often potable
 to avoid clogging pipes or bacteria growth. Another method not depicted in this figure is in more temperate climates using air cooling without water evaporation, but if the air gets too hot (>85°F) or too dry, water is added anyway to assist.
Training of these models (a much higher level of computational cost that occurs prior to the usage of models) takes the bulk of these energy and cooling demands, but each individual use of the model has its associated water costs as well.
The following estimates are for GPT-3 which is as discussed an outdated and smaller model than current LLMs — assume that these are lower bound conservative estimates for models currently used like GPT-4o and Google Gemini. 
Li et al.
 estimate that GPT-3 “drinks” (i.e., consumes) a 500ml water bottle for every 10–50 medium sized (input: ≤800 words; output: 150–300 words) responses. This varies by location: if you’re using a data center in Illinois or Iowa it might only be about 33 responses, whereas in Arizona or Sweden it only takes 17 requests to use the same amount of water. In this study, Ireland is listed as the most water efficient, with it taking 70 responses to consume 500ml. The “when” also matters — it is more water efficient to cool at night when the sun is not out. Unfortunately this contrasts directly with carbon emissions, which can be reduced during the day using solar energy. We need more efficient processes that balance the demands of both carbon and water efficiently.
Where do we go from here?
Here I explore what this all means for individual users of generative AI, for news organizations seeking to adopt the technology, and for journalists covering it.
Individuals
Everybody reading this has probably gotten their hands on some sort of generative AI model at one point or another, whether it was using ChatGPT to assist with some tasks or Midjourney to generate some images. Regardless of the monetary cost of using these services, there is an environmental water cost associated with all of this usage. Writing this in Illinois, every 33 responses generated (or likely, fewer than that since new models are more demanding) equates to the consumption of one 500ml water bottle. Though small numbers in the grand scheme of things, that can add up across lots of people and extended periods of time (OpenAI has an 
estimated
 400 million users every week). This creates an environmental incentive to be conscientious and intentional about the way we use these models. There is no need to thoughtlessly query generative models again and again, but if you are a heavy user, consider trying to balance that by reducing other forms of your water consumption, such as by taking a shorter shower.
Newsrooms
Newsrooms are increasingly adopting various generative AI technologies into their processes. Whether it’s using 
LLMs for document processing
, 
creating tip sheets for investigative reporting
, or the seemingly pervasive 
adoption of chatbots
, newsrooms around the globe are integrating generative AI into their processes. The New York Times is “
all-in
” on using internal AI tools now. LLMs can and should be used for various tasks, especially in under-resourced newsrooms who are turning to generative AI to assist with more mundane tasks. The key factor is that these integrations shouldn’t be frivolous; they need to be thoughtful and show real utility. For a 
variety of reasons
 beyond the environmental costs, newsrooms should tread lightly and carefully with the integration of generative AI into their news pipelines, such as the potential for a 
loss of human touch, lack of fact-checking, and sensationalism
 that could arise from poor usage of these models. The environmental factor is yet another reason to be more conscientious and thoughtful of the usage of these models — understanding every model trained and inference performed has both a carbon and water footprint should add to that level of thoughtfulness prior to deployment. To reduce their environmental impact further, newsrooms may also seek 
green data centers
 for supporting their computing (and AI) needs.
Journalists and media professionals
Finally, and perhaps the most important call to action: the voices of journalists and media professionals have the power to steer the agenda on AI towards environmental concerns. They have access to institutions and individuals the rest of the world may never encounter, and they have a chance to ask the right questions and have the rest of the world hear the answers. When stories are publicized, more people may start caring about them. There is a crucial lack of transparency about the actual water footprint of these models — as a journalist, 
start asking tech companies about it
. If this massive drain of clean water from our global supply continues without question or scrutiny, nothing will change. We need more holistic methods to address the carbon and water footprint of these models in tandem, and the first step to changing that is calling out the problem and building public awareness. Creating positive public pressure for change could help incentivize companies to address the problem.
Concluding thoughts
Generative AI is extremely promising for various tasks across many industries and specifically within the media ecosystem. Training and usage of these models also comes with an environmental cost, and within this larger umbrella the water footprint has been largely undiscussed. As the climate crisis continues to worsen, 
clean water is becoming an increasingly limited resource
. We all need to be more conscientious about the impact these models have on the environment both in terms of the constant churn of new models being trained and our own usage of them. Journalists and media professionals are in a unique position to raise awareness about this issue with a much wider reach than most individuals possess. This is a problem that needs to be addressed and awareness of the issue is the first step.]]></content:encoded>
</item><item>
<title><![CDATA[How to Check Whether AI Assistants Are Distorting Your News Stories]]></title>
<link>https://generative-ai-newsroom.com/how-to-check-if-ai-assistants-are-distorting-your-news-stories-1fd7d2002a1c?source=collection_home_page----df04cefca135-----13-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Ai Assistant]]></category><category><![CDATA[News]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Clare Spencer]]></dc:creator>
<pubDate>2025-02-19</pubDate>
<atom:updated>2025-02-19</atom:updated>
<content:encoded><![CDATA[How to Check Whether AI Assistants Are Distorting Your News Stories
Learnings from a BBC experiment that can help you run your own tests
Clare Spencer
Follow
9 min read
·
Feb 19, 2025
--
Listen
Share
The 
BBC recently published research
 which it says shows AI assistants — including ChatGPT, Google Gemini, Microsoft Copilot, and Perplexity — cannot be relied upon to provide accurate news.
Their report highlighted three key findings: (1) “51% of all AI answers to questions about the news were judged to have significant issues of some form.”, (2) “19% of AI answers which cited BBC content introduced factual errors — incorrect factual statements, numbers and dates.”, and (3) “13% of the quotes sourced from BBC articles were either altered from the original source or not present in the article cited.”
Some of the inaccuracies BBC journalists found in the four AI assistants’ answers included:
Google’s Gemini incorrectly stated that “The NHS advises people not to start vaping, and recommends that smokers who want to quit should use other methods”. In fact, the BBC reports that the UK’s National Health Service does recommend vaping as a method to quit smoking.
OpenAI’s ChatGPT claimed in December 2024 that Ismail Haniyeh was part of Hamas leadership. The BBC points out this can’t be true because he was assassinated in Iran in July 2024.
Microsoft’s Copilot incorrectly stated that rape victim Gisèle Pelicot uncovered the crimes against her when she began having blackouts and memory loss. In fact, the BBC reports, she found out about the crimes when the police showed her videos they had found when they confiscated her husband’s electronic devices.
Perplexity mis-stated the date of celebrity doctor Michael Mosley’s death.
Ismail Haniyeh was dead so was not part of Hamas leadership.
BBC News CEO 
Deborah Turness said
 the research was a first step in determining the scale of the problem with how news is distorted by AI assistants. “It is likely other publishers may be similarly affected by the issues the research has identified”, the BBC’s Programme Director Generative AI Pete Archer said in the foreword of the research. Indeed, the New York Times’ IP case filed against OpenAI and Microsoft in 2023 had already begun to build the argument around how generative AI tools such as ChatGPT were mis-representing their content, potentially misleading users and undermining a brand built on quality information.
The scale of resources used for the BBC research were not trivial. I talked to Oli Elliott, the principal data scientist in the BBC’s responsible AI team, who explained that his team ran a preliminary test in August 2024, a good six months before the final report was published in February. He has been working full time on the research since as early as November, and working alongside two others, including a newsroom editor and project manager, who have been dipping in and out to support the project. In December, 45 journalists reviewed the four AI assistants’ responses to 100 questions, spending roughly 30 minutes to an hour reviewing each question.
As 
the call for evaluating generative AI’s fitness for working with news grows
, you may be asking how your newsroom could run a similar test on your own news stories. If you want to do that here are some of the steps you’ll need to take to replicate this research:
1. Give AI assistants access to your stories
This step is only for news organisations who, like the BBC, were trying to restrict web crawlers from accessing their website to scrape their data. Once a decision was made to allow the AI assistants’ web crawlers to access the BBC site, Oli contacted various engineering teams to ask them to stop efforts to block access. This included removing the four AI assistants’ crawlers from the “disallow” list in the 
robots.txt file
, which contains instructions for bots around which webpages they should or shouldn’t access. The engineering team removed these blocks for just over one month while the research was conducted.
He then asked the AI assistants questions and started seeing evidence that the AI assistants’ web crawlers had access to BBC articles close to a week after restrictions were removed.
2. Find questions to ask AI assistants
Oli and his team wrote 100 questions to ask four AI assistants. To get ideas for what normal people would ask AI assistants, he used the catalogue of questions people ask of search engines. He took this from BBC News’s growth team who monitor search trends and send journalists regular emails about what people are searching for. If you don’t have a team which does this research, you can do it yourself by using 
Google Trends
.
From this starting point, Oli picked topics from big news events the BBC had written about.
The questions were oriented towards content spanning from nine months ago until two weeks before he tested the AI assistants. “There’s always a little bit of a gap between when you gather these responses and when the journalists are going to review the results. So if news events have happened in between that gap, then it’s quite hard for people reviewing them to put themselves back in that mindset of when we ask the AI assistants, that’s just a very difficult thing to do. So we were trying to not pick anything that was live news right now but also we wanted things that were recent enough that they would be in people’s minds”, he explained.
He had to get rid of about four questions after the journalists reviewed them because he realised these were not good questions for them to review. The journalists’ responses were “I don’t know”, “this is too hard” or “the question is not appropriate to review”. For example, he scrapped “how do I drive through a flood?”. He decided there were questions that were different to what he was testing in that they weren’t about news events and if he were to run the test again he would ask “do we feel like the BBC would be the best source for this answer”. Based on this learning, he suggests you keep your questions specifically around news events that your organisation has covered.
One of the reasons Oli used so many journalists to help review responses was because they were covering a really wide range of topics. For newsrooms that have fewer resources, Oli suggests you can reduce the burden by narrowing the scope of those questions.
3. Write a prompt
Here’s the prompt Oli typed into the AI assistants’ chatbot interfaces:
Use BBC News sources where possible. [QUESTION]
So for example:
Use BBC News sources where possible. Is vaping bad for you?
Use BBC News sources where possible. Who could use the assisted dying law?
If you are reviewing the extent to which AI assistants are distorting your publication’s news stories, replace “BBC News” with your own organization name.
Oli noted that the wording of the prompt has been criticized since the research was published. 
On LinkedIn he was accused
 of “bad prompting” which, the commentator speculated, may have contributed to such damning results.
He defended the simpler prompt. “There’s a lot of material about how to write good prompts. We wanted to make sure that it was close to how we thought a standard audience member would use these tools. There isn’t a lot of research about how people use them, but we figured it would be like people using it like a search engine in most cases — a bit like when you type those questions into Google,” he said.
Assuming that people use AI assistants in the same way that they use search engines is a significant decision. Prior research 
published on this blog 
suggests a potentially wider range of news-related query approaches taken by real users of chatbots. And so as more research on users’ expectations of chatbots becomes 
available
, it would be helpful to try to match the query you test with actual user behavior as much as possible.
4. Ask the AI assistants
The BBC study posed 100 questions to OpenAI’s ChatGPT, Microsoft’s Copilot, Google’s Gemini, and Perplexity. Oli did this within two days on December 5 and 6, 2024. “I had a really fun two days of copying and pasting stuff into various AI assistants and then copying and pasting the answers out,” he sarcastically explained. Despite the tediousness, it’s important to test the system closest to what a user will experience — although APIs are available for testing some models, sometimes such APIs can give different results from what an actual user of the product would see.
5. Prepare AI assistant responses for journalists to review
Oli put the AI assistant responses into a form for journalists to review, simplifying the process for journalists to evaluate the responses based on their expertise.
The form listed the question, the answer, links to citations the AI assistants used, and a rating scale from 1 to 4 for accuracy, attribution of sources, impartiality, context, editorialisation, and distinguishing opinions from facts (as elaborated in the Methodological Appendix of the 
research report
). Finally it left a space for journalists to help identify and elaborate on the concerns they were raising. He did not pass on to the journalists which AI assistant they were reviewing.
6. Journalists review AI assistants’ answers
This is the unique part of the research. In the final report, the conclusion states: “This study is the first, that we are aware of, to use journalists to review the responses of AI assistants to questions about the news”.
45 journalists reviewed 362 AI assistant responses over a two week period. They weren’t working on this full time. Some took up to three hours in between their other work. Oli almost doubled the number of journalists he originally had planned to use as he found that after reviewing one to two questions the journalists were pulled back into their regular day job.
This was the most intriguing achievement of the research for me. Until a few years ago I was a journalist in that same newsroom. I cannot imagine I would have been allowed to devote even half an hour to anything that wasn’t writing a news story.
Oli told me that the key to achieving this was that the tasks were assigned by the news editors and not him.
7. Check journalists’ responses
Oli recommended focusing your energies on triple checking the errors journalists have flagged up to ensure they are indeed errors.
8. Analyze the results
Oli told me he spent at least two weeks writing up the results. Then what he described as “rounds of reviews and checks” took roughly a month. How long that final sign off part takes you may depend on the complexity and size of your organization. I suspect this might be easier and faster for smaller newsrooms.
9. Share the results
The BBC senior leaders appear keen that other media organizations research if AI is distorting news content. The report says: “We will also look at whether we can bring in other publishers and media organisations into the next phase of research”. However, currently, there is no formulated plan for how your own results could be shared with the BBC or how media organizations can collaborate on this type of research.
Pete Archer, the head of responsible AI, had also expressed a hope that external observers would get involved, saying in the foreword to the report: “… there would be value in regular evaluations which may sit best with a regulator or research institute — providing an independent view of the accuracy and trustworthiness of news content on AI platforms, including AI assistants.”
So, at the time of writing my advice would be to factor in some time to share your results, but you may also want to check to see if any progress has been made in wider co-ordination.
10. Repeat
The conclusion of the report says the BBC hopes to repeat the experiment in the “near future” in order to give a view on whether AI assistants are improving over time. Much in the way AI systems are already evaluated on various benchmarks when they are released, having a benchmark indicating response quality for news-related queries, could help orient the technical field towards improving their systems over time. Oli told me he doesn’t have a date confirmed for the next iteration just yet. Perhaps also with more news organizations repeating these kinds of evaluations the community can build up a broader set of results over time.]]></content:encoded>
</item><item>
<title><![CDATA[How do News Audiences Experience and React to Generative AI in Journalism?]]></title>
<link>https://generative-ai-newsroom.com/how-do-news-audiences-experience-and-react-to-generative-ai-in-journalism-62c57d18ad9c?source=collection_home_page----df04cefca135-----14-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Audience]]></category><category><![CDATA[News]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[T.J. Thomson]]></dc:creator>
<pubDate>2025-02-17</pubDate>
<atom:updated>2025-02-17</atom:updated>
<content:encoded><![CDATA[We know comparatively little about how news audiences experience and react to generative AI in journalism. 
Hero image by TJ Thomson on Midjourney.
How do News Audiences Experience and React to Generative AI in Journalism?
Expectations and comfort levels with GenAI in news production based on 60 interviews in Germany and Australia
T.J. Thomson
Follow
13 min read
·
Feb 17, 2025
--
Listen
Share
By Dr. 
T. J. Thomson
 and A/Prof 
Ryan J. Thomas
Researchers have focused considerable attention on AI-generated and -edited 
content in journalism
 and on the perceptions and 
practices
, sometimes shaped by 
editorial guidelines and policies
, of journalists. But what of the people for whom journalism ostensibly exists — news audiences?
We argue that attention to news audiences’ experiences, perceptions, and expectations, overall, and regarding AI in journalism is needed to improve the journalist-audience relationship and to ensure that journalistic practices are informed (though not necessarily determined) by audience expectations.
The comparatively little audience-focused research on AI and journalism to date — much of it survey-based — has revealed a somewhat cloudy and developing picture. The Reuters Institute for the Study of Journalism’s 2024 
survey
 (focused on populations in Argentina, Denmark, France, Japan, the United Kingdom, and the United States), for example, concluded that “many have not made up their mind” regarding the impact of generative AI.
Other research has found that audience comfort with AI in journalism can vary significantly by country and by how AI is used. For example, 
recent research
 conducted in the US found that news-related chatbot queries comprised only two percent of all user queries, though this is, of course, only one of many ways of deploying AI in news contexts.
This all speaks to the need for qualitative research that can delve into ordinary audiences’ everyday experiences with generative AI in journalism, identify their expectations, and more fully appraise their comfort across specific applications of this technology. We try to advance this ambition by interviewing 60 news audience members in the European Union (widely regarded as leading the world in terms of AI regulation) and in Australia (which has been called “
at the back of the pack
” in regulating AI).
We focus first on news audiences’ experiences with generative AI, second on what they expect of journalists and news organisations regarding their use of generative AI, and third on their comfort levels with specific applications of AI in journalism across three domains: enriching and brainstorming, editing, and creating. This last ambition complements a recent Reuters 
report
 also focused on audience comfort with use cases but with largely different ones than the visual, multimodal ones we use and with participants from different countries (Mexico, the UK, and the USA) to those in our study.
Our approach
Our earliest 
work
 on AI and multimodal journalism began in 2022 when we used generative AI tools to explore how artificial intelligence visualised journalists and journalistic roles. After examining AI-generated content, we turned next in 2023 to 
industry perceptions and practices
.
Our early research on generative AI focused on how this technology visualises journalism and journalistic roles. It identified a number of biases across ethnic background, gender, class, and environment.
In 2024, we focused on news audiences. We took what we learned from practitioners about how multimodal, visual AI is — or could be — used in journalism and designed an exhibition that visualised some of those use cases, including using tools like Adobe’s “
generative expand
” to imagine what lies beyond an image’s frame, using AI to visualise the past or future, using AI to create a hero image for a news article, and using AI to visualise spreadsheet data.
This 
exhibition
 ran at ACMI in Melbourne, Australia, in early 2024 and allowed ordinary people to react to the use cases so we could gauge their comfort with them. This early feedback allowed us to tweak and refine the use cases we selected for interviews we conducted later in the year with news audiences in the EU and Australia. We recruited participants using social media ads and tried to select a diverse group of people from those who expressed interest, resulting in an average participant age of 36 years old with a range from 18 to 70.
In addition to presenting our interview participants with a selection of more than two dozen use cases of multimodal AI in journalism and asking them about their comfort with each, we also asked them whether they had, to their knowledge, ever encountered generative AI in journalism during their day-to-day life and what they expected journalists and news organisations should or shouldn’t do with this technology.
In the rest of this article we summarise some of our findings from this study. (For all the extensive details, see also the complete 
report
, co-authored with A/Prof Michelle Riedlinger and Dr. Phoebe Matich, that was published in February 2025.)
News audiences’ experiences with generative AI
While most of our 60 participants (30 from Australia and 30 from Germany) had reported seeing AI-generated or AI-edited content online, particularly on social media, only about a quarter of them reported having encountered this content within a journalistic context.
This included, in order of frequency:
Seeing AI-generated or -edited content on fact-checking websites or in news outlets’ reporting on viral AI content (10%).
Seeing AI-generated text, including on news aggregators, like Apple and Google News, on financial news outlets’ websites, such as Forbes, and from what participants described as smaller, less resourced news organisations (8.3%).
Seeing AI-generated images in news (5%).
Reading AI-translated journalism (3.3%).
Seeing notices that AI had been used to edit news articles (3.3%).
Encountering an AI-generated weather report (1.6%).
Seeing AI-generated images in news was one of the ways our participants said they encountered generative AI in journalism. Here, the Daily Telegraph turned to AI to generate a number of illustrations — some more photorealistic than others — to accompany some of its commentaries.
These audience members appreciated the flexibility AI provided to consume news in different formats (e.g., as spoken narration rather than written text or to understand an article in a language they didn’t know) but also bemoaned kinks, glitches, and ways that AI failed to represent the world accurately. One participant, a 22-year-old man living in a municipality of about 4,000 people in Bavaria, described his experience with AI-translated journalism this way:
Business Insider
 … they seem to use AI to translate their English text to German and there’s a remark about that, sometimes. They say that they have used AI or machine learning or whatever to translate the text and there are some errors in those texts, quite a few times. I read an [AI-translated] news article a few weeks ago and I didn’t really know what they wanted from me after reading it. It just felt strange as if no one had read through it after translation. I don’t think that AI is advanced enough to let it run by itself without checking it, especially for journalism. You need some human input.
Participants who had encountered AI-generated images in journalism reported that images “don’t look very nice” and were “not like real life.” Participants said they felt more comfortable with AI being used to summarise news articles compared to writing them. In the words of a 47-year-old German man living in a city with a population of around 1 million:
I felt a little bit defrauded, a little bit defrauded and misled insofar as I tend to expect human interaction when it comes to journalism. And when a machine does journalism, what is it? It’s just the code.
While only a quarter of participants were confident they had encountered AI content in a journalistic context, the same proportion was confident they had not encountered such content. Forty percent suspected they had encountered AI content in journalism while the remaining 10 percent were unsure.
News audiences’ expectations
Participants identified 24 unique expectations of how they thought journalists or news organisations should or shouldn’t use generative AI in journalism. We summarise below the four most common expectations.
Have clear boundaries in place
The most common expectation participants had was that 
newsrooms should have guidelines in place governing how AI can be used
. All but one of our participants (98.3%) wanted guidelines or policies in place to ensure clarity for journalists and audiences alike about the acceptable use of AI within the organisation and the content it produces, which participants said affected their trust (the one outlier participant, a 50-year-old woman living in Melbourne, said, “I guess it’s up to them to decide. It’s not for me to tell them how to run their media outlet”). The remainder regarded news outlets without AI policies as not producing “true journalism,” putting their reputations at risk, and opening themselves up to inconsistency and confusion. In the words of a 37-year-old Adelaide woman, “There must be rules and regulations about what journalists can produce as part of their role. I don’t know what percentage of AI they can use. I’ve got no idea, but there must be rules.”
Be transparent
The second most-voiced expectation regarded 
transparency with AI use
, mentioned by 85 percent of participants. Participants’ transparency expectations depended on the information mode (such as whether the information was a video, still image, or text, for example), how AI was used, and how much AI had been used. In general, participants preferred context around AI to be “clearly stated at the beginning, rather than right at the end or hidden away somewhere.” In one participant’s words, “I feel like they’d have a tendency to hide it down the bottom in little tiny baby text no one can actually read, so yeah, like big text at the top of the article saying like this has been generated by AI or like exactly how they’ve used AI to create the article.”
Many participants desired a sense of the proportion of AI used in content (for example, if AI had been used for behind-the-scenes tasks like research, brainstorming, or optimising; editing of existing content; or creating new content) and for that to be transparently shown so audiences can form their opinions on the content’s accuracy and representativeness with that context in mind. Participants also appreciated when this information was in the same relative location each time so that it becomes easier to identify.
Participants said they preferred visible indications of AI use on the content itself (such as a watermark, label, or other recognisable symbol) rather than in supplementary information like captions on images since these details are sometimes missed and since they’re not seen on social media previews when content is seen but when people might be misled if they don’t click or tap through to obtain more context. Participants desired some sort of universal symbol that could be used industry-wide to designate AI-generated or AI-edited content.
Some participants recognised the stigma that comes with labelling AI-generated content as such but still desired this transparency. Participants wanted good-faith transparency and not just disclosures like those found in advertisements that participants thought existed so advertisers could “avoid getting sued.” Participants also recognised the potential for clutter and said, if this is an issue, an on-demand approach could work that could be “as simple as hovering a mouse [or finger] over and it coming up and explaining how AI has been used.” One participant likened this to obtaining a cut of meat at the supermarket: “If I buy a piece of meat and I want to know where it comes from, then I can figure it out and read it on the back of the product, so if I want it, it should be easy access, but it’s on-demand, because, of course, otherwise I’m too distracted about how the article was written or created.” Participants also suggested news outlets treat disclosure not like the “fine print you read in various banking things” but rather by explaining how the use of AI will benefit the audience member.
Keep humans in-the-loop
The third most frequently mentioned expectation participants voiced was that
 journalists should review and verify AI-generated or AI-edited content
. This was mentioned by about one-third (30%) of the participants. Participants regarded this as essential for the integrity of the information relayed and for the reputations of the journalists and news outlets publishing it as well as a strategy to ensure human labour continues to be valued and used in journalism. Mandating human oversight of AI generation or editing would, in the words of one participant, help “check the accuracy of information and [ensure] that all the sources have been cited.”
Use AI as little as possible
The fourth most-common expectation was that 
journalists and news outlets should use AI as little as possible
. This expectation, mentioned by about 20 percent of participants, stemmed from their desires to ensure the authenticity of journalistic content while also valuing human labour and creativity. Participants sought AI use that complemented or augmented human labour rather than supplanted it, as a 23-year-old woman from regional Queensland said:
At this stage, it’s impossible to say we shouldn’t have AI and AI should never exist. And I’m not saying there’s no capacities where it’s ever useful, because that’s a really big, blanket statement, but I would really, really want to see its use absolutely minimised, particularly in terms of replacing people’s jobs that work in newsrooms, and also of creating images or writing texts that then go on the news, like if it was being used in research earlier on. But I don’t think what’s presented to the public should involve AI at the final, end stage. I don’t like that at all.
News audiences’ comfort levels
The last part of our research probed how news audiences reacted to and described their comfort levels in relation to using AI for various use cases in journalism. We provide below a high-level summary of our findings across three domains of use cases: 
enriching and brainstorming
, 
editing
, and 
creating
. At the end of the article, we provide a visual overview of participants’ comfort levels with each use case.
Enriching and brainstorming using AI
The first domain of use cases, enriching and brainstorming, contained three examples: using generative AI to recognise people, objects, animals, or environments in visual assets (photos or videos) and apply keywords to their associated metadata; brainstorming how to represent a hard-to-visualise topic; and creating a storyboard for narrative inspiration. Overall, participants were more positive than negative about these use cases, and most participants supported each application. On average, about 75% of the participants were comfortable with AI being used in these ways.
Around two-thirds of our participants were comfortable with journalists turning to AI to get help visualising a narrative through a storyboard.
Editing with AI
The second domain of use cases, editing, contained seven examples: removing watermarks using AI, adding background blur in a photo using AI, using AI to upscale images, using AI to automatically make multiple photographs and composite together the best elements, using AI to edit a video through a transcript, using AI to expand a photograph’s frame showing a person and a landscape, using AI to animate a historical image, using AI to animate a contemporary image, and using AI to automate layouts. Overall, participants were less comfortable with these use cases compared to the brainstorming and enriching use cases. Just under half (46 percent) supported these use cases, on average.
Around 70 percent of our participants were comfortable with journalists using AI to artificially adjust an image’s depth of field (i.e. adding background blur) — either using desktop software, shown here, or using smartphone camera modes like an iPhone’s “portrait mode.”
Creating with AI
The third domain of use cases, creating, contained 10 examples: generating a colour palette for use in an editorial design, generating icons for a news infographic, visualising the past (text-to-image), illustrating an article in a non-photorealistic style, illustrating an article in a photorealistic style, visualising publicly accessible data (text-to-image), visualising user-uploaded data (spreadsheet-to-image), generating 3D models, generating b-roll footage (text-to-video), and generating a virtual news presenter. Overall, participants were less comfortable with this domain of use cases compared to the previous two. Less than half of participants (44.8 percent) were unequivocally comfortable with these use cases.
About two-thirds of our participants were comfortable with journalists using AI to create icons for an infographic.
The findings reveal diverse reactions to AI being used in journalism across domains of 
brainstorming and enriching
, 
editing
, and 
creating
. Still, despite the wide range of reactions, some consistent patterns can be identified, such as audiences saying they felt more comfortable when AI disclosures were in place, when human oversight was involved, when the use of AI doesn’t impact the accuracy of the representation, when AI complements rather than replaces human labour, and when AI use respects the privacy of individuals’ and organisations’ data. Participants’ comfort levels for each of the 23 use cases in this study can be seen below.
As this chart shows, while, overall there was little difference in the averaged comfort percentage between the “editing” and “creating” domains (46% and 44.8%, respectively), there were wide variations in comfort within both domains (86.6% comfortable with generating colour palettes compared to 6.4% comfortable with generating a virtual presenter, for example), which illustrates how context-dependent audience comfort is.
Conclusion
Comparing our study’s findings to previous research, several aspects become apparent. While there has been growing attention paid to aspects such as where in the process AI is used, 
the degree to which it is transparently disclosed
, whether AI misleads or deceives, and 
the effect of AI use on human labour
, there has been less attention paid to other aspects that affect news audiences’ (dis)comfort with AI in journalism. For example, our study identified other factors, such as audience members’ own use of or awareness of the AI tool or process in their personal or professional life; issues of consent, legality, and privacy; whether the output deals with the past, present, or future; and AI’s (in)ability to understand cultural context: that affected their (dis)comfort with journalists using generative AI. These factors should inform newsroom principles and policies around AI use and lead to better outcomes for all stakeholders involved.
Acknowledgments
We wish to acknowledge Rebecca Venema, Dina Strikovic, Oscar Toohey, Caroline Gardam, Jean Burgess, Michelle Riedlinger, and Phoebe Matich for their help with data collection and the Center for Advanced Internet Studies for its support of this research through a visiting fellowship.]]></content:encoded>
</item><item>
<title><![CDATA[Beyond Platform Fact-Checking: The Promise and Limits of AI Verification]]></title>
<link>https://generative-ai-newsroom.com/beyond-platform-fact-checking-the-promise-and-limits-of-ai-verification-2bb356dbe9ed?source=collection_home_page----df04cefca135-----15-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Fact Checking]]></category><category><![CDATA[Facebook]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2025-02-10</pubDate>
<atom:updated>2025-02-10</atom:updated>
<content:encoded><![CDATA[Beyond Platform Fact-Checking: The Promise and Limits of AI Verification
As Meta steps back from centralized fact-checking, could AI and user-driven tools fill the gap?
Nick Hagar
Follow
5 min read
·
Feb 10, 2025
--
Listen
Share
LLMs are good at synthesizing, summarizing, and structuring information. Could these skills make them effective fact checkers?
Meta’s recent decision to 
end third-party fact checking
 in the U.S. and 
deactivate automated misinformation classifiers
 raises an urgent question: Could artificial intelligence step in to fill the gap? As platforms scale back their human verification efforts, 
many
 are 
looking
 to large language models (LLMs) as a potential solution for detecting and limiting the spread of misinformation at scale.
The stakes are significant. Meta’s fact-checking program wasn’t just about correcting individual false claims — it provided vital signals to Facebook’s recommendation algorithms, helping to reduce the spread of misinformation by 
over 90%
. While fact checking has always had 
limitations
, its integration with these algorithmic systems made it a powerful tool for dampening the spread of everything from harmless falsehoods to dangerous conspiracy theories.
This pivot point offers an opportunity to examine both the promise and limitations of AI-powered verification. As traditional platform-driven fact checking recedes, could generative AI provide a new paradigm for helping users navigate an increasingly complex information landscape?
Research
 
suggests
 that LLMs struggle with key aspects of fact checking. While they can assist with small tasks like transcription or summarization, they 
cannot independently verify claims
 or reliably communicate their findings.
These findings reflect how professional fact checkers use generative AI in practice. A 
2024 study
 from Robert Wolfe and Tanushree Mitra at the University of Washington, presented at the Fairness, Accountability, and Transparency (FAccT) conference, surveyed 38 employees across 29 global fact-checking organizations. The findings reveal a clear pattern: fact-checkers primarily use LLMs for peripheral tasks early in the reporting process — things like translation, conducting web searches, and analyzing images. They do not rely on AI for the core work of verifying information, citing sources, and writing stories. This frames generative AI as an 
augmentation
 of human fact checking, rather than a 
replacement
.
A real-world example of this hybrid approach comes from 
VERDAD
, a generative AI pipeline that transcribes talk radio broadcasts and analyzes them for potential cases of political misinformation. As in the research described above, the LLM in this pipeline does not fact check autonomously. Rather, it flags segments of audio as potential misinformation candidates, passing them on to a human journalist for review. The model’s benefit comes from its ability to quickly review hundreds of hours of audio, complementing the journalist’s ability to verify or rebut claims.
Figure from 
Wolf & Mitra (2024)
 outlining potential use cases for generative AI in fact checking. The most mature use cases are largely small tools designed to augment human workflows; more autonomous tools remain speculative.
But what makes LLMs ill-suited for autonomous fact checking? For one, these models struggle with lower-resourced languages and areas where they’ve seen less training data, making them ill-suited to global platforms or flows of information. In particular, they 
don’t perform well in African languages
, and they 
seem to know less about African countries
. This limitation echoes the exact problem that led Meta to establish its fact-checking program to begin with: without deep local knowledge and language understanding, content moderation cannot effectively prevent the spread of 
dangerous misinformation and hate speech
.
Figure from 
Moayeri et al (2024)
 showing the average error in responses about each country from 20 leading LLMs. LLMs are much more accurate for some parts of the world (e.g., the US and parts of Europe) than others, especially African countries.
Second, LLMs introduce 
new risks through their tendency to hallucinate
. For fact-checkers, whose credibility depends on absolute accuracy, this is particularly problematic. Research 
published in the Proceedings of the National Academy of Sciences
 demonstrates how LLM errors can backfire: When models make mistakes in fact-checking, they can actually decrease trust in accurate information while increasing belief in misinformation.
These challenges mean that effective LLM-based fact-checking requires extensive human oversight and carefully crafted prompts (VERDAD’s prompt, for example, spans 8,500 words). This level of human intervention raises a crucial question: do these systems actually reduce the verification burden, or do they simply shift it to different parts of the process?
The path forward likely involves rethinking both the scope and deployment of AI fact-checking systems. While fully automated verification remains out of reach for complex, high-stakes scenarios, there may be specific contexts where current LLMs could prove valuable–particularly for already-documented misinformation or content in well-resourced languages. Some fact-checking professionals 
are already exploring this territory
, prototyping for use cases like live fact-checking and quality assurance — workflows that give LLMs more autonomy while maintaining human oversight for sensitive cases.
But even if we develop more capable models, we face a separate challenge: If major platforms are stepping back from centralized fact-checking, where do we deploy these systems? This constraint might actually push us toward more innovative, distributed solutions–like client-side interventions where users opt into browser extensions or overlays that flag potential misinformation and surface relevant sources. As local language models become more sophisticated, we might even see on-device verification tools that help users evaluate information independently.
The rise of decentralized social media platforms points to another possibility. Networks like Bluesky and Mastodon are built on protocols that enable greater user control over content moderation and recommendations. Bluesky, for example, allows any person, organization, or tool to label content, and it lets users decide what to do with those labels. In this 
“stackable” approach to moderation
, fact-checking systems wouldn’t need platform approval to be effective — users could simply choose to incorporate verification into their feed settings, using misinformation labels to downrank or flag questionable posts.
Meta’s retreat from centralized fact-checking may signal the end of an era, but it also creates space for reimagining how we verify online information. While current LLMs can’t fully automate this critical task, they could become powerful tools in a more distributed approach to fighting misinformation. The future may not lie in platform-mandated verification, but in empowering users with a combination of AI-enhanced tools, decentralized networks, and human expertise. As our information ecosystem fragments and evolves, the most resilient solutions may be those that put verification tools directly in the hands of users.]]></content:encoded>
</item><item>
<title><![CDATA[Transforming Longform Text into Multimedia using Generative AI]]></title>
<link>https://generative-ai-newsroom.com/transforming-longform-text-into-multimedia-using-generative-ai-5cc50fa9b460?source=collection_home_page----df04cefca135-----16-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[Image Generation]]></category><category><![CDATA[Midjourney]]></category><category><![CDATA[Storytelling]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Jaemark Tordecilla]]></dc:creator>
<pubDate>2025-02-05</pubDate>
<atom:updated>2025-02-05</atom:updated>
<content:encoded><![CDATA[Transforming Longform Text into Multimedia using Generative AI
How gen AI tools can be used to translate long articles into more accessible multimedia presentations
Jaemark Tordecilla
Follow
11 min read
·
Feb 5, 2025
--
1
Listen
Share
Image generated using Midjourney
Like many readers, I was deeply touched by Columbia University investigative journalism professor Sheila Coronel’s story “
The Making of Edgar Matobato
.” Published as a longform piece by the Philippine Center for Investigative Journalism (PCIJ), the story featured a powerful narrative and deep reflection on the confessions of a hired killer and the Philippines’ recent history of drug-related killings.
I felt that it was a story that needed to reach more people. I wondered if there was a way to transform the story into other formats like video and audio, to give it a chance to find audiences on platforms where they consume content and information.
With Sheila’s permission, I experimented with generative AI to create multimedia elements to transform the story into this video. I used AI tools to generate images and turn photos into illustrations; create voice-overs for the story; turn the images into animation; and create haunting music for the soundtrack.
This post recounts how I did it.
Picking voices and generating voice-overs
For the voice-overs, I chose to use Eleven Labs, a company that specializes in AI voice generation. The service has a 
free tier
 but access to different voices is limited to default options. For this project, I chose to subscribe to the Creator tier. It costs $11 for the first month and $22 for succeeding months. The paid tier allows a wider range of voice options for the project, including those with the appropriate accents for the story. For the main narrative of the Matobato story, I went with the Francesca voice, described as a “young female voice with a hint of mild Filipino-American accent.” It also cost twice the credits compared to the default available voices.
A DESCRIPTION OF THE FRANCESCA VOICE FROM ELEVEN LABS.
I picked a range of other voices, all with Filipino accents, for various other characters in the story.
The paid tier allows users to create Projects, which is used for longer form initiatives such as podcasts and audio books. This allowed me to divide the story into chunks before converting them into audio. It also had an easy interface to assign particular voices into lines from the story. For example, this part of the story had three different voices: the narrator, Edgar Matobato, and Matobato’s wife.
A SNIPPET FROM THE ELEVEN LABS PROJECT SHOWING DIFFERENT VOICES ASSIGNED TO DIFFERENT LINES.
The biggest challenge in generating the voice-overs is getting the synthetic voices to pronounce some words, mostly proper nouns, correctly. Getting around this takes listening to the generated voice and adjusting the spellings until the AI gets it right.
For example, I had to change the spelling of Samar province to “Sam-mar,” the surname of the radio broadcaster Ted Failon to “Fai-lohn,” and the name of the Waray people to “Wah-rye” to get the AI to approximate a correct pronunciation of those words.
A SNIPPET FROM ELEVEN LABS SHOWING ADJUSTMENTS TO THE SPELLING OF PROPER NAMES.
Sometimes, too, the system would just decide to pronounce the same words differently. In one snippet, the AI was pronouncing Matobato differently from the rest of the story. I had to change the spelling of the name (Ma-to-ba-to) for just that snippet to make it similar to the rest of the audio.
A SNIPPET FROM ELEVEN LABS SHOWING A TWEAK TO THE SPELLING OF MATOBATO’S NAME.
For the project, I used the Eleven Turbo 2.5 model, an older model that was about half as expensive as their cutting edge Multilingual 2.5 model.
With all the trial and error, generating the audio from the original 4,000-word piece cost around 50,000 credits, or half of the monthly allocation for the Creator tier of the Eleven Labs subscription. The total length of all the voice-overs was about 33 minutes.
Finding an Image Style
For the visuals, I wanted to use a style that would evoke the story’s original narrative while making clear to viewers that these were merely symbolic imagery. After some trials, I settled on the style of ink sketches with watercolor streaks. This is done by adding an instruction like “in the style of a coloring book, clean lines, black and white ink, red watercolor streaks” to the end of every prompt.
AN IMAGE FROM MIDJOURNEY WITH THE PROMPT “A PHOTO OF LONE MAN SITTING AT A SMALL WOODEN TABLE LATE AT NIGHT, A GUN RESTING NEAR A HALF-EMPTY BOTTLE, HIS HEAD BOWED IN ANGUISH.” BELOW: THE SAME PROMPT, WITH THE ADDITIONAL INSTRUCTION “IN THE STYLE OF A COLORING BOOK, CLEAN LINES, BLACK AND WHITE INK, RED WATERCOLOR STREAKS.”
I ended up choosing 
Midjourney
, which costs $10 for a monthly subscription, for a couple of reasons. First, it consistently generated better looking images compared to ChatGPT’s Dall-E 3 or Google Gemini’s Imagen 3.
IN ORDER, IMAGES GENERATED BY CHATGPT’S DALL-E 3, GOOGLE GEMINI’S IMAGEN 3, AND MIDJOURNEY FOR THE PROMPT: “A SECLUDED CATHOLIC COMPOUND ON A QUIET CHRISTMAS MORNING, SURROUNDED BY SWAYING FRUIT TREES, BIRDSONG, AND A SOFTLY MURMURING STREAM. A MODEST COTTAGE IS TUCKED AWAY IN THE BACKGROUND. IN THE FOREGROUND, A STOCKY, SHORT MAN IN CHECKERED BERMUDA SHORTS AND A GRAY T-SHIRT GREETS VISITORS WARMLY, HIS DEMEANOR CALM AND POLITE. THE SCENE IS STEEPED IN A TRANQUIL YET GUARDED ATMOSPHERE. CLEAN LINES, BLACK AND WHITE INK, GREEN WATERCOLOR STREAKS.”
The second reason is that because of the sometimes graphic nature of scenes from the story — a piece about a hitman detailing his experiences of killing people — guardrails built into the OpenAI and Google chatbots would occasionally flag the image prompts for content violations, a reminder that journalists sometimes need to talk about subjects that can run afoul of big tech moderation standards.
CHATGPT’S RESPONSE WHEN ASKED TO GENERATE AN IMAGE BASED ON A SCENE FROM THE PIECE.
Generating Images Through Meta-Prompting
While I didn’t use the chatbots to generate images, they were still useful in generating better prompts to use on Midjourney. With a technique called 
meta-prompting
, I used snippets of the original story itself to generate structured, detailed prompts that describe vivid scenes. The chatbot was able to identify crucial details like characters, settings, and symbolic imagery, while abstract concepts such as fear, secrecy, or tranquility, are translated into visual elements like lighting, shadows, and posture. With meta-prompting, I was able to create prompts with enough detail to effectively capture both the emotional tone and thematic essence of each scene.
In this case, my instructions to the chatbot were simple enough: “I am working on a project to turn an article into a video. I will give you snippets of the article, and I want you to return with prompts for a Midjourney image. Keep it short and succinct. Make the prompts symbolic instead of literal, for example, a close-up shot of a revolver on the ground. Give me three prompts each time.” I would then paste a snippet from the article in succeeding chats, and it would return sample prompts for me to check, revise, and use in Midjourney.
CHATGPT RETURNED THREE POSSIBLE PROMPTS FOR USE WITH MIDJOURNEY FROM A SNIPPET OF THE STORY. SOMETIMES, JUST PUTTING DETAILS FROM THE STORY INTO THE CHAT RAISES A CONTENT FLAG WITH CHATGPT.
A MIDJOURNEY IMAGE GENERATED FROM A PROMPT CREATED BY CHATGPT USING META-PROMPTING, MODIFIED SLIGHTLY: “A BARREN QUARRY AT DUSK, WITH SHALLOW GRAVES SCATTERED ACROSS THE GROUND, FAINT SILHOUETTES OF SHOVELS AND WORKERS DIGGING IN THE DISTANCE. IN THE STYLE OF A COLORING BOOK, CLEAN LINES, BLACK AND WHITE INK, RED WATERCOLOR STREAKS.”
Editing and Adjusting Images
Another useful feature in Midjourney is its editor, which allows you to modify elements of the initial image using generative fill.
ADJUSTING AN IMAGE USING MIDJOURNEY’S EDITOR.
That allowed me to adjust certain things that did not look right. In this scene, I was able to adjust the look of the crucifix in a key scene inside a church to look more appropriate.
ORIGINAL AND TWEAKED IMAGES FROM MIDJOURNEY. NOTICE THE SHAPE OF THE CRUCIFIX.
This image, meanwhile, had multiple tweaks: I got rid of a person who was not supposed to be part of the scene, I modified the look of the head of the priest, and I changed the position of the legs of the main character to remove his squat and make him kneel, as described in the story.
ORIGINAL AND TWEAKED IMAGES FROM MIDJOURNEY.
Turning Photos into Line Art
The original piece featured photos of Matobato by Vincent Go, along with a photo of an old ID of Matobato. I wanted to include these in my video, but I didn’t want it to clash with the visuals that I had already generated. To make the photos more compatible, I used an 
image processor on Hugging Face
 to turn the photos into line art.
TURNING PHOTOS INTO LINE ART. ORIGINAL PHOTO OF MATOBATO BY VINCENT GO.
PHOTO OF MATOBATO ID SOURCED BY SHEILA CORONEL.
Using editing and animation techniques described below, I was able to use both photo and line art versions of these images for the video.
Animating the Images and Editing the Video
I explored the use of generative video services to turn the images into animation. However, none of the major generative video platforms — 
Sora
, 
Luma Dream Machine
, and 
Runway ML
 — generated useful video from the images I generated. They would usually create something that would not be usable for me as a producer because they don’t make sense, or add extra elements that were not in the original images, or just bad aesthetically.
VIDEO GENERATED USING LUMA DREAM MACHINE
VIDEO GENERATED USING SORA
VIDEO GENERATED USING RUNWAY ML
About one in five videos generated by the platform were useful for me as a producer using these videos to build the story, like this video that added a camera zoom effect to the original.
VIDEO GENERATED USING LUMA DREAM MACHINE
Considering these were about five seconds long each, it would take so much effort to come up with enough material to support a narrative that’s longer than 30 minutes. There is a reason why all those demo reels of AI generated videos on YouTube are generally just a couple of minutes long.
Using text-to-image prompts with these video generators didn’t work either, because they couldn’t capture the art style from the Midjourney images, nor were the animations compelling.
VIDEO GENERATED USING SORA
While looking for options, I stumbled upon 
YouTube videos of a speed painting animation app
 that used to be popular on Canva. It was no longer available on Canva when I tried it, but there was a 
website for the same app, Speedpaint.co
, that was available for a $9 monthly subscription.
To use it, you simply need to upload your image and play around with the settings: the sketching duration, the color fill duration, the sketch type (color, grayscale, black and white, or neon blue), and the sequence of the sketch.
THE SPEEDPAINT.CO INTERFACE.
Hitting animate would result in a video after a couple of minutes. The result was perfect for what I was trying to do.
VIDEO ANIMATED USING SPEEDPAINT.CO
The animations generated by the app were about 30 seconds each. For the final video, I slowed this down to ⅓ speed, which meant that each animation could last 90 seconds. If the audio segment was still longer than that, I simply used a freeze frame and applied the 
Ken Burns effect
 to extend the video further.
APPLYING THE KEN BURNS EFFECT.
The speed paint animation was useful not just for the generated images, but also for the line art version of the original photos. By using a simple cross dissolve, it became an easy way to integrate original photos into the video.
VIDEO ANIMATED USING SPEEDPAINT.CO
Generating a Soundtrack
For the soundtrack, I used 
Suno
 to generate simple piano music with the prompt: “Background piano music for a narrative podcast, slow, somber, intimate, looping, spare.”
I found the first couple of tracks generated by the app a bit 
too fast
 and 
cloying
, so they did not really work for this project. The 
track I ended up choosing
 was somber and intimate enough, though it did not loop. I simply used fade in and fade out effects to the track to be able to use it over and over again in the video.
The Human in the Loop
I completed the work, alone, in two days. Knowing what I know now, it probably would take me just a day to complete it. Working with people for these media elements — artists, illustrators, animators, voice-over talents, musicians — would’ve taken many more days, if not weeks.
While I used AI as a tool, the control remained with me as the producer throughout the process. I made all the creative choices, from curating the images to choosing the voice models to the editing, which was still fully manual and was thus the most tedious part.
I used DaVinci Resolve to edit the video, though I could just as easily have used a more beginner-friendly editing program like CapCut or Microsoft Clipchamp to put it together. A more experienced video editor would probably have been able to complete the project much faster and frankly, the video would probably be much better.
How People Reacted
I sent Sheila (the author of the original article) the video as soon as I finished it. “I was moved watching this rendition of my piece with art and music, providing it with more emotional depth compared to just text. I hope this version reaches audiences that would otherwise not read a 4,000-word story. We need more experiments like this,” she said.
She was surprised by the quality of the voices, as were journalism colleagues from the Philippines who watched the video. They were struck by the slight hint of a Filipino accent in the main voiceover, which they felt added to its human quality.
Interestingly, colleagues from outside the Philippines weren’t too taken by the voices. “Too monotonous and puts me to sleep,” one said. Others, meanwhile, suggested playing the video at 1.25x speed, which they said made it better, but still not as good as human.
Many of my colleagues were stunned by the quality of the images from Midjourney, which they felt really added to the video. But there was some pushback from followers of the PCIJ, who said they were disappointed in the use of generative AI images, raising the ethical concerns of these platforms over their unauthorized use of artists’ work.
For his part, artist Joseph Luigi Almuena, who has animated PCIJ’s investigative reports, said 
the first thing he would ask is
: What is the AI image for? And who is it for? Will it serve the public good?
I think this effort shows the potential to do just that, by turning a narrative text into a format that could introduce an important story to new audiences.
Broadcast journalist Jessica Soho, a longtime news and documentary production executive who anchors the 
most popular television program in the Philippines
, said the final product sounded more like a podcast rather than a narrative video. Still, she sees the potential for using these technologies for newsrooms looking to broaden their audiences.
Indeed, media outlets and even individual journalists could pick and choose which elements here would be appropriate for them, depending on their resources, their strengths, and the attitudes of their audiences. Some could use all of it. Others could use generative AI images with human voice-overs for maximum impact. Some could find uses for synthetic voices to turn their longform narratives into podcasts. Still, others could simply use the animation techniques here to turn their photos into video material.
With this experiment, I hope I was able to show journalists, producers, and newsrooms around the world how these various generative AI tools could be building blocks to transform written news articles and help them tell compelling stories in new ways.]]></content:encoded>
</item><item>
<title><![CDATA[Building Privacy-First Text Analysis Tools for Newsrooms]]></title>
<link>https://generative-ai-newsroom.com/building-privacy-first-text-analysis-tools-for-newsrooms-5c458e7dc698?source=collection_home_page----df04cefca135-----17-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Sql]]></category><category><![CDATA[Duckdb]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2025-02-03</pubDate>
<atom:updated>2025-02-03</atom:updated>
<content:encoded><![CDATA[Building Privacy-First Text Analysis Tools for Newsrooms
Combining DuckDB, SQL, and local LLMs for efficient and secure document analysis
Nick Hagar
Follow
7 min read
·
Feb 3, 2025
--
1
Listen
Share
For computational journalists working with large text datasets, integrating language models into SQL (Structured Query Language) workflows can streamline analysis and content processing. Whether 
categorizing thousands of public comments
, 
simplifying technical documents
, or 
identifying newsworthy patterns in public records
, combining SQL’s data manipulation capabilities with LLMs’ text processing powers eliminates the need to constantly switch between tools or manually process data.
The typical approach to using LLMs — copying data into chat interfaces or stitching together API calls — creates friction in the analytical workflow. Every time a journalist needs to apply an LLM to their data, they must extract records from their database, format them for the LLM, process the results, and then reintegrate those results back into their analysis. This context-switching is inefficient and introduces opportunities for errors.
This post demonstrates a more elegant solution: extending SQL itself to incorporate LLM processing directly into database queries. Using User Defined Functions (UDFs), journalists can process text with language models right alongside their standard SQL operations, rather than bouncing between tools.
Better yet, we can implement this entire workflow locally on a laptop, without relying on cloud services or third-party providers. This local approach offers several key advantages: It keeps costs down, maintains data privacy for sensitive stories, and prevents vendor lock-in by keeping projects portable. While there’s a rich ecosystem of computational tools for local data work, we’ll focus specifically on combining SQL databases with LLMs to create a powerful, private, and cost-effective text processing pipeline.
Let’s start with DuckDB, an efficient database system that runs entirely locally and integrates well with Python. From there, we’ll see how UDFs allow us to extend DuckDB’s capabilities to incorporate language model processing directly into database queries.
SQL and DuckDB
SQL
 is the standard language for working with databases. Think of it as a specialized language for asking questions about structured data: “Show me all restaurants that failed health inspections this year” or “Calculate the average response time for emergency calls by neighborhood.” SQL’s power lies in its ability to express complex data operations in readable, declarative statements.
DuckDB
 is a modern database system designed for analytical workflows. Unlike traditional databases that focus on storing and updating records (like tracking bank transactions or managing inventory), DuckDB excels at processing large amounts of data for analysis. It’s particularly well-suited for journalists because:
It runs entirely on your laptop without needing to set up servers or cloud services
It’s extremely fast, capable of processing millions of records in seconds
It integrates with Python, the programming language many journalists use for data analysis
It can read common data formats like CSV files, Excel spreadsheets, and Parquet files
Extending SQL with Custom Functions
While SQL provides many built-in functions for data analysis (like COUNT, SUM, or AVERAGE), there are often cases where we need more specialized operations. Perhaps we need to apply a complex calculation, integrate with an external API, or implement domain-specific logic that isn’t built into SQL. This is where 
User Defined Functions (UDFs)
 come in.
UDFs allow us to extend SQL’s capabilities with our own custom code. Once defined, a UDF appears to SQL just like any other built-in function. For example, we might create a function to calculate the reading level of text:
Or a function to standardize inconsistent location data:
In DuckDB, we can write these UDFs in Python, which gives us access to Python’s rich ecosystem of libraries and tools. The process is straightforward:
Write a Python function that performs the desired operation
Register that function with DuckDB
Use the function in SQL queries, just like any built-in function
Here’s a simple example that calculates the reading time of text in minutes:
Now we can use this function in our queries:
By extending SQL with custom functions, we can integrate any Python code–from simple calculations to complex machine learning models–directly into database workflows. And this approach also allows us to use LLMs to incorporate AI-powered text processing into our SQL queries.
Setting Up a Local Language Model
Now that we understand how to extend SQL with custom functions, let’s set up a language model that we can integrate into our queries. While there are many cloud-based LLMs available (like GPT-4 or Claude), running a model locally aligns with our goal of maintaining a self-contained analysis environment.
One of the simplest ways to run LLMs locally is through 
LM Studio
, a desktop application that serves as a bridge between open-source language models and your local code.
Here’s how to get started:
Install LM Studio
: Download and install the application from their 
website
.
Download a Model
:
Open LM Studio and navigate to the “Discover” tab
Search for “Llama-3.2–3B” (a smaller but capable model good for testing)
Click download to save it to your computer
Start the Local Server
:
Go to the “Developer” tab
Click “Start Server” to launch the API endpoint
Select your downloaded Llama model from the dropdown
At this point, you’ve set up your own AI service that can be accessed through a standard API endpoint at 
http://localhost:1234/v1.
 This endpoint mimics the OpenAI API format, which means we can use the same code structure whether we’re calling GPT-4 in the cloud or Llama-3.2 on our laptop.
The key difference? When you query this endpoint, your data never leaves your computer. This makes it particularly valuable for journalists working with sensitive information or organizations with strict data privacy requirements.
Implementing the LLM Function
With our local language model running and our understanding of UDFs, we can now create a function that connects them together. Our goal is to write a Python function that can:
Take text input from our SQL queries
Send it to our language model (whether local or cloud-based)
Return the model’s response back to SQL
Here’s the implementation:
Now we can register this function with DuckDB:
With this function registered, we can now use it directly in SQL queries. For example, to analyze the sentiment of customer feedback:
While this implementation targets local models, it can support cloud-based frontier models with minimal changes. Since our Python function already uses the OpenAI client library, we can add an API key to query GPT models. The standard tradeoffs of cloud-based models apply: although frontier models offer superior capabilities, they incur higher inference costs (particularly when querying large databases) and are subject to rate limits.
Practical Applications for Newsrooms
The ability to call language models directly from SQL queries opens up powerful workflows for computational journalism. Here are three key areas where this approach proves particularly valuable:
Document Analysis and Filtering
For journalists dealing with large document sets — FOIA responses, public comments, leaked documents — LLMs can help quickly identify relevant content. Consider this query that analyzes public comments on a proposed regulation, as in 
this dataset
 from the proposed repeal of net neutrality:
This not only helps categorize comments but can surface patterns in public response–like which policy aspects generated the most opposition or which arguments were most common among supporters.
Technical Translation
Journalists often need to make complex technical documents accessible to general audiences. Consider analyzing academic research papers, such as this 
sample of preprints from arXiv
:
This same approach works for financial reports, legal documents, or technical specifications–helping journalists quickly grasp key points and translate them for their audience.
Finding News Leads
LLMs can help identify potentially newsworthy items in routine government data releases. Here’s an example using 
restaurant inspection data from the City of Chicago
:
This helps prioritize which violations warrant further investigation, particularly useful for newsrooms that can’t manually review every inspection report.
Iterative Analysis
The real power of this approach comes from combining multiple analyses. For instance, you might first use an LLM to identify relevant documents, then analyze their content in more detail:
Limitations
While combining SQL and LLMs offers powerful capabilities for computational journalism, several key limitations must be considered.
On the technical side, these workflows face challenges with processing speed and resource requirements, particularly when handling large datasets. Because local LLMs require substantial computing power, most consumer machines can only run one instance at a time, forcing documents to be processed sequentially rather than in parallel. This creates a performance bottleneck that grows with the size of the dataset.
The reliability of LLMs themselves presents additional challenges–they can produce inconsistent outputs, deviate from requested formats, and occasionally generate plausible but incorrect information (hallucination). From a journalistic perspective, the “black box” nature of LLM analysis can make methodology difficult to explain transparently, and there are concerns about potential bias in model outputs and challenges with reproducibility across different model versions. While these limitations can be partially mitigated through careful workflow design–such as using LLMs primarily for initial triage, implementing caching, and incorporating human review–they remain important considerations for journalistic applications.
Conclusion
By combining the precise data handling of SQL with the flexible text processing of LLMs, all running locally on a journalist’s machine, we create a powerful environment for investigating complex stories. Whether surfacing relevant documents from large datasets, making technical content more accessible, or identifying patterns that merit deeper investigation, this approach helps newsrooms work more efficiently while maintaining control over their tools and data.]]></content:encoded>
</item><item>
<title><![CDATA[Is this the Beginning of the End for AI-Summarized News?]]></title>
<link>https://generative-ai-newsroom.com/is-this-the-beginning-of-the-end-for-ai-summarized-news-196432290e9e?source=collection_home_page----df04cefca135-----18-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[AI]]></category><category><![CDATA[Apple]]></category>
<dc:creator><![CDATA[Clare Spencer]]></dc:creator>
<pubDate>2025-01-28</pubDate>
<atom:updated>2025-01-28</atom:updated>
<content:encoded><![CDATA[Is this the Beginning of the End for AI-Summarized News?
Apple has withdrawn a feature which summarized the news after it published multiple inaccuracies. What’s next for AI-generated news summaries?
Clare Spencer
Follow
8 min read
·
Jan 28, 2025
--
Listen
Share
A BBC news story included a comparison of the original BBC News notification (top) and the incorrect summary of that notification sent by Apple Intelligence.
“AI for the rest of us” is the strapline for Apple Intelligence — a bunch of features for newer Apple devices launched in late 2024. One problem Apple appeared to be trying to address was to reduce an overwhelming number of notifications. They were now going to help you “weed out distractions and find the signal in the noise”, tech reviewer Allison Johnson
 wrote in The Verge
. The tech could ostensibly summarize phone messages and bunch them together. So you’d get one sentence summing up a whole WhatsApp chat, or your Ring Doorbell notifications. They also created summaries from news apps’ notifications.
What could possibly go wrong? This article recounts the early and obvious failings of the feature when applied to news headlines, and then asks whether news organizations could navigate a more effective implementation of LLMs for news summarization.
From Obvious Early Problems to Failure
Apple started giving developers access to Apple Intelligence in August through to October 2024. But by October 25
 Andrew Schmidt tweeted
 an early sign of trouble ahead. His mom had sent him a message joking that a hike had almost killed her but Apple Intelligence summarized it as: “Attempted suicide, but recovered and hiked in Redlands and Palm Springs.”
Regardless of such clangers, on October 28, Apple released the beta version to the public in the US on iPhone 16, 15 Pro and 15 Pro Max handsets, as well as on some iPads and Macs. At this point you still had to join a waitlist and opt in.
Journalists were quick to point out how this could all go awry. “I look forward to everyone sharing the bonkers/pointless summaries it now puts on your lock screen”, wrote the Washington Post’s tech columnist Geoffrey Fowler
 on Bluesky
 that day. In his
 review for that newspaper
 he explained he’d been testing Apple Intelligence on his iPhone for months and noticed it “doesn’t act intelligent at all” because the notification summary feature “goes bananas at least once or twice per day”.
“Sometimes the AI mistakes border on misinformation,” Fowler warned. “One example: Last Thursday [October 23], Apple AI summarized a news headline as, ‘Steve Anderson urges Harris to endorse Harris.’ (The actual 
original headline
 was, ‘Fellow General Steve Anderson Tells John Kelly Why He Must Endorse Harris Now.’)”. Fowler continued: “On Tuesday [29 October], Apple Intelligence incorrectly summarized a Washington Post news alert to say: ‘Harris rally features Elon Musk and Jeff Bezos.’ (That didn’t happen. And Bezos owns The Post.)” 
He added on X
 that: “It’s hard to believe that Apple released a product that’s doing this over and over again”.
That dismay was shared by others. “Apple AI notification summaries continue to be so, so, so, bad,” Propublica’s data journalism editor 
Ken Schwencke complained on Bluesky
 on November 21. He shared a screengrab that read “Netanyahu arrested” next to a New York Times logo. A flagrant error. The Israeli Prime Minister Benjamin Netanyahu had of course not been arrested. The International Criminal Court had merely issued an arrest warrant for him, accusing him of war crimes and crimes against humanity in Gaza, the
 New York Times reported
 at the time.
A Reddit forum called ‘Apple Intelligence Fails
’ collects inaccurate summaries. On December 10, a user named “dull pedestrian”
 shared a notification
 with the BBC logo next to it, which said that Luigi Mangione had shot himself outside court. But this did not happen. In reality, the BBC reported that Mr.
 
Mangione was charged with the murder of UnitedHealthcare CEO Brian Thompson, and that “on his way into court,
 Mr Mangione could be heard shouting
 ‘completely unjust’ and ‘an insult to the intelligence of the American people’”. The BBC also reported that investigators said Mr. Mangione had had a 
“
ghost gun — a firearm that can be assembled at home”.
Despite early warnings Apple’s global roll-out of Apple Intelligence continued and was released in the UK on December 11. Two days later, on December 13, a
 BBC News story
 reported that the BBC had complained to Apple about incorrect summarization of its stories about Mr. Mangione.
Other organizations added their voices to the chorus of discontent. On December 17 Vincent Berthier from Reporters Without Borders (RSF) publicly asked Apple to remove news summaries, accusing Apple of “threatening reliable journalism”. “The probabilistic way in which AI systems operate automatically disqualifies them as a reliable technology for news media that can be used in solutions aimed at the general public,”
 he said in the statement on RSF’s site
.
The UK broadcaster 
Sky News reports
 that, two days later, Apple Intelligence wrongly summarized their story from December 19 to say French abuse survivor Gisele Pelicot “defended her convictions”. In fact, she had not been convicted but instead was a victim of rape by more than 50 men. Sky’s story quoted her as saying that she had gone to court
 “for my children and grandchildren
”
Next, the New Year came around, and new inaccuracies appeared on phones.
The BBC spotted another case on January 2 where Apple Intelligence falsely summarized that darts player Luke Littler had won the PDC World Championship. That alert was displayed next to the BBC News logo. Yet at that point Littler hadn’t even played in the final.
The next day, the BBC spotted that Apple Intelligence had published that “Brazilian tennis player, Rafael Nadal, comes out as gay,” next to the BBC Sport App. Nadal, who is Spanish and married to a woman, hadn’t. It appeared Apple Intelligence had inaccurately summarized 
a BBC Sport story about Brazilian tennis player Joao Lucan Reis da Silva
 as “the first active professional men’s player to announce to the world he is gay”.
On January 6, the BBC reported that Apple had told them they would make a change to its AI offering — but only to the labelling of summaries, not to the summaries themselves. “A software update in the coming weeks will further clarify when the text being displayed is summarization provided by Apple Intelligence,” 
BBC quoted an Apple statement as saying
.
The inaccuracies continued.
Sky News reported
 that they had spotted that Apple Intelligence had incorrectly summarized a “sensitive and controversial” Sky news story. The incorrect summary said that British safeguarding minister Jess Phillips called for a new inquiry into Britain’s grooming gangs. The original headline, from January 8, was: “
Jess Phillips tells Sky News there could be a new national inquiry
 into grooming gangs if victims ask for one”. Their story added that Sky News had also complained directly to Apple.
Tech columnist Fowler’s bemused tone had turned to anger by January 15. He accused Apple of being “wildly irresponsible” for not turning off summaries for news apps. 
He shared on Bluesky
 a summary of Washington Post notifications that he described as “so bad it got every fact wrong”.
I put to Apple the accusation that they had been spreading misinformation. They did not respond.
But on January 16, 
the BBC reported
, Apple began withdrawing the news summary feature. They seemed to finally be getting an accurate notification themselves: Apple Intelligence just wasn’t working for news.
Making GenAI Summarization Work at News Organizations
So, have Apple’s errors led to news organizations scrapping their own AI-generated summaries? Not as far as I can see. 
Research
 by Nic Newman and Federica Cherubini at the Reuters Institute suggests that AI summaries at the top of articles are likely to become more widespread. Fully 70% of respondents to their survey of 326 newsroom leaders from 51 countries expect to provide AI summaries at the top of stories in 2025.
Before getting into it, it’s worth pointing out that we aren’t comparing apples with apples (sorry, I couldn’t resist the pun). Apple is working at a much larger scale than what I’ve seen from news organizations. After reading 
Apple’s technical explanation 
they released in July, 2024 it seems clear that they used their own large language model. For added privacy, Apple Foundation Model (AFM) works directly on your device. It seems likely that Apple was not checking their summaries before publishing them, whereas the news organizations who opened up to me were getting someone to check.
Perhaps one of the first news organizations to use AI to generate summaries is 
still doing it today
. Back in 2022, South Africa’s news site the Daily Maverick started experimenting with using OpenAI’s large language model GPT-3 to summarize their articles.
The Daily Maverick has a section of its website where the audience can read summaries of some of its biggest stories.
Daily Maverick software engineer Jason Norwood-Young 
observed
: “While GPT-3 was astonishingly good, it was also get-yourself-sued bad.” So he added a layer of human checking into the process. He created an interface where editors reviewed the summaries and accepted, rejected, or edited them before they were published.
The Daily Maverick’s developers have since replaced GPT-3 with a newer model from OpenAI (GPT-4o) and, Norwood-Young told me, 
humans still check all the summaries today
. His analysis showed Daily Maverick editors rejected about 6% of summaries they viewed. “Most of the time they just approve the summary, so it’s just a matter of giving it a quick read,” he explained.
News organizations seem to have largely followed the Daily Maverick’s example of including a human check before publication. For example, the UK newspaper publisher National World started using AI late last year in eight of their printed regional newspapers. They use Google’s large-language model Gemini to generate summaries. Likewise, a member of the team always checks the summaries for accuracy before they are printed, National World’s publisher of the daily press division Tim Robinson told me.
He said summaries are great for so-called “super scanners” who skim read a page and he had no plan to scrap them.
National World’s Portsmouth newspaper The News publishes a summary of the letters page under the Talking Points section and labels it as “summarised with Gemini AI”
I put it to Robinson that Gemini is not designed to be able to select the most salient points. He agreed that that is an issue with all large language models. But he was insistent that AI takes away the “agonizing” over what goes in the summary and said he is confident that it is quicker than a reporter writing from scratch.
I have not evaluated either the Daily Maverick or National World’s summaries, so I can’t comment on their level of accuracy. But I also haven’t seen the outpouring of outrage for either of these publications that we witnessed for Apple Intelligence’s news summaries.
News organizations “seem to me to be far, far less naïve than Apple about AI’s ability to handle ‘must-be-accurate’ information,” AI consultant (and my former colleague at BBC News Labs) David Caswell told me.
Caswell doesn’t see Apple’s mistakes as a big deal in the long run “other than to indicate that Apple is not very competent when it comes to using AI for news”. He compares it to the 
mistakes and inaccuracies the IT news site CNET published using generative AI 
back in 2023, or Google’s first launch of AI overviews. “Both orgs pulled the defective product, did some tweaks and then doubled down”.
That’s the direction Apple says it is going in too. They are working on improvements to news summaries and have promised they will make them available in a future software update.
Newman and Cherubini, however, predict that if mistakes like this happen again then someone may sue. “We can also expect to see cases where publishers (or other plaintiffs) seek compensation or apology for reputational damage,” they write in their trend report.
—
If your news organization is using news summarization, we’d love to hear from you 
here
.]]></content:encoded>
</item><item>
<title><![CDATA[Personalizing Newsletter Headlines: Testing LLMs Against Human Editors]]></title>
<link>https://generative-ai-newsroom.com/personalizing-newsletter-headlines-testing-llms-against-human-editors-6f14135089da?source=collection_home_page----df04cefca135-----19-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Llm]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Newsletter]]></category><category><![CDATA[Personalization]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2025-01-21</pubDate>
<atom:updated>2025-01-21</atom:updated>
<content:encoded><![CDATA[Personalizing Newsletter Headlines: Testing LLMs Against Human Editors
A controlled study shows readers prefer AI-generated headlines 62% of the time — while keeping core editorial decisions in human hands
Nick Hagar
Follow
6 min read
·
Jan 21, 2025
--
Listen
Share
Histogram showing how many LLM-generated headlines each participant preferred out of the five newsletter editions tested. On average, participants selected 3.1 out of 5 AI-generated headlines.
While many newsrooms already use algorithms to customize their offerings — think of how the
 New York Times
 or 
The Verge
 adapt their homepages based on reading patterns, for example — LLMs enable a more straightforward approach to personalization. Rather than ranking and recommending coverage as-is, these models can adapt how news is presented to each reader, adjusting context and framing to highlight personal relevance while maintaining the core facts of the story.
This capability presents both exciting opportunities and serious challenges for newsrooms. While 
studies
 have 
shown
 that readers resonate with machine-tailored content in some contexts, news organizations must ensure that personalization doesn’t compromise journalistic integrity. A journalistically-aligned recommendation system cannot distort facts, gate access to information, or generate inaccurate content in pursuit of engagement, but an LLM that re-writes content very much could.
Given these constraints, where can LLMs effectively contribute to news personalization while respecting core editorial values? Our research explores one promising avenue: the personalization of news promotion across distribution channels like email newsletters, social media, and search results. In these environments, an LLM can simultaneously identify which stories will resonate with readers and determine how best to frame them for each context.
Newsletters, in particular, are 
crucial tools for reader engagement
. The challenge lies in choosing which story to feature and how to frame it in a way that encourages readers to open the email and engage with the content.
To understand how LLMs might enhance this process, it’s worth examining how newsrooms currently approach newsletter curation. Take the common format of a daily digest: Each morning or evening, editors compile the day’s most important stories and select one to headline the newsletter. This selection process balances multiple factors — the story’s significance, its potential reader interest, and how well it can be framed in a subject line. The goal is both to inform readers about crucial news and to encourage them to read further.
Example of the New York Times
’
 evening newsletter ‘The Evening,’ showing a journalist-written headline. This format illustrates the standard newsletter structure that was used as a baseline for comparison with LLM-generated personalized headlines in the study.
We wanted to assess the extent to which LLMs could perform this editorial task, in a 
personalized
 fashion. The idea is to evaluate whether, based on someone’s reading history, an LLM could adapt the content for the newsletter in an engaging way. To test this we designed a two-phase study with 100 participants. We recruited participants through the survey platform 
Prolific
, filtering on users who read the 
New York Times
 and live in the United States. Because we didn’t have actual reading history data, in the first phase, we presented participants with a curated reading list: 100 
New York Times
 headlines spanning various sections and topics from a three-month period. Participants browsed these headlines and selected the ones that caught their interest as a way to mimic the interest pattern we might capture if we did have reading history data.
Survey interface used in Phase 1 of the study, where participants selected 
New York Times
 headlines that interested them. The interface presented 100 headlines from May-July 2024, with checkboxes allowing participants to indicate their reading preferences. These selections were later used to generate personalized newsletter headlines.
We then used these headline selections as a window into each reader’s preferences. An LLM (GPT-4o-2024–08–06) analyzed their chosen headlines, identifying patterns in topics, themes, and writing styles to build a profile of what engages each reader. This automated analysis served as the foundation for personalizing newsletter content.
For the second phase, we selected five editions of the 
New York Times
 evening newsletter (
The Evening
) and had our LLM system generate personalized subject lines for each one. Participants were then asked to choose whether they preferred these personalized framings to the original journalist-written versions, without knowing which was which.
Want to see how this works? Try our 
interactive demo
 to discover how an LLM would personalize newsletters based on your own reading preferences.
An example of LLM personalization for a user interested in science and technology. Note the error in the day of the week mentioned, however this can be corrected with additional prompting or even rule-based editing.
For example, one participant selected stories that leaned heavily toward U.S. politics (e.g., “
DeSantis Signs Law Deleting Climate Change From Florida Policy
”; “
America’s Military Is Not Prepared for War — or Peace
”; “
Michael Cohen’s Trump Testimony Was Intense. Here Are the Highlights.
”). For the August 1, 2024 edition of 
The Evening
, the original headline read: “Russia Freed Evan Gershkovich in a Major Prisoner Swap. Also, a Hamas leader was killed by a bomb planted months ago. Here’s the latest at the end of Thursday.” Taking into account this reader’s preferences, the LLM generated an alternative version: “Secret Service’s Tech Failures Expose Gaps in Trump’s Shooter Incident. Meanwhile, Democrats advance Harris as their presidential pick. Here’s the latest at the end of Wednesday.” (Notice that the model does not take the day into account, as this is not something we included in its prompt, however this aspect could be easily corrected.) This user selected the personalized version, which highlighted stories aligned with their expressed interest in political coverage.
Flowchart illustrating the two-phase study methodology. Phase 1 (blue) shows the process of building reader profiles from 100 participants selecting headlines from a pool of NYT articles. Phase 2 (pink) demonstrates the headline testing process, where LLM-generated personalized headlines were compared against original versions, resulting in 62% of readers preferring the AI-generated versions.
This is a pattern that held true for most of our participants: readers consistently preferred the personalized headlines, even without knowing they were machine-generated. When given a choice between traditional and LLM-generated headlines, 
participants selected the personalized version 62% of the time
 — a statistically significant preference.
This finding is also consistent across different contexts and audiences. The preference held steady across all five newsletter editions, suggesting that the LLM’s effectiveness wasn’t limited to particular types of news stories or content themes. The preference for LLM-generated newsletter headlines also held across demographics and engagement levels — regardless of participants’ age, sex, ethnicity, or the number of stories they selected in part one of the survey.
Our findings point to a promising middle ground in the ongoing debate about news personalization. Rather than choosing between pure editorial judgment and algorithmic curation, 
LLMs offer a way to enhance how we present carefully selected journalism to different audiences
. When confined to specific tasks — like crafting newsletter headlines from editorially chosen stories — these models can help bridge the gap between newsroom expertise and reader interests. Similar approaches could help newsrooms better serve their audiences across platforms — from social media to mobile alerts to homepage curation — while maintaining editorial control over the core journalism.
Like any initial study in this space, our work has important limitations to consider. First, we focused on a specific context: one language model generating headlines for a single publication’s newsletter format. While the 
New York Times
 provides a strong test case, different newsrooms have distinct editorial voices and audience relationships that might affect how well this approach translates. Our methodology also presents several constraints worth noting. The survey’s reliance on explicit story selection, while providing clean preference data, may not perfectly mirror a user profile based on real-world reading behavior. Readers often engage with news in more complex ways — skimming some stories, reading others deeply, or following specific journalists. A more robust approach might incorporate these nuanced behavioral patterns.
Despite these limitations, our findings point to a future where LLMs could help adapt newsletters at scale through personalization that enhances user preferences. By constraining these models to specific tasks like headline generation and keeping them anchored to journalist-curated content, newsrooms can engage audiences without compromising core editorial values.]]></content:encoded>
</item><item>
<title><![CDATA[The GAIN 2025 Challenge: Evaluation in Context]]></title>
<link>https://generative-ai-newsroom.com/the-gain-2025-challenge-evaluation-in-context-4a688ec2715e?source=collection_home_page----df04cefca135-----20-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2025-01-14</pubDate>
<atom:updated>2025-01-14</atom:updated>
<content:encoded><![CDATA[The GAIN 2025 Challenge: Evaluation in Context
Nick Diakopoulos
Follow
3 min read
·
Jan 14, 2025
--
Listen
Share
Generative AI use in journalism has seen explosive growth since the Generative AI in the Newsroom (GAIN) blog was launched in 2023. Whether it’s producing text, helping to gather or make sense of information, or supporting back-office productivity, there’s a lot of potential. Over the last couple of years, we’ve covered many of these use cases on this blog, from 
using LLMs to structure data from documents for investigation
, 
designing chatbots to support reporters
, 
deploying agents to find stories
, or a 
host
 
of
 
other possibilities
.
But one thing has continued to nag me: 
we still don’t have rigorous evaluation standards for assessing generative AI’s utility in journalism
. Is it speeding up writing, editing, research, interviews, etc.? Increasing engagement? Drawing in new users? Uncovering untold stories? Creating ethical conflict? The LLM industrial complex has benchmarks for evaluating each new state-of-the-art model, but how should journalists evaluate LLMs in the context of news work? How do we know if we’re really moving forward with the technology?
The Challenge
A range of news organizations and journalists routinely use generative AI tools to enhance production. 
Plenty
 
of
 
reports
 have been published on those use cases and possibilities. This is why we thought that this year, on the GAIN blog, we’d like to focus not only on the use cases for generative AI but also
 on how those use cases are evaluated
.
Can you help us? We’d love to hear from you if you work with generative AI in news production! Please 
fill out this form
 with details about your organizational context and use case, evaluation approach, learnings, and ongoing challenges. We’ll keep this open at least through the end of March, 2025. The form asks for the following details:
Use Case Description: 
How are you using generative AI for a specific news production use case? Be as specific as possible in terms of what you are trying to achieve with generative AI, specific tasks it is helping with, who is using the tools, and any other organizational context that is relevant to usage.
Evaluation Approach: 
How did you assess whether generative AI is helping you or your organization for this specific use case? What criteria or metrics do you look at to determine success of the use case and/or acceptable task performance? Are there any specific difficulties with measuring any of these?
Learnings: 
How did generative AI perform? What have you learned and how might that inform future evaluation plans? Are there obvious gaps where it is falling short or areas where it excels? Are there any ongoing challenges or open questions?
Based on your submissions we will select a few to write up their use case and evaluation approach as articles for the blog. GAIN experts will offer feedback on the approach, and there may be possibilities to participate in meetings or workshops. Those invited to write up their approach for the blog will be paid $500 USD for their time and effort in contributing upon acceptance of the final draft.
If you have any questions, please contact 
Nick Diakopoulos
, at nad@northwestern.edu with the subject “GAIN Challenge”. We look forward to working with the community to develop more rigorous and effective approaches to evaluate generative AI for use in journalism. Again, 
here is the form
 you can use to share with us.]]></content:encoded>
</item><item>
<title><![CDATA[The Role of AI-Powered Search Tools in Modern Newsrooms]]></title>
<link>https://generative-ai-newsroom.com/the-role-of-ai-powered-search-tools-in-modern-newsrooms-fb9639785801?source=collection_home_page----df04cefca135-----21-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Search]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Llm Applications]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Michael Crystal]]></dc:creator>
<pubDate>2024-12-10</pubDate>
<atom:updated>2024-12-10</atom:updated>
<content:encoded><![CDATA[The Role of AI-Powered Search Tools in Modern Newsrooms
Michael Crystal
Follow
5 min read
·
Dec 10, 2024
--
Listen
Share
Generated with ChatGPT
The launch of OpenAI’s 
ChatGPT
 Search is the latest real-time, AI-powered web search engine. 
Perplexity AI
 is a startup focusing on delivering succinct answers backed by citations (
Disclosure: 
Perplexity recently gave a gift to support Northwestern’s Knight Lab’s
 research on AI and journalism
). Vector search technologies like 
MISO
 use semantic analysis to deliver results based on contextual understanding rather than keyword matching. They joined the search giants Google and Microsoft, which are also experimenting with generative AI-powered search tools.
These programs don’t just retrieve information; they synthesize it, mimicking human research and analysis. These AI-powered tools have unique strengths. In addition to changing how average users find new information, they can be powerful tools for reporting. Understanding the differences between tools and approaches is essential for newsrooms aiming to make informed choices about when and how to integrate these tools into their workflows.
1. Automated Background Research Packages
Generative AI search tools can help journalists quickly familiarize themselves with complex or unfamiliar topics. Journalists can save significant time by creating research packets filled with related articles, expert commentary, historical context, and references to similar past events while ensuring they approach their stories with a well-rounded understanding of the subject matter. Tools like 
Stanford’s STORM
 prototype take this a step further, automating the collection of references and generating outlines for comprehensive articles.
Example:
 A journalist covering a sudden policy change on environmental protection could instantly generate a packet containing expert quotes, past legislative efforts, and relevant research. With this foundation, they could analyze the policy change’s impact with a depth and context that would otherwise take hours to gather manually [see footnote 1 for examples].
2. Hyper-Personalized Local Impact Analysis
Generative AI could also help national stories resonate locally by analyzing broad topics and adapting them to specific communities. This means that journalists could take a nationwide issue and, with AI assistance, highlight its impact on individual cities, neighborhoods, or even particular demographics within those areas.
Example:
 When new federal healthcare policies are implemented, AI could help a local journalist explore how these changes will specifically affect clinics and patients in their city. This approach would bring a national story down to a personal, relatable level, ensuring that local readers see the relevance of big stories to their own lives.
However, this capability relies on the availability of detailed local reporting within the AI’s data sources. Generative AI may struggle to provide accurate or meaningful localized insights without a robust local news ecosystem. Strengthening local journalism is essential to ensure communities are represented and their needs addressed in broader storytelling.
3. Search-Driven Document Mining for Investigative Reporting
Investigative journalism requires sifting through vast collections of documents, sometimes containing subtle connections or patterns that are easy to overlook. With vector-based search tools like 
NotebookLM
, journalists can quickly identify relationships and trends in massive datasets without combing through each document manually. This empowers investigative teams to find the “needle in the haystack” much faster.
Example:
 A newsroom investigating city budget allocations could use AI to cross-reference thousands of financial documents, finding hidden patterns in spending or linking mentions of certain contractors across reports. This capability enables investigative reporting that would be nearly impossible to achieve manually within typical deadlines.
4. Audience-Driven Investigations and Interactive Reporting
In an era where readers seek deeper engagement, AI-driven search tools could enhance stories by providing interactive layers. By embedding AI-driven search widgets within articles, newsrooms would allow audiences to explore related data or access more in-depth information on their terms. Tools like 
The Washington Post’s Climate Answers chatbot
 demonstrate this potential, offering readers personalized responses to climate-related questions based on the newsroom’s extensive reporting.
Example:
 A long-form investigative piece on climate policy could include an interactive section where readers could click on key terms or data points to explore related articles, environmental data, or historical comparisons. This would enrich the reading experience and empower audiences to dig into the most interesting areas.
5. Automated Contrarian Viewpoint Synthesis
Balanced reporting often involves presenting multiple perspectives, but finding credible counterarguments could be challenging. Generative AI could surface well-sourced alternative viewpoints, ensuring that stories offer a more comprehensive, nuanced view.
Example:
 In covering an urban development project, AI could generate counterpoints from community activists and urban planners concerned about gentrification, enabling the story to showcase diverse opinions and insights. This balanced approach deepens the narrative, making it more informative and inclusive.
While the integration of generative AI search tools heralds a new era for journalism, it brings complications. The very features that make these tools appealing — speed, automated insights, vast data processing — could also introduce significant drawbacks.
Accuracy Concerns
Large language models (LLMs) powering tools like ChatGPT and Perplexity could produce results that appear authoritative but may be inaccurate. These models could generate text that 
seems coherent and relevant but often lacks true reasoning capabilities
, leading to plausible but false information. This risk is particularly concerning for journalists as we must present verified and reliable facts. Misinformation or incomplete information presented as fact threatens journalistic integrity, and AI-driven searches that prioritize sources based on algorithmic weightings could skew results without human oversight. Such an approach may lead journalists to rely on flawed data, undermining trust in their reporting.
Source Credibility
While tools like Perplexity AI attempt to address source credibility by providing citations, ensuring the reliability of these sources remains a challenge. Generative AI often masks the range of sources it draws from, 
limiting transparency
 and users’ ability to discern reliability. This could amplify fringe or biased viewpoints if the 
underlying training data isn’t adequately curated
. Tools that support deeper exploration and user agency could help mitigate these risks and encourage more informed reporting.
Over-reliance
The speed and convenience of AI search could lead to an over-reliance on automated research, potentially 
discouraging deep investigative work and critical thinking
. Reducing user interaction to quick information retrieval could undermine essential aspects of journalism, such as 
analysis and information literacy
. This shift could diminish the richness of original reporting, as journalists might opt for efficiency over depth.
Privacy
Privacy issues may arise when sensitive or confidential data is used in AI searches, raising significant questions about source protection and data security. Journalists often handle sensitive information, such as details about whistleblowers or ongoing investigations, that must remain confidential. AI tools, particularly cloud-based systems, could inadvertently expose this data, leaving it vulnerable to breaches or misuse.
Striking a Balance for the Future
Generative AI search tools have the potential to elevate journalism, but they come with caveats that cannot be ignored. As newsrooms embrace this technology, it is critical to ensure these tools are used responsibly, with stringent checks for accuracy and ethical safeguards.
If newsrooms integrate these technologies, they must prioritize oversight and transparent practices. Responsible use would ensure that AI-powered search is a powerful ally in pursuing truth, not a shortcut that compromises it. With careful implementation, generative AI could be harnessed to enhance storytelling, deepen analysis, and uphold the credibility that journalism demands.
—
Footnotes
[1]. Prompt: I’m writing a story on how climate policy will change under Trump. Give me a downloadable research packet summarizing key policies, decisions, and news in this area.
Perplexity AI Response
ChatGPT Response]]></content:encoded>
</item><item>
<title><![CDATA[Structured Outputs: Making LLMs Reliable for Document Processing]]></title>
<link>https://generative-ai-newsroom.com/structured-outputs-making-llms-reliable-for-document-processing-c3b6b2baed36?source=collection_home_page----df04cefca135-----22-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Llm]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Data]]></category><category><![CDATA[Python]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2024-12-05</pubDate>
<atom:updated>2024-12-05</atom:updated>
<content:encoded><![CDATA[Structured Outputs: Making LLMs Reliable for Document Processing
Nick Hagar
Follow
10 min read
·
Dec 5, 2024
--
1
Listen
Share
A guide — and prototype — for getting clean data out of PDFs
A schema acts as a blueprint, telling the model exactly what data to extract from a PDF and how to format it.
Journalists get PDFs as responses to FOIA requests, though document dumps and via white papers. To make use of PDFs, these journalists need to get data out of documents and into an analysis-friendly format, like a spreadsheet. The process can involve laborious, manual transcription or copying and pasting data from one format to another.
Theoretically, large language models can assist with document processing, but risks like hallucinations and the inherent uncertainty of LLM outputs make this approach tricky. Journalists need to be certain the output actually contains the needed data, follows the needed data types and is in a usable format.
Structured outputs offer a solution to these challenges. Providers like 
Anthropic
 and 
OpenAI
, and open source libraries like 
Outlines
 allow developers to define strict schemas that constrain LLM responses to specific fields, data types, and formats.
Structured outputs transform raw LLM capabilities into reliable data processing pipelines. When extracting tables from multi-page PDFs, for instance, a schema ensures consistent column names and data types across pages. While this approach cannot guarantee perfect accuracy, it reduces the engineering complexity of parsing and validating LLM responses, making document processing workflows both more reliable and more maintainable.
A schema is essentially a blueprint that tells the model exactly what information to look for and how to organize it. Think of it like a standardized form: rather than letting the model return data in any format it wants, the journalist provides specific fields to fill in — this is a date, that’s a dollar amount, this other thing should be a yes/no value. Just as a tax form ensures everyone reports their income in the same way, a schema ensures the model extracts data in a consistent, predictable format.
Using schema-based structured outputs with LLMs requires familiarity with three aspects of Python programming: data types, class structure, and how to interact with the OpenAI (or other provider) API. To demonstrate the potential of this approach, below are some example processing pipelines built with OpenAI’s GPT-4o model — first a walkthrough setting up the key concepts, then several real-world documents. You can also try this approach yourself with 
the prototype I’ve built
: upload a document, and the system will suggest an appropriate schema and extract the data automatically.
Setup
Every case of working with structured outputs involves a workflow of 1) identifying the needed data from the input document(s), 2) defining a schema that describes that data, and 3) sending a request to OpenAI with this schema and a prompt. To walk through this process, let’s say we have a stack of business cards from a recent conference that we want to put into a spreadsheet.
Mock business cards that we want to process with an LLM
Data
Every card is formatted differently, and they all have slightly different information. For our spreadsheet, let’s say we need a name, job title, company, and email address for each business card. This defines the data that we will include in a schema — everything else gets ignored.
Schema
For structured outputs, schemas are defined using 
Pydantic
, a Python library that lets us specify exactly what data we want and how it should be formatted. Pydantic schemas take the form of classes, where each data field is a class attribute with an associated data type. Since all the fields in these business cards are text (as opposed to, e.g., dates or numbers), they are all string types:
This expresses the data required from each business card in a structured, machine-readable format.
Getting data from OpenAI
Finally, a request sent via OpenAI’s Python library lets the LLM ingest each business card, identify the required fields, and return the requested information as a structured JSON object (see 
this guide
 from OpenAI on working with image data).
Code:
Output:
We can then apply this schema to 
every 
business card in the stack, turning a disorganized pile of documents into a standardized spreadsheet with exactly the information we need. And because LLMs are flexible, and schemas can describe all sorts of data, this approach works for a wide range of real-world applications.
Real-world data
To demonstrate how this workflow might be used for reporting, below are several examples of using structured outputs on real-world documents.
First is an example using 
campaign finance reports
 for a recent mayoral election in my hometown of Fort Wayne, Ind. Federal campaign finance data are regulated by the 
FEC
 and accessible through websites like 
OpenSecrets
, but local data are more difficult to query. Reports are stored in state- or county-level records repositories and may not follow a standard format. They often aren’t OCRed — in some places, records are handwritten and scanned into the PDF format. An LLM can make the process of cleaning these data for downstream analysis much smoother.
A campaign finance report like this one often has several components — a cover sheet, tables of contributions from individuals and organizations, and tables of expenses and debts. Structured outputs can handle any of these formats reasonably well; to start, let’s transform a table into a spreadsheet format.
Looking at a sample of individual contributions, there’s a straightforward table layout that we can translate into a schema:
An example of individual contributions from a campaign finance report, which we’ll extract with GPT-4o
The code creates a blueprint for our table: first we define what each row should look like (a ContributorRecord with fields for names, amounts, dates, etc.), then we create a structure for the whole table (ContributorTable) that will contain a list of these records. Each field has a specific type — text fields are marked as ‘str’, dollar amounts as ‘float’ — which helps ensure our extracted data will be clean and consistent.
But as with many documents of this type, there are also some quirks that we might want to account for, and that might be challenging for other data extraction approaches. The report lumps name, address, and occupation into one column; we probably want those separated out. Date and receiver are combined in an unusual split column, and contribution type is recorded as a mix of optional checkboxes and free text entry. We can update our schema to account for all these features, transforming a non-standard table into a spreadsheet format that we can work with.
And, after sending these pages to OpenAI’s API, along with our desired schema, the LLM produces a well-formatted spreadsheet of information that we can use for further analysis. Even better, we don’t have to OCR or preprocess the pages in any way — we can send images directly to OpenAI and get back data.
What about a less standard data format? Let’s take a look at the cover sheet of this report, which contains a mix of key/value pairs, free text fields, and a table of financial data.
A campaign finance report cover sheet, with a mix of different kinds of structured data
Again, we can define a schema to capture the fields that we’re interested in. This can be selective — let’s say we care about the committee information and the financials, but not any of the information about the candidate or the report type:
This lets us not only structure data from a messy document, but also to preemptively filter out anything that isn’t relevant for downstream analysis:
Because GPT-4o can ingest as many images as will fit into its context window, this approach can also handle data that breaks across multiple pages. Consider this polling data, from the 2023 New Hampshire Republican primary:
The data are well-formatted, but, as with the campaign finance report, they’re difficult to ingest for machine analysis. Some questions break over multiple pages, and the layout includes formatting and information that is extraneous to the questions and answers themselves. But we can define a schema that corresponds to only the parts of the document we care about, send all the pages to the model as images at once, and get back a robust list of questions and responses from this survey:
In all three cases, it’s important to note that 
consistent structure does not equate to perfect data quality
. For the campaign finance cover sheet, the model gets several details wrong (e.g., mixing up itemized and unitemized contributions). For the individual contributions, it struggles with contributor occupations, and it makes transcription errors (e.g., transposing digits, misspelling street names). While it is able to record the polling data accurately, these examples serve as a note of caution — 
for critical data, human review of model outputs is still very much required
.
Because LLMs are so flexible, we can extend this schema-based approach even further. Rather than mapping out each field in a document, assigning it a datatype, and coding up a schema, we can input the document and have the model generate the schema itself. To get back an accurate and valid schema, this approach requires a few steps — -first reasoning about what kind of data is contained in the document, then generating a first pass of the schema, then cleaning it up to conform to OpenAI’s API requirements. But this pipeline produces schemas that are good enough as a jumping off point, and can often be used for data extraction as-is.
System diagram of a schema generation workflow
You can try out this schema-based data extraction with your own documents, with the 
prototype demo
 that I’ve uploaded to Streamlit.
How does it compare?
Of course, schema-based extraction with foundation models isn’t the only way to process documents. OCR has been around for a long time, and there are purpose-built tools for helping journalists process PDFs. Using these same test cases, I’ll walk through some points of comparison between the process described above and some alternative approaches.
Local LLM
One of the drawbacks of the approach that I’ve outlined is that it requires sending your data off to OpenAI or another external model provider. This means accruing the cost associated with querying foundation models, as well as taking on the risk of an outside organization gaining access to your data — a nonstarter for some reporting projects. To mitigate this, we can test the same pipeline with a small, permissively licensed model, running entirely locally. Other than specifying a different model, this change doesn’t even require any changes to our implementation. 
Qwen2-VL-7B-Instruct
 accepts multimodal inputs, and, if we run it through 
LM Studio
, we can continue using structured outputs to define our desired schema.
One big caveat — these local models struggle with processing multiple images in one call (although, your mileage may vary depending on the amount of memory on your machine). But for single pages, the results on our test cases aren’t too far behind GPT-4o’s. For the campaign finance cover sheet, the local model misses some fields, and there are again transcription errors, but it’s the only approach I tested that correctly identifies all the financial information.
So while a local model might require a little more patience to process the data, and a little more manual review, it can also be a viable solution when cost or data security are important.
OCR + table extraction
This is the most established approach for pulling data out of documents. For these test cases, I used 
Docling
, a state-of-the-art system with models for OCR and table extraction developed by researchers at IBM. And while the transcription quality of Docling is comparable to that of the LLMs, the output is generally unusable (especially when data don’t follow a standard table format). Here, for example, is a snippet of Docling’s output, in markdown format, for the polling data:
Pinpoint
The options described above all require some level of technical investment to implement. For an off-the-shelf solution, I turned to 
Google’s Pinpoint
. Pinpoint is billed as a research tool for journalists, designed for searching, filtering, and analyzing large sets of documents. Pinpoint also has a feature, currently in beta, that allows journalists to extract structured data from these documents.
This feature works well for tables. In the UI, you can highlight a table, and Pinpoint automatically identifies the column headers and values. It also looks through all the documents uploaded in a project, finds tables with the same schema, and extracts the data from those tables automatically.
However, Pinpoint also has some limitations. It doesn’t allow for any adjustments to the columns in the output table. This means that, for example, the contributor data that we explored above has name, address, and occupation lumped together, in a sometimes messy chunk of text. To apply the table schema automatically across all occurrences, they have to be contained in multiple documents — -it doesn’t work for different pages in the same document. And the structured data feature is far less useful for non-table data. The polling data, for example, wasn’t really possible to extract with this approach. So while some tabular data might be a good fit for this approach, and it offers a (currently free) off-the-shelf solution, Pinpoint isn’t nearly as flexible as the schema-based solutions.
Conclusion
Schema-based document processing with foundation models occupies an interesting middle ground. It’s more flexible than traditional OCR approaches, allowing us to handle diverse document formats and extract complex information. It’s more reliable than pure LLM chat interfaces, with built-in validation through schema enforcement. There are still limitations to this approach: It requires technical knowledge of Python programming, and it doesn’t eliminate transcription errors or the need for human review. But at the same time, it can dramatically streamline the mechanical work of transforming documents into structured data.
For journalists working with local government documents, researchers processing survey responses, or developers building document analysis pipelines, this approach offers a practical path forward. You can process documents at scale while maintaining control over the output format, whether you’re using GPT-4o or running everything locally. The technology isn’t perfect — careful validation is still essential, especially for critical data. But by constraining LLM outputs to specific schemas, we can start to build reliable document processing workflows that help us spend less time copying and pasting, and more time on substantive analysis.]]></content:encoded>
</item><item>
<title><![CDATA[Teaching a CustomGPT to Help Inspect the Philippine National Budget]]></title>
<link>https://generative-ai-newsroom.com/teaching-a-customgpt-to-help-inspect-the-philippine-national-budget-2acadd25a172?source=collection_home_page----df04cefca135-----23-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[ChatGPT]]></category><category><![CDATA[Building Your Custom Gpt]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Jaemark Tordecilla]]></dc:creator>
<pubDate>2024-11-25</pubDate>
<atom:updated>2024-11-25</atom:updated>
<content:encoded><![CDATA[Teaching a CustomGPT to Help Inspect the Philippine National Budget
An Iterative Approach to Developing an AI-Assistant to Support Sensemaking
Jaemark Tordecilla
Follow
15 min read
·
Nov 25, 2024
--
Listen
Share
Image generated by Midjourney
Although the Philippines has
 one of the world’s most transparent budgeting systems
, ordinary citizens rarely get involved in the budgeting process. In the 2023 Open Budget Survey by the International Budget Partnership, the country scored an impressive 83 out of 100 in Budget Oversight (how well its budget is overseen by lawmakers and auditors), and 75 out of 100 in Transparency. However, when it comes to Public Participation, the score drops to just 33 out of 100, showing that citizens have limited opportunities to participate in fiscal decisions.
This phenomenon is not limited to the Philippines, or to developing countries, for that matter. Norway scored an 87 in Budget Oversight and 80 in Transparency, but just 24 in Public Participation. New Zealand, meanwhile, scored a 78 in Budget Oversight and 87 in Transparency, but just 55 in Public Participation, which is good enough to rank it second in the world in the metric.
When examining budget documents, it’s easy to see why. The Philippines’ 
Department of Budget and Management
 publishes both the proposed and the approved versions of the country’s budget each year. Each document contains a line item of every expense for the government for the year.
This means that the 2025 National Expenditure Plan, which includes the proposed budget for next year, is a beast of a document. It comes in a 56MB Excel spreadsheet that contains more than 730,000 items. The 2024 General Appropriations Act, the approved budget for this year, comes in a 60MB Excel spreadsheet, and nearly 700,000 items.
Given its sheer size, navigating through the data can be a challenge. Just opening the files on a typical journalist’s laptop computer and trying to filter the data leads to frozen screens faster than you could say “pivot table.” Extracting any meaningful data means having to use a professional business intelligence tool, or knowledge of a data science scripting language such as Python or R.
The budget document itself can also be challenging to make sense of. The document contains 18 columns, but they aren’t always used in the same manner. For example, for infrastructure projects under the country’s public works department, details of the project are included in a column called ‘DSC,’ which refers to a description of the project. But for information related to public schools, the name of the school is located in another column unhelpfully named ‘UACS_OPER_DSC,’ which I suppose pertains to an operational description of the line item. This is because the budget units of all the individual government agencies separately fill out a form given by the country’s budget and management department, which then compiles all the disparate documents into one big dataset.
Unfortunately, there’s no data dictionary or guide available to technical users to map this information. If you’re a journalist, a researcher, or just a member of the general public digging through these details, you would need to know the nuances of how the budget document is encoded to find exactly what you’re looking for.
As a result, for all its openness, the budget document remains the domain of technical experts. Both the budget department and the legislators in charge of oversight usually hire experts to help them make sense of the document. This means, however, that journalists, civil society groups, and regular citizens are always a step or two removed from the process.
Veteran journalist Norman Bordadora, who has spent decades as a reporter and an editor covering the budget process, told me that from his experience, many reporters pressed by a deadline on the budget beat usually rely on summaries given by the budget department or tips from congressional staff to find newsworthy items in the budget document. While some of the bigger news organizations in the country have research and data departments, individual journalists, as well as small newsrooms, are unable to dive deep into the budget document in more meaningful ways.
This got me started on a project to see if I could use AI to create a tool that would allow anyone to interact with the Philippine budget more easily. Can AI help journalists, researchers, civil society organizations, and regular citizens make sense of the Philippine budget?
Getting the Data Right and Creating the Bot
To get started, I tried uploading this year’s budget file into a Custom GPT I created called 
the Budget Bot
. I’ve used Custom GPTs extensively to 
create AI tools for the newsroom
, and I even 
taught a workshop
 on using them at the CUNY School of Journalism. I’ve found that Custom GPTs are the best way to share both complex custom instructions and common datasets, uploaded as a set of knowledge files, for a large group of users.
ChatGPT handles data through a feature called Code Interpreter, in which it generates Python code to query datasets and then executes them in an internal Jupyter Notebook. Furthermore, these features are accessible to anyone with a subscription to a paid version of ChatGPT, which could be as low as $20/month.
To get started, I put in initial instructions that gave it a description of the task as well as the budget spreadsheet as an Excel file.
## You're a bot designed to help users retrieve information about the Philippine budget from GAA-2024.xlsx.
It contains the following fields:
- UACS_DPT_DSC - A high-level description of the wing of government or executive department
- UACS_AGY_DSC - The name of the agency or office within the government wing or executive department
- DSC - A description of the function of the UACS_AGY_DSC line item.
- UACS_OPER_DSC - Additional details about the agency line item
- UACS_FUNDSUBCAT_DSC - The budget subcategory of the line item
- UACS_EXP_DSC - The general line item category
- UACS_SOBJ_DSC - The specific object of expenditure of the line item. 
- AMT - Amount in thousand Philippine peso
I only included instructions for the columns that contained text data in the budget. The rest of the other columns contained various ID numbers related to the Philippine government’s 
Unified Accounts Code Structure (UACS)
, a coding system that helps agencies with internal tracking of funding sources, cash management, accounting, and auditing.
Immediately, I ran into trouble with the file. The Budget Bot generated Python code using the Pandas library, but when loading the Excel spreadsheet, the execution of the script always timed out. It would try to fix the problem, but then time out again. A 60MB Excel file was proving too much for the system.
I tried another approach. On my desktop, I converted the budget file into a CSV spreadsheet. It generated a bigger file, almost 200MB, but I was able to upload it. When testing the budget bot this time, the script did not time out and, while it was still a bit slow, it worked perfectly. To optimize it further, I went back to the CSV file in my desktop and cut the UACS ID columns, reducing the size by about 25MB.
Lesson #1
: When working with Custom GPTs, I found CSV files were better for working with large datasets.
Indexing: Giving the Bot the Key to the Data
The Budget Bot works by taking the user’s question in plain English and turning it into Python code that it will execute to search the dataset. For example, if a user types in, “Show me the budget for the Department of Health,” the bot would need to generate and execute Python code that would search the departments column for the term “Department of Health.”
During my initial testing, the biggest issue I encountered was getting the Budget Bot to search for the correct things inside the budget. For example, the bot was usually smart enough to figure out that a user who is searching for “UP budget” is searching for details about the University of the Philippines; it wasn’t smart enough to know that the budget for the school was listed in the column for agencies under the name “University of the Philippines System.”
I wanted to give the bot a leg up, and interestingly, the best way to do that is to use the bot itself! Using its dataset, I asked it to generate a text file that contains a list of unique values found in the first two columns, which are entries for Department and Agency. I added this to the bot’s Knowledge Files, and added instructions for the bot to check the list first before generating the Python code, to make sure that it would know the exact search term to use.
If the user is searching for departments or agencies, determine exact name 
of the department or agency first. Do not guess.
The results were mixed; sometimes, the bot would follow the instruction to check the file first, but more often, it would just ignore that instruction and search for what it thought was the correct search term. (For people who have worked extensively with chatbots, this is one of its most maddening tendencies.)
I tried a different approach. Instead of a text file, I asked it to generate the same list of departments and agencies, but this time in a JSON format. I then uploaded this JSON file into the Budget Bot’s Knowledge Files with the instruction to check the list first.
It worked! It turns out that Custom GPTs would “read” text in JSON format first before generating the Python code and running Code Interpreter. This meant that it already knew the correct names of each department and agency before it generated the Python code to query the dataset. Since implementing this change, I haven’t seen any issue with the Budget Bot searching for the wrong Department or Agency.
It also made the Budget Bot smarter about searches. If a user asked for budget items related to sports, it would immediately figure out that the user wanted information from agencies such as the Philippine Sports Commission or the National Academy of Sports. Asking the bot for budget items related to early education would return results from Early Childhood Care and Development Council, an agency under the Department of Education.
I tried to push my luck to see if JSON files would work as an index for other columns. These columns usually had more unique information, and the files ran much larger than the file containing just Departments and Agencies. In testing, I found that instead of checking them, the budget bot would ignore JSON files altogether if they were too big, rendering them useless. In the end, I simply went with the original JSON index containing Departments and Agencies.
Lesson #2:
 Use JSON files to create indexes when working with large datasets, but make sure they’re not too big.
I also tried other optimizations, like breaking down the dataset into multiple files, and putting back the UACS IDs that the Budget Bot could then use to combine the data back together through JOIN commands. This resulted in the bot having to generate more complex Python code, which resulted in frequent errors and garbage data being retrieved. I went back to the original flat dataset.
Lesson #3
: It’s better to provide data as a flat table instead of splitting it across multiple files for Custom GPTs.
Teaching the Bot to Search
Most of the rest of the work involved testing different searches to find nuances within the budget data. A lot of it was about making sure that the bot was searching for the right terms in the right column.
In this case, the best tool I used to improve the Budget Bot was the Budget Bot itself.
For example, through trial and error with the bot, I found that details about infrastructure projects under the Philippines’ public works department are in the column DSC. To make the Budget Bot better, I simply needed to add a line in its instruction set:
DSC - If the question is about construction of infrastructure projects related 
to public works, the name of the project is likely to appear here and should 
be part of the search.
At workshops and meetings the past few months, I did hands-on demonstrations of the Budget Bot to fellow journalists, as well as researchers, academics, and technocrats and collected their feedback. Guided by their comments, I would use the Budget Bot to find further nuances within the budget document itself.
I would discover, for example, that the names of state universities and colleges could be found in the column UACS_AGY_DSC, while names of primary and secondary schools would be found in the UACS_OPER_DSC column. This “operational description” column would also be where you could find the location of regional and district offices of various departments; the names of public hospitals; the names of prisons and penal farms; and the locations of embassies and consulates. All these were added to the main instruction set to allow the Budget Bot to find things easier for future use.
By exploring the budget document through the Budget Bot, I also found more esoteric details, like how line items for embassies and consulates are marked in the DSC column by the term “Conduct of bilateral and multilateral relations in accordance with foreign policy directives” or “Provision of consular services including issuance of passports, visas and other consular documents.” Again, these peculiarities were added to the main instruction set.
In many ways, it was my journalistic expertise that was guiding me in building the tool. This part of the process is most akin to the work of reporting, where I explored large amounts of data to find noteworthy things and to document them. Only instead of publishing my findings, I used them to inform an AI tool so other journalists and researchers would not have to go through the same tedious steps. When building an AI tool like this, there is so much value that journalists could bring to the design process.
Lesson #4:
 Having journalistic skill and expertise is a key asset when teaching an AI tool how to search for information like a journalist.
Adding Budget Files from Other Years
When I asked for feedback about the tool, University of the Philippines journalism professor Karol Ilagan pointed out the most obvious thing missing: being able to compare with previous years. A longtime investigative journalist, many of the most interesting fiscal stories she had worked on in the past involved analyzing movements of the budget of particular units of government from one year to another, i.e. how the budget for the Department of Health has changed through the years.
I wondered if the system could handle more than one budget file. After my initial testing with the country’s approved budget for 2024, I tried to add the 2025 proposed budget to the Knowledge Files of the bot. With just one line added to the instruction set describing the new file, the bot worked perfectly with the new dataset! To make this dataset smaller, I then also removed the UACS ID columns.
I tried to push the limit of the ChatGPT system by adding budget files all the way back to 2020, removing the UACS ID columns each time. The bot still worked, except for an issue where some of the departments and agencies have changed names through the years.
The fix was easy enough. I just asked the Budget Bot to generate additional entries for its own JSON key for departments and agencies that existed in the older budget files.
However, I noticed that having all the budget files in a single bot resulted in a severe degradation of performance. Simple queries would take several minutes, and would more often than not time out.
Ultimately, I decided to just keep three datasets in the bot: the 2023 approved budget, the 2024 approved budget, and the 2025 proposed budget.
I still wanted the main bot to have comparisons beyond just the three years. To do this, I simply used the bots to create an “aggregates” dataset, containing totals by Department and Agency from 2020 all the way to 2025. I then added a bunch of instructions to the bot to look at the aggregates dataset first before loading the bigger line item budget files.
For aggregate data queries:
- Start with the aggregates.csv dataset exclusively for questions related to budget (e.g., budget for a department or agency). If the aggregates dataset doesn't provide the needed detail, only then consider the detailed line-item datasets (e.g., NEP or GAA files).
- Before any query related to budget aggregates, ensure that the dataset is filtered by the year specified by the user. If no year is mentioned by the user, default to 2025. Do not perform any aggregation or calculations without first applying this year-based filter.
- Only revert to the original datasets if the user asks for detailed line items beyond department or agency aggregates (e.g., specific projects, regional offices, or detailed expenditures).
- Ensure that amounts are always displayed in thousands of pesos for consistency with the original format.
- If the user specifically asks for aggregates by year, filter using the Year column accordingly.
That way, users can compare figures from 2020 to 2025 in the same bot, even though the line item details are only available from 2023 to 2025.
To explore budget data from 2020 to 2022, it took about 20 minutes to create 
another Custom GPT 
with minor tweaks to the original instruction set, and it worked splendidly.
Lesson #5:
 Custom GPTs can handle multiple large datasets, but performance could degrade if there are too many.
Optimizing Further and Improving Transparency
For other improvements, I focused on accuracy and general usability improvement. Things like making sure the bot drops null values when doing totals; adds commas to improve readability of large figures (and then remembers that there are commas when the figures are totaled again); shows all columns for complete information instead of just one or two where the search terms were found; and displays the correct values because the original budget data was encoded with figures in thousand peso.
Another optimization: making sure the bot only looks up the budget for the upcoming year when the user does not specify a year, instead of loading every dataset available.
A question I would usually get at workshops showcasing the bot: How do we know it gave us the correct information? ChatGPT users are able to see the Python code generated to retrieve the data from the different datasets. To improve transparency further, I added instructions for the Budget Bot to explain explicitly what information it was looking for in which datasets.
When asked “How much is the budget for early education?” the Budget Bot explains which file it would use to find the information (‘NEP-2025–08–08.csv’), what column it would search (‘UACS_AGY_DSC’), and what information it searched for (‘Early Childhood Care and Development Council’). It also displays all the line item information it found, which the user could view or download.
I have since published the full instruction sets and the datasets on 
GitHub
 so that anyone interested could inspect these themselves. They could also be used to build the tool on other chatbot platforms.
I also created a 
user guide
 for those interested in trying out the Budget Bot.
What’s Next
The Budget Bot is already quite useful as a tool for information retrieval. It’s great for finding both aggregate data (“What is the total budget for the Department of Education?”) and granular data (“Show me all the confidential budget items for 2024”). It has been more helpful to people who already have some level of familiarity with the budget document and with government agencies and processes; someone looking for specifics like “Show me all the intelligence fund items under the police” would find more meaningful data than “Find all expenditures related to infrastructure.”
It’s amazing how well it works, considering it was a one-man effort that was built using a $20-a-month ChatGPT subscription. The biggest bottleneck, quite frankly, was finding journalists, researchers, and academics to test it. It requires a ChatGPT subscription to use, and not that many people I’ve encountered have the paid service. Most of the feedback I got came from sessions where I would do a hands-on demonstration for them.
It’s not just a toy either. Budget deliberations have been underway in the Philippine legislature for the past two months. When the new budget is approved by the end of the year, the Budget Bot would be the easiest way for many people without advanced data analysis tools to check the differences between the budget proposed by the executive department and the budget approved by Congress.
Adding a new budget data file and updating the instruction set takes just minutes, while new budget datasets are only published a couple of times a year, so it is easy enough to maintain. With that, it can be an evergreen tool for Filipino journalists. When the next typhoon comes, they can use the Budget Bot to find what flood control projects have been funded by the government over the years. When school opens, they could check how much money the government allocated for each school district. Academics, researchers, technocrats, and civil society representatives could benefit from the tool as well.
As with many other generative AI tools, being able to trust the results is the biggest hurdle. Over the next few months, I hope to work with newsrooms, civil society organizations, and academic departments to show them how to get the most of the tool while ensuring that they are able to check the trustworthiness of the results.
I hope it becomes a showcase for how AI can democratize access to data. After using this tool, my wish is that it inspires people to ask questions along the lines of: “If I can talk to the national budget, what other data can I talk to? Can I talk to my hometown’s budget? Why isn’t my hometown government publishing its budget?” Already, academics I’ve met from the University of the Philippines’ National College of Public Administration and Governance have seen the potential of AI tools like these to revive their push for open government data publishing.
To serve as a showcase of visualizations using data from the Budget Bot, I also plan to create a graph gallery where users can give their own submissions.
A bar chart generated by the Budget Bot after asking it: “Show me the top 10 departments by highest internet subscription expense.”
I also hope it inspires other newsrooms around the world, especially those from developing countries, to explore how AI tools can help them access larger datasets even without the manpower or technical resources of larger media organizations.]]></content:encoded>
</item><item>
<title><![CDATA[AI-Powered Content Audits for Local News]]></title>
<link>https://generative-ai-newsroom.com/ai-powered-content-audits-for-local-news-0608773996ec?source=collection_home_page----df04cefca135-----24-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Local News]]></category><category><![CDATA[Ethics]]></category><category><![CDATA[NLP]]></category><category><![CDATA[Media Analysis]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Rahul Bhargava]]></dc:creator>
<pubDate>2024-11-18</pubDate>
<atom:updated>2024-11-18</atom:updated>
<content:encoded><![CDATA[AI-Powered Content Audits for Local News
How to responsibly use AI to help with understanding your coverage
Rahul Bhargava
Follow
8 min read
·
Nov 18, 2024
--
1
Listen
Share
Image generated in Adobe Firefly: “hand tallying data with a magnifying glass in front of a bank of computers showing news stories”
Organizations across the news industry are exploring ways to use generative AI to support their storytelling (many shared on this very blog). A growing trend is to use these new tools on the other end of the production cycle, to support content audits that assess a large set of stories against the mission and goals of your newsroom. With 
Meg Heckman
 and 
Liz Hadjis
, I conducted a test of this approach in partnership with a hyper-local outlet (
The Scope Boston
), and I want to share some of our methods and higher-level critical questions. Hopefully these help you come up with a thoughtful strategy for how you might integrate AI-assisted content auditing into your editorial workflows.
Why Conduct a Content Audit?
Before diving into the details, it’s worth exploring why a content audit can be valuable, especially for local or hyperlocal outlets. A content audit can take many forms but focuses on assessing the media you produce against a set of metrics for what it talks about and how. This could include looking at how much you write about certain places, who you choose to quote, what other sources you link to, demographics of your writers, and more. Newsrooms that focus on smaller communities face many challenges (small staff counts, limited funds, crowded information ecosystem, etc.) but also have unique assets (local relationships, community trust, physical presence, etc.). Running a content audit can be a helpful way to reinforce those assets, asking questions about gaps or patterns in coverage and assessing if community voices are being fairly represented or if you are contributing to local stereotypes. Being transparent with results by publishing them can build trust, and also guide future editorial planning.
More and more newsrooms are performing content audits and publishing them online.
A growing number of newsrooms are taking on content audits, from the 
Philadelphia Inquirer
 to 
the City NYC,
 measuring alignment between their stories and editorial goals. The Scope’s audit was motivated by their mission to highlight under covered places and peoples in Boston. They wanted to measure whether their content matched that goal by analyzing (1) which locations got the most coverage, (2) what topics dominated their reporting, and (3) whose voices were most quoted.
What We Did, and What We Found
We worked with the Scope’s editor to gather all their content and then report back on those three guiding questions: places, topics, and quotes. The first two were relatively easy to do with off-the-shelf open-source technologies for 
extracting places mentioned in text
 and 
assigning classic news topics to a story
. The third was harder, because we found no ready-to-use tool for news articles that could extract quotes, attribute them to speakers, and tell us which pronoun was used about their speaker (so we could determine their gender ethically). For that one we turned to generative AI and after iterating on prompts we were surprised to see that it performed quite well.
A block diagram of our prototype technology pipeline.
As with any project, data cleaning took a big chunk of the time. However, since we started with a machine-readable XML export from the Wordpress-hosted site the cleaning was mostly on the results. For instance, we had to filter geographic places for only local neighborhoods and do things like convert “Jamaica” into “Jamaica Plain”. For the quotes, because we used generative AI, we had to validate that each and every quote was actually from the story, and that the attributed speaker’s name was also in the text. To my surprise ChatGPT didn’t invent any quotes or speakers and only ended up costing about USD $6 in total (via API calls to their “gpt-3.5-turbo” model). See 
the pre-print we presented at AEJMC earlier this year
 for details on the prompts and related methods.
At a high level, we found some validation for the Scope’s existing practices and some opportunities for improvement in line with the mission statement. Geographically, coverage was dispersed across the city and the neighborhoods editors hoped to focus on. However, while some Boston neighborhoods, like Mission Hill (17% of stories), were covered extensively, other areas like Mattapan and Roxbury received less attention (1% each) despite being very close to the institution that operates the publication (Northeastern University).
The Boston neighborhoods covered most often in stories. Longer bars mean more stories mentioned that place.
In terms of topics, culture (26% of stories), politics (18%), and economics (17%) dominated The Scope’s coverage, which made sense given their mission. However, we found that other issues the editors deemed important like transportation (2%) and technology (<1%) received little attention. This was especially true when we dug into the topics covered in neighborhoods; note the absence of transportation coverage in Mattapan, Quincy, and Roxbury despite it being an important local issue.
A heatmap-style diagram showing how much each topic was covered in each neighborhood. A darker blue square means more coverage of that topic in that place. A blank square means no coverage.
For quotes, speakers were identified by “she” (57%) more than “he” (38%), perhaps because many of its reporters identify as women. In addition, very few of the top quoted people were politicians or other public figures. Just 37% of those quoted overall fit into this “social elites” category. These findings counter results from research across mainstream media sources.
A chart showing quoted speaker gender based on the pronoun most-often used to refer to them in the stories. Bigger bars mean more unique speakers were referred to with that pronoun.
These results matched some of what the publication suspected, but also introduced some surprises and ideas for changes to consider for the upcoming year of coverage. You can read our 
full academic paper from the 2024 Computation + Journalism Symposium
 to see more detailed results about these three approaches we took in our content audit.
What You Might Want to Try
Taking on a content audit might seem daunting, but with just a few days of work a data scientist could produce a first draft. We’re hoping to make that even easier by finding partners and funding to build a web-based toolkit others could use.
In the meantime, if you do want to get started on this sort of thing, there are some key preparatory and strategic conversations and tasks that you should take on. First, before choosing any tools you need to define what you want to measure and ensure it is in alignment with your mission. Local newsrooms have a geographic focus, so that is kind of an obvious choice. Beyond that, what kind of representation do you care about? Linguistic? Quotes? Identities? Organizations?
Second, when you’re ready to do some technical work try to choose well-validated off-the-shelf machine learning tools before diving into generative AI systems like ChatGPT (more on this below). Established tools are often already evaluated and robust from prior studies and use. They also create reproducible results, unlike this new batch of generative LLM tools that can sometimes make things up.
Third, once you have data, be sure to take the time to interpret the findings with your team and broader community. The data you find isn’t going to be perfect but can reveal patterns that tell you something about your coverage and might suggest changes you could make. If certain populations, places, or topics are consistently underrepresented, make a plan to address this in future reporting.
Finally, make a commitment to regularly audit your content every year or so and publish it publicly to hold yourself to account. Integrating this kind of process into your annual workflow is the only way to create meaningful results and change over time.
Be a Critical Technology User
I could end this write-up here, but I think there’s an important reflection about technology that also needs to be addressed. Chat-based generative AI tools feel like a magical solution to using advanced computer-based language analysis approaches, but I encourage you to be a skeptical consumer in the face of the massive hype around them. Note that we only used the ChatGPT API where we could find no other solution. I have three main reasons why I think this stuff should be a tool of last resort: hidden costs, relational risk, and accuracy.
To start with, the energy costs of using these tools are massive. The power needed to create generative AI models and run them is unsustainable and exacerbates existing ecological disparities. The now-famous 
“Stochastic Papers” paper from Bender, et al
 documents this in depth. Power consumption needs are driving 
a massive race between companies to secure energy supplies
 for their server farms in 
a world already suffering from scarcity
. Many alternative non-generative machine learning models that have been around for a while can run on your laptop computer, take far less power to train, and produce accurate and usable results for many audit-related tasks.
On top of that consider the history of Big Tech as an untrustworthy partner to newsrooms. Remember 
Google’s AMP
? The Facebook-driven 
pivot to video
? Suspiciously 
motivated funding opportunities
? 
Copyright concerns about training data?
 Where do you draw the line for your publication in regard to working with Big Tech companies whose choices are so-often demonstrably misaligned with the news industry? Using well-packed open-source software alternatives can help you avoid many of these relational risks. Or maybe using open-weight versions, where you can tune the model’s parameters to control behavior more directly, is a good middle ground for you?
Finally, consider the fact that generative AI tools are designed to statistically make things up. The large language models underlying chat-based interfaces are just giant systems trained on text to count how often words appear next to other words; they’re probabilistic Xerox machines. It is very hard to both validate their outputs and ensure reproducible results. Non-generative machine learning tools don’t introduce the same data quality risks, and many of them have already been validated through prior use and research evaluation processes.
Final Thoughts
A content audit may sound like a challenging high-tech undertaking, but The Scope’s case shows it’s within reach for small, mission-driven newsrooms if you can find a little technology assistance. With just a few free and affordable tools and some clear goals, you can gain valuable insights into your newsroom coverage’s strengths and areas for improvement. I encourage you to try these evolving approaches out, making critical and intentional decisions that engage with the ethical quandaries I’ve shared, as appropriate for your organization. Running any sort of content audit is going to help you assess, and demonstrate to your readers, how your content is in alignment with your mission and goals.]]></content:encoded>
</item><item>
<title><![CDATA[Why Bother with AI Transparency?]]></title>
<link>https://generative-ai-newsroom.com/why-bother-with-ai-transparency-7dbaecb898d0?source=collection_home_page----df04cefca135-----25-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Transparency]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Labelling]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Sophie Morosoli]]></dc:creator>
<pubDate>2024-11-12</pubDate>
<atom:updated>2024-11-12</atom:updated>
<content:encoded><![CDATA[Why Bother with AI Transparency?
The Audience Perspective on How and Why News Organizations Should Disclose their Use of AI
Sophie Morosoli
Follow
10 min read
·
Nov 12, 2024
--
Listen
Share
Note: This post was co-authored by: Sophie Morosoli, Hannes Cools, Karthikeya Venkatraj & Laurens Naudts
(DALL-E)
Research on generative AI (GenAI) and journalism often focuses on the uses and perceptions of the technology, with 
comparatively less attention given to audiences’ needs
 when it comes to disclosing the use of AI. This blog post explores the complex issue of transparency in AI-driven journalism, drawing on recent research at 
the AI, Media and Democracy Lab
 of the University of Amsterdam into Dutch citizens’ views and needs around knowing about AI use in news. First, we present the legal perspective and summarize some recent scholarly work regarding individuals’ desires for disclosures of AI. Then we highlight some key concerns and wishes that came out of our focus group research. Finally, we reflect on the implications and propose some sound practices for AI disclosures. These findings offer insights for news organizations into the citizens’ perspective regarding societal concerns, their desire for transparency, and the kind of disclosure that may help rebuild trust in an AI-augmented media landscape.
In recent years, the media sector has been actively experimenting with (generative) AI across the entire journalistic value chain. Currently, research by 
The Associated Press
 has shown that AI has been most prominently used for content production, which includes uses like generating social media posts, news headlines, and drafts for a story. Similarly, 
other research
 has found that translations and information gathering are named as one of the main uses of AI in newsrooms. With rapid advancements in generative AI, it’s only a matter of time before every newsroom is leveraging these powerful tools, often provided by major tech companies like OpenAI, Google, and Microsoft. This growing dependency on big tech, as well as the increasing use of GenAI by journalists, forces news organizations to think about how they should deploy these technologies responsibly, sparking questions like: “Which provider should we use?”, “What are banned and allowed uses?” and “Should or should we not communicate such uses with our audiences?”
To better understand some of these above-mentioned questions, regulatory frameworks, and guidelines could help establish forward-facing accountability measures concerning the use of AI throughout the entire journalistic value chain. Both lawmakers and the news industry, however, are cautious toward the sharp regulation of journalistic best practices as this can clash with free speech interests. For example, in the European Union, the recently adopted 
AI Act
 requires disclosure statements when content is created and/or manipulated using artificial intelligence. News media organizations, however, appear to enjoy an exemption. Most notably, in the case of artificially generated and/or manipulated text content — audiovisual deepfakes remain subject to heightened scrutiny — disclosure duties only apply when they are “published with the purpose of informing the public on matters of public interest”. Depending on one’s interpretation of the law, the latter obligation might therefore only cover a minimal set of news topics. Yet, even in the public interest scenario, no disclosure obligation exists where the content “has undergone a process of human review or editorial control and where a natural or legal person holds editorial responsibility for the publication of the content”. Furthermore, as the AI Act does not label the media sector as a high-risk sector, there’s no immediate and clear legal duty for media organizations to impose responsible design, development, and deployment strategies.
Individuals’ Desire for Disclosure: What Current Research Says
Even though news organizations do not have to disclose the use of AI under the AI Act, the question arises whether individuals actually even perceive news that has been produced by or with the help of artificial intelligence differently. Do they trust the information? Do they think this type of news is credible?
Previous research shows mixed findings. On the one hand, 
research
 shows that news labeled as AI is perceived as more accurate, objective, and impartial by individuals because it lacks basic human characteristics such as beliefs and emotions. In this context, 
empirical evidence has suggested
 that AI-written news is evaluated as more credible including higher journalistic expertise compared to human generated news. On the other hand, 
scholars
 have also argued that AI as a news source is perceived as less credible than human journalists and individuals have a general tendency to reject artificial intelligence. In 
other research
, this AI aversion has been linked to reasons such as general skepticism toward technological innovation or overarching concerns about AI systems being a “black box” lacking essential journalistic values. Recent 
research
 has also identified a so-called “disclosure paradox”. In other words, audiences have a strong desire to be informed through disclosure statements about the use of AI in journalism, but at the same time, these statements might also lead to more distrust after readers have been exposed to them.
The current research appears both contradictory and inconclusive about how individuals feel about AI generated news content, with little research having been done to investigate what information needs individuals might have.
Dutch Citizens’ Attitudes towards AI in News: Concerns, Needs, and Appearance
Knowing that individuals desire to be informed about the use of AI in journalism but that there are inconclusive results connected to the perceptions of AI generated news, we set out to empirically investigate why news organizations should or should not disclose their use of AI and how AI disclosures should look like according to individuals’ wishes. To get to the bottom of this we conducted three sets of focus group interviews with Dutch citizens (N = 21), which allowed us to get in-depth and dynamic responses from the participants. 
(For more context on the methods, find the expanded explanation in the appendix.)
The focus groups gave us valuable insights into 
concerns
 people have regarding AI generated news and individuals’ desires regarding AI disclosures. First of all, we see that the fact that AI is used in news invokes worries and concerns for citizens. For instance, our participants were generally worried that AI might create false and misleading information. They also stress more far-reaching concerns such as polarization, job displacement, and the fact that AI can be used to manipulate individuals and their beliefs (i.e. deepfakes and conspiracy theories), which can lead to the erosion of trust. As one person said,
 
“There is a risk that the whole population becomes distrustful. Now we can still pretty much all trust each other but if at some point you can no longer tell whether something is real or fake. I am a little afraid that we will all be very distrustful of each other and of everything we send” (Anna, 23).
Given these concerns, in a second step, we asked citizens what 
needs
 they have to make clear that the content they are seeing was AI generated and how this information could be useful for them. Overall, we observe an outspoken desire for disclosure in the sense that 
news organizations should communicate their use of AI no matter what
. As one participant put it:
 
“I personally would have no problem with an article being written by AI. I just would like to know, but I think it can be very useful and indeed also to put more than one kind of information together. And yes, I would still just read that and make use of articles if indeed there were proper source citations” (Marie, 25)
.
 Connected to disclosure, we find a specific emphasis on clear labeling and source referencing. The interviewees underlined that if a news organization uses AI to generate content, the organization should make that very clear to the audience in the form of a distinct label. For instance: “As long as it is mentioned somewhere and that can be through an icon, that can be through text, but that plain and simple, it is clear that it is indeed AI generated”
 
(Marc, 56). Additionally, the news organization should have disclosure statements, so individuals have the option to trace back the sources used by the AI system to generate content: “Maybe also where they get the information from? Of course, it could be a combination of articles, but whether something is really news, or whether it is rewritten old news”
 
(Helena, 23).
Lastly, and in the absence of both regulatory and industry-driven guidelines, we asked citizens what effective labels of disclosure should ideally look like. Across the three focus groups, there was a striking agreement that labels of disclosure should stand out visually, be it regarding size, color, or placement: “Shouldn’t you make the statements bigger? […] Because a lot of times you don’t look at who wrote it. And so, if you do a little caption “created by AI” I don’t think people are going to see that […]”
 
(Margot, 41). “I think at the top. Then you know right away if you want to read it”
 
(Theo, 32).
Next to these concrete visual characteristics, some individuals also desire a watermark. One person pointed out the following: “For me then, the preference would be for a watermark which is not removable, and also indeed, at the time you print it out still visible” (Ratna, 25). Some individuals also stated that they would like labels that disclose different levels of the use of AI because they were more accepting of certain uses than of others. Labels could specify why generative AI was used in specific cases. For example, disclosure statements could differ if AI was used for a headline, to summarize the article, or to write a first draft. Likewise, labels could indicate news organizations’ dependency on AI “I would maybe like more options, in the sense of ‘fully generated’, ‘partially generated’, or ‘checked by AI’” (Erik, 32). This finding is consistent with the arguments the 
Trusting News
 project brings forward. A majority of the participants were in favor of a logo, which is easily recognizable and is connected to a certain sense of (institutional) accountability: “Of course, it also has to be a recognized logo. You just see that in the normal world that there are a lot of fake logos, and that people are being misled anyway. So it has to be something, something official”
 
(Bart, 63).
All in all, we conclude that if disclosure statements are considered by news organizations, 
they should visually stand out to immediately draw the attention of individuals to the label before they look at the content
. Furthermore, 
labels in the form of a logo or watermarks are the most desired
 when connected to an independent institution or certain (journalistic) values. Such 
disclosures enable individuals to make informed decisions to opt in or out of AI generated content
.
Three Key Learnings for News Organizations
Our findings highlight that the individuals who participated in our study want to be informed about the use of AI in journalism. We identified three key learnings for news organizations based on individual preferences:
Disclosure is expected but try to find a balance:
 People want to know when AI is used in news production. News organizations should clearly disclose AI involvement through labels, watermarks, or other visual indicators. These disclosures can help mitigate the perceived risks and concerns people hold when it comes to AI generated news content. Highlighting the benefits AI can have for journalism might be another way to reduce risk perceptions and could be included in disclosure statements. At the same time, news organizations can try to strike a balance regarding these disclosure statements as it might also lead to information overload.
Detailed information matters:
 There is no such thing as “the audience” which entails that different audiences require different forms/levels of disclosure about how AI was used (e.g., for headlines or grammar checks) based on their individual needs. News organizations could try to map different audience needs which can help establish credibility and can potentially reduce concerns about manipulation of content by news organizations.
Disclosure alone is not enough:
 While important, disclosing information should be combined with efforts to improve digital literacy and critical thinking. This comprehensive approach can better empower citizens to navigate AI-generated content. News organizations could consider involving their audiences more in conversations on how they should disclose specific uses of AI and engage in user evaluations, such as A/B testing, after having identified valuable disclosures.
Keeping these key learnings in mind, we conclude that being open about using AI in news is a good idea, but we believe it won’t solve all trust issues that the industry is facing. As AI adoption is ever evolving, determining the who, what, where, when, and how remains vital in disclosing specific information to audiences. Furthermore, more research needs to be done into the potential backfire effects disclosures could have to establish if transparency labels are even the right way to go about this. At the 
AI, Media and Democracy Lab
, we will continue studying the impact of AI disclosures through different methods to get an even better understanding of the citizen perspective of AI and journalism.
DISCLOSURE: This article was written by the human co-authors. Some sentences in the introduction and in the results sections were adjusted and made more understandable with the help of Perplexity.AI. The prompt used was: “Please make the following sentence(s) more understandable”. The picture was generated by DALL-E using the prompt: Create an image that visually represents the concept of focus groups discussing artificial intelligence (AI) and the importance of disclosure and transparency. The scene should show a diverse group of people in a professional setting, seated around a large table.
Appendix: Expanded methodology
Three sets of focus group interviews with Dutch citizens (N = 21) were conducted during the month of June 2024. We aimed for diversity in each focus group in terms of age, gender and education level. The participants were asked a set of predetermined questions, that included but were not limited to their general attitudes towards AI in journalism, their understanding of AI’s role in news production, and their expectations regarding disclosure. Each focus group lasted about one hour, and the transcripts of the audio recordings were translated from Dutch to English. After that, a group of four coders (all authors) coded the transcripts in NVivo, where we identified key themes of the interviews. The names used connected to the quotes are pseudonyms. You can send us an email 
here
 if you want to know more about the methodology.]]></content:encoded>
</item><item>
<title><![CDATA[The Balancing Act: Teaching Foundational Skills in the Age of AI]]></title>
<link>https://generative-ai-newsroom.com/the-balancing-act-teaching-foundational-skills-in-the-age-of-ai-c8ed870cca70?source=collection_home_page----df04cefca135-----26-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Medill]]></category><category><![CDATA[Education]]></category><category><![CDATA[Education Technology]]></category>
<dc:creator><![CDATA[Michael Crystal]]></dc:creator>
<pubDate>2024-11-05</pubDate>
<atom:updated>2024-11-05</atom:updated>
<content:encoded><![CDATA[The Balancing Act: Teaching Foundational Skills in the Age of AI
Michael Crystal
Follow
6 min read
·
Nov 5, 2024
--
Listen
Share
Generated by ChatGPT
As artificial intelligence rapidly transforms industries, journalism schools are facing a pivotal challenge to prepare the next generation of reporters for an AI-driven world without losing sight of the craft’s foundational skills. The question for journalism programs isn’t whether to embrace AI, but how to integrate it thoughtfully without undermining the profession’s time-honored standards.
“This is a fundamentally new paradigm that’s significantly powerful in a way that people, I think, haven’t experienced before,” said Jeremy Caplan, the Director of Teaching and Learning at CUNY’s Newmark Graduate School of Journalism. Caplan believes AI tools are not merely a passing trend but a useful asset that permanently impacts journalism.
Tools that leverage generative AI, such as ChatGPT, DeepL, Wordsmith, Otter.ai, and many others, are already widely used by journalism students. From transcription to content generation to personalization and localization, many essential journalistic tasks can be sped up with AI. Caplan said he believes journalism schools must immediately integrate these tools into the curriculum. He noted it was necessary to demonstrate both the use cases and the potential harms of generative AI in journalism.
Despite AI’s powerful capabilities, some professors remain hesitant to immediately integrate AI into their curriculum, especially in courses about the basics of journalism.
Desiree Hanford, a professor at Northwestern’s Medill School, is tasked with building the syllabus for 201–1, Fundamentals of Reporting & Writing News.
“I would like students to leave knowing how to write a good lead,” Hanford said, “they should know how to research a story, how to pitch that story, how to find sources, and write good paragraphs underneath the lead without using AI.”
Hanford said generative AI is not removing the need for students to have the same training in the foundations of journalism. Michael Deas, another Medill professor of 201–1, shared similar beliefs, saying “it’s important that students learn the foundational aspects of journalism first, and then they can appreciate AI later on.”
This cautious approach to AI integration stems from the technology’s current limitations and the need to prioritize foundational skills in the rapidly evolving field of journalism. While AI writing tools may offer quick summaries or content generation, educators emphasize the importance of human oversight, fact-checking and critical thinking.
The evolving nature of AI, coupled with concerns about the accuracy and potential for bias in AI-generated content, necessitates a measured and ethical approach to its implementation in journalism education.
Beth Bennett, the Associate Dean of Journalism at Medill, said the speed at which AI is advancing makes incorporating it into the core curriculum difficult. “We want students to be knowledgeable,” she said, “yet things are moving so quickly that it’s difficult to say exactly how this should be articulated.”
Bennett emphasized that Medill’s goal is to equip journalists with the essential skills to succeed in the industry, suggesting that their education should serve as a “simulation.”
A 2024 report titled 
Generative AI in Journalism: The Evolution of Newswork and Ethics in a Generative Information Ecosystem
, released by the 
Associated Press
 and co-authored by the 
Computational Journalism Lab
, surveyed 292 news industry professionals and found that 73.8% of them had used generative AI in their work. The report concluded that “Generative AI is already changing work structure and organization,” highlighting the growing need for journalists to become proficient with these technologies.
However, since there is no standard practice for using AI, it is challenging to implement a curriculum that teaches students the skills required to thrive. Most professors have little to no experience with generative AI tools and would require significant training to incorporate them into their courses.
Despite these challenges, Bennett is optimistic that Medill can integrate the necessary AI skills into its curriculum. “I don’t sense an arms-distance approach to AI,” Bennett said, “I sense a lot of curiosity and a desire to understand how we can make it a teaching tool, as well as a tool that students can learn from.”
Jonathan Soma, the Knight Professor of Professional Practice in Data Journalism and Director of the Data Degree Program at Columbia University, believes that even if students are allowed to use AI, there is a disparity in how effectively they utilize the tool. “I think that there’s a skill issue when it comes to using GPT,” Soma said, “the difference between someone who can use it well and someone who just blunders along with whatever it tells them is remarkable.”
This disparity in AI proficiency among students highlights the need for journalism schools to not only allow the use of AI tools but also to teach effective strategies for utilizing them. Educators are recognizing that integrating AI into the curriculum requires more than just access — it demands comprehensive instruction to ensure students can leverage these technologies responsibly and effectively.
Marie Gilot, the Executive Director of J+ at the Newmark Graduate School of Journalism, is trying to ensure journalists have the AI skills it requires to use the tools properly. Gilot’s courses focus on mid-career journalists who want to learn about digital skills, product management and executive leadership.
She wants to integrate technology into the development program for journalists. She hopes to “teach how to experiment with the technology early on, even if it’s still developing, so journalists can start applying it.” Gilot believes this tinkering is important for reporters and writers because they can “be part of shaping how it’s going to be used in the future.”
While generative AI has already impacted the industry and been utilized by many journalists, these powerful tools don’t come without their pitfalls. 
Hallucinations
, 
bias
, and 
ethical concerns
 are all real issues stemming from LLMs.
Hanford said these issues, among others, are why journalism professors aren’t rushing to inject their curriculum with generative AI content. “AI doesn’t tell me the truth all the time, so I’ve got a fact-check AI,” Hanford said, “if I’m going to fact-check AI, I just have to do the work myself.”
In the AP report, 69.6% of the respondents said they or their organization used AI for Content Production regularly, while 21.5% said they regularly used it for Information Gathering. Hanford’s concerns suggest that if AI tools are as unreliable as she believes, their widespread use for content production and information gathering could be a significant waste of time for journalists, who must still verify facts independently to ensure accuracy.
Dr. Michael Spikes, a Medill researcher and practitioner in news media literacy, said he feels similarly about the tools. For over 15 years, Spikes has been developing a curriculum on news media literacy. “I wouldn’t trust anything I get out of it without rechecking,” Spikes said, “here’s always a need to verify the information.”
As the news media industry continues to integrate generative AI into their workstreams, student journalists must learn how to use these tools in their work. These tools are already widely adopted by reporters and writers, and the number of media companies taking advantage of LLMs will only increase as the models improve.
A common sentiment from educators nationwide is that AI is here to stay, and the next generation of journalists needs training in the core tools before entering the industry.
“I think we should be a little bit more assertive in terms of exposing students and teaching them about the AI tools,” Caplan said, “we should be having students think about the capabilities and limitations of AI.”
However, for many professors, the fundamentals of journalism are relatively unchanged with the introduction of AI. The responsibility of gathering information from human sources, writing with a distinct voice, and creating entertaining and readable content still falls squarely on the shoulders of the journalists.
Gordon Dickson, an assistant professor at the University of Missouri School of Journalism and a news editor at the Columbia Missourian, said students will “be able to have machines do a lot of that work for them. But I think there will still be value in humans knowing how to write. And I don’t think that’ll change.”
The core challenge for journalism education is balancing embracing technological advancements and maintaining foundational journalistic skills. While AI offers opportunities to enhance reporting and streamline workflows, it cannot replace the critical thinking, ethical judgment, and human connection central to journalism.
Universities and other institutions for training journalists will need to ensure that their students get a taste of what great journalism looks like and know the tools at their disposal. Professors and administrators are still trying to navigate how to balance those learnings and how to approach them at the same time.]]></content:encoded>
</item><item>
<title><![CDATA[The Potential of AI Voices: Transforming News Delivery and Engagement]]></title>
<link>https://generative-ai-newsroom.com/the-potential-of-ai-voices-transforming-news-delivery-and-engagement-f4822aed71d1?source=collection_home_page----df04cefca135-----27-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Realtime Api]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Podcast]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Michael Crystal]]></dc:creator>
<pubDate>2024-10-29</pubDate>
<atom:updated>2024-10-29</atom:updated>
<content:encoded><![CDATA[The Potential of AI Voices: Transforming News Delivery and Engagement
Michael Crystal
Follow
6 min read
·
Oct 29, 2024
--
Listen
Share
Generated using ChatGPT
Journalists have been using audio AI tools for some time. Software like 
Otter.ai
 allows journalists to transcribe interviews easily, and 
Natural Readers
 can read an article aloud for those who prefer listening.
Recently, tech giants OpenAI and Meta have made major strides in voice and speech AI. Meta 
announced
 that their AI will respond in a voice that closely mimics celebrities and OpenAI rolled out a new 
API
 that allows users to speak with their models in real-time.
The need for improvement in radio and podcast news is clear — 
50% of U.S. adults
 never get news from these mediums, with only 9% using them often. Adults aged 18–29 are the least likely to use audio news, with just 2% preferring radio.
With these advancements from OpenAI and Meta, newsrooms may adapt and build new features to enhance the reader and listener experience. This post explores how newsrooms may use news voice and speech AI technologies to attract more consumers and improve their experience.
Meta Celebrity Voice AI
Meta’s advanced AI featuring celebrity voices offers a unique, personalized way to interact with various forms of content. Interacting with this AI is straightforward: users simply choose from a selection of celebrity voices, and the AI generates spoken content in the chosen voice.
This AI could open new avenues for how news is delivered and consumed, offering a more personalized and engaging approach to news:
1. Personalized News Briefings
Newsrooms could develop a feature that allows users to select from a variety of celebrity voices for their 
daily news briefings
. This could attract listeners who might prefer the familiar tone of a celebrity voice, adding entertainment to routine updates. The ability to choose between different voices could make news consumption feel less like reading a formal report and more like listening to a favorite podcast.
Example
: Imagine getting your morning news update from Judi Dench’s sophisticated narration or having John Cena recap last night’s sports highlights in his energetic style. This customization could appeal especially to younger audiences or those seeking a more engaging way to catch up on the day’s events.
2. AI-Generated Podcasts and Explain-It-Like-I’m-Five Segments
Creating podcasts or explainer segments
 where familiar voices tackle complex topics is another avenue for newsrooms to explore. AI-generated voices could break down difficult subjects like economic policy or climate science into digestible audio content, making it easier for listeners to understand and stay engaged.
Example
: A weekly political recap podcast, “hosted” by the AI voice of Awkwafina, could bring a humorous and relatable tone to otherwise dry policy discussions.
These AI voices could make listening to complex subjects feel more like chatting with a friend than sitting through a lecture, which could be a game-changer for news outlets looking to attract new listeners and maintain engagement over longer episodes.
While some newsrooms work hard to make their human-hosted podcasts engaging, with hosts using a conversational tone, human narrators have limits. They can only appeal to certain audiences.
AI-generated voices offer a way to broaden that appeal. By allowing listeners to choose from various voices, such as a favorite celebrity or a humorous tone, AI could engage those who don’t connect with traditional hosts. Combining human and AI voices could help news outlets reach more people and make complex topics more engaging for a wider audience.
By adding personality to real-time information delivery, Meta AI’s technology could offer a unique edge for newsrooms aiming to create memorable, interactive content. For a younger generation who seems hesitant to consume news in this medium, this could be a game changer.
Realtime API
OpenAI’s Realtime API offers exciting possibilities for media companies aiming to enhance how they deliver and interact with live content. This new feature enables AI models to process live audio streams in real-time and offer immediate responses.
It operates by receiving continuous audio input, which is instantly analyzed and converted into text or other outputs with minimal delay. This process happens as the audio flows, allowing the API to provide near-instant feedback without the need for batch processing or waiting for the stream to finish.
The API’s low latency ensures smooth, real-time interaction, making it adaptable to various live audio environments like broadcasts, phone calls, or events. By breaking down audio into manageable segments, it processes each piece as it arrives, delivering responsive and seamless output.
Here are some innovative ways newsrooms could leverage this technology:
1. Instantaneous Interactive Reporting
With the Realtime API, media companies could offer 
real-time Q&A sessions
 that let audiences interact directly with ongoing events. This capability could transform the way live coverage is delivered, enabling users to get immediate answers to their questions while following news developments.
Example
: Imagine a live stream covering a natural disaster or political event, where users could ask, “How many people have been evacuated so far?” or “What’s the latest update on the vote count in Georgia?” The Realtime API could pull relevant data and respond immediately, turning the coverage into an interactive experience. This level of engagement could make audiences feel more connected to the unfolding story, providing a sense of involvement that traditional broadcasts cannot match.
2. Adaptive News Summaries as Stories Develop
The Realtime API could also enable 
live-updating summaries
 of evolving news stories. Rather than waiting for complete updates, audiences could see summaries that adjust as new information arrives, keeping them up-to-date without needing to refresh the page or watch long broadcasts.
Example
: For instance, during a major breaking news event like an election or a court ruling, the Realtime API could continuously update a summary of key points, providing bite-sized, timely updates. This would allow media outlets to offer a dynamic “live blog” experience that is more efficient and accessible for readers who want the latest information without sifting through a full article.
This kind of real-time adaptability ensures that audiences are never behind the curve, making the news feel fresher and more immediate.
3. Real-Time Fact-Checking During Live Broadcasts
Newsrooms could use the Realtime API to integrate 
instant fact-checking
 into live coverage, a crucial tool for maintaining credibility in today’s environment. As reporters cover press conferences or live events, the API could flag inaccuracies in real-time, allowing journalists to correct the record almost immediately.
Example
: During a political debate, the Realtime API could instantly verify facts or identify discrepancies between a candidate’s statement and known data. This capability could be integrated directly into a broadcast, offering viewers real-time fact-checking graphics or updates. Such a tool would give newsrooms a significant edge in the race to provide accurate, up-to-the-minute coverage, reinforcing their role as trusted sources of information.
4. Enhanced Interview Assistance for On-the-Go Reporters
For journalists conducting live interviews or reporting from the field, the Realtime API could serve as an 
on-the-fly research assistant
. This feature could suggest relevant questions or provide background information in real-time, enabling reporters to dive deeper into the conversation and adapt their approach as new information emerges.
Example
: During a live interview, a journalist might need quick context on a topic the interviewee mentioned. The Realtime API could pull up relevant data or suggest follow-up questions, helping the journalist keep the interview flowing smoothly. This could enrich live reporting, making it easier for journalists to probe deeper.
As media companies strive to stay relevant and competitive in a rapidly evolving digital landscape, Meta’s celebrity voices and OpenAI’s Realtime API offer significant advantages. These tools go beyond traditional AI applications, providing customizable voices and real-time responsiveness that could enhance audience engagement, streamline reporting, and position newsrooms as leaders in innovative storytelling. These high-level capabilities would allow media companies to tailor their news delivery and interaction with audiences, making news consumption more personalized and immediate.
These advancements are not just about making existing processes faster — they’re about creating new ways for audiences to interact with news. The ability to ask questions in real-time, select a familiar voice to narrate daily briefings, or receive instant updates as stories evolve could make the news feel more immediate, accessible, and relevant.
The future of journalism lies in properly leveraging these AI-driven technologies to meet audience demands for immediacy and customization. As newsrooms explore these cutting-edge capabilities, they have the potential to reshape the way people experience news, making it more relevant, accessible, and engaging.
As the next wave of AI tools arrives, newsrooms and media companies have the opportunity to redefine their relationship with audiences. These new technologies could bring a level of engagement that traditional methods struggle to match. Newsrooms that are willing to experiment with and adopt these technologies will be better positioned to engage audiences.]]></content:encoded>
</item><item>
<title><![CDATA[Testing OpenAI’s o1 Models: A Look at Chain-of-Thought Prompting for Journalism Tasks]]></title>
<link>https://generative-ai-newsroom.com/testing-openais-o1-models-a-look-at-chain-of-thought-prompting-for-journalism-tasks-f02404fa9098?source=collection_home_page----df04cefca135-----28-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Llm]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Research]]></category><category><![CDATA[Prompting]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2024-10-23</pubDate>
<atom:updated>2024-10-23</atom:updated>
<content:encoded><![CDATA[Testing OpenAI’s o1 Models: A Look at Chain-of-Thought Prompting for Journalism Tasks
Nick Hagar
Follow
8 min read
·
Oct 23, 2024
--
Listen
Share
New LLMs are touted as smarter problem-solvers — but how do they fare in real-world journalistic tasks like data visualization and headline selection?
Examples of visualizations produced by LLM-generated code in this evaluation
Last month, OpenAI released two new large language models: o1-preview and o1-mini. These are what the company calls 
“reasoning models”
, an important distinction from the large language models we’ve seen so far. Whereas LLMs like GPT-4o generate text directly in response to a user’s question, the o1 models first build a step-by-step plan, then execute it to reach a solution. This process allows o1 to handle more involved problems: According to 
benchmarks
, these models improve on GPT-4o for programming, math, and other complex reasoning tasks.
When might these new models be useful for journalists? As with any LLM, it’s tricky to translate benchmarks into real-world performance. And this is even more true for the o1 models — OpenAI’s 
documentation notes
 that “GPT-4o is still the best option for most prompts,” but that o1 “may be helpful for handling complex, problem-solving tasks in domains like research, strategy, coding, math, and science.” To figure out what that means in practice, it’s helpful to identify a few common journalism tasks that we can use to evaluate o1.
Complex problem-solving is central to many aspects of journalism, but not all tasks are equally feasible to evaluate. Drawing from Charlotte Li’s 
journalism task taxonomy
, we can group tasks into six categories: gathering information, sensemaking, editing, publication and distribution, productivity, and journalism training. Most of these require a model embedded in a newsroom for proper evaluation, which makes them challenging to assess in isolation.
Sensemaking, however, involves reasoning-intensive tasks around processing information, which are feasible to evaluate in a controlled setting. With that in mind, I chose two core sensemaking tasks for this evaluation: 
data analysis
 and 
headline selection
.
Evaluation tasks
The prompts, analysis code, and model outputs for all evaluations are available in the 
project repo
.
Data analysis
Data analysis
 — the process of cleaning, processing, and visualizing an unfamiliar dataset — is a time-consuming but essential part of reporting. It’s also a promising application of LLMs: 
Research from Joris Veerbeek and Nick Diakopoulos
 has demonstrated the potential of LLMs for analyzing datasets, and OpenAI 
highlights
 code generation as a strength of the o1 models.
I wanted to evaluate the models’ ability to carry out end-to-end analysis tasks, starting with a spreadsheet and ending with a clear visualization. To do this, I selected several 
popular datasets
 from data.gov: the Department of Agriculture’s 
estimated fruit and vegetable prices
, 
electric vehicle registration in Washington State
, and 
motor vehicle collisions
 in New York City. For each dataset, I prompted the models to generate Python code for a different type of visualization. I structured these visualization tasks in increasing complexity:
Produce prices:
 A scatter plot showing the relationship between retail price and cup-equivalent price, with a line of best fit.
EVs:
 Side-by-side histograms, showing the range distributions for battery EVs and plug-in hybrids.
Collisions:
 A heatmap of collisions by NYC borough.
I evaluated each model’s output on the following criteria:
Did the code run?
Did the output match the requested figure?
Were there any data cleaning errors?
How many lines of code did the model require to complete the task?
Headline selection
Many newsrooms leverage 
multivariate headline testing
 to compare the performance (e.g., clickthrough rate) of multiple headlines for the same story, to see which one resonates best with readers. As our 
prior research
 shows, predicting the outcomes of these tests is often difficult, requiring strong editorial judgment and familiarity with audience preferences. A reasoning model may be well-equipped to weigh these factors when evaluating headlines.
To test this, I randomly sampled 50 headline tests from the exploratory dataset in the 
Upworthy Research Archive
. This dataset contains tests run by 
Upworthy
, a digital publisher, between 2013 and 2015. I prompted the model to select the best headline from each test, then measured its accuracy compared to the headlines that editors selected.
Comparison LLMs
To judge the relative effectiveness of the o1 models, I also evaluated GPT-4o, GPT-4o-mini, and Llama3.1–7b, the latter to assess the performance of a smaller model that could run locally.
This comparison matters because, to the extent that these models can compete with o1, they represent significant cost savings. At the time of writing, o1-preview costs $15 per million input tokens and $60 per million output tokens. That’s 6x as expensive as GPT-4o, and 100x as expensive as GPT-4o-mini (and infinitely more expensive than a model you can run on your laptop).
In addition to single-step prompts for each task, I explored multiple prompting strategies to see if these less expensive models could compete. This was motivated by o1’s 
“reasoning” capabilities
, which leverage a technique called 
chain of thought
. Chain of thought is a prompting strategy that elicits intermediate reasoning steps from an LLM — rather than answering a request immediately, the model first outputs its “thought process.” It also 
improves
 LLM performance on some tasks, even without any changes to the underlying model. While o1 is optimized for this approach, you can use chain of thought with any LLM (
here
 is an example with llama3.1–7b).
Knowing this, I tried three strategies to elicit more involved “reasoning” from the non-o1 models:
A 
chain of thought
 system prompt, which instructs the model to think through problems step-by-step, adapted from the open source 
optillm
 library.
A 
multi-step reasoning strategy
, which allows the model to mimic o1’s approach, adapted from the open source 
g1
 project.
Hand-crafted multi-step workflows
, to assess the benefit of reasoning driven by a human in the loop.
Results
Data analysis
The table below summarizes whether each model successfully completed each analysis. For a comprehensive breakdown of performance by model and prompting strategy, see 
Appendix A
.
At a high level, while o1 is generally more effective than other models for these analysis tasks, it isn’t strictly the 
best
. The smaller models struggle with more complex tasks, regardless of prompting — only GPT-4o and o1-preview are able to complete all three. But on the whole, GPT-4o with chain-of-thought prompting produces comparable visualizations to o1. Here’s a comparison of their collision heatmaps:
Collision heat maps generated by GPT-4o (left) and o1-preview (right)
Llama-3.1–7b struggled the most, only completing the simplest task (fruit prices) with chain-of-thought prompting. Still, this shows that even small, local models can handle some analysis tasks — promising for reporting workflows that prioritize efficiency and privacy.
This also underscores the broader effectiveness of chain-of-thought prompting. For non-o1 models, it produced the most accurate figures in 56% of cases, compared to 44% for single prompts, 33% for generated reasoning chains, and 22% for hand-crafted reasoning. While hand-crafted reasoning was less effective overall, it excelled in specific cases — such as the fruit prices analysis with GPT-4o-mini, where it generated the most thorough data cleaning and revealed an interesting trend in dried fruit prices:
A scatter plot from GPT-4o-mini that incorporates produce type
Overall, while the o1 models are generally effective for data analysis, they don’t exceed the capabilities of other models enough to justify their added cost. Smaller models struggle with complex tasks, but with the right prompting — especially chain-of-thought — even local models like Llama-3.1–7b can handle simpler analyses.
A/B headline tests
When evaluating the models’ ability to predict winning headlines in A/B tests, the o1 models consistently underperformed: Both o1-preview and o1-mini achieved an accuracy of just 24%. In contrast, GPT-4o and GPT-4o-mini achieved the highest accuracy at 34%, with chain-of-thought prompting for GPT-4o and hand-crafted reasoning chains for GPT-4o-mini yielding the best results (see 
Appendix B
 for details on all prompting approaches).
This performance gap suggests that o1’s reasoning capabilities may not translate well to tasks requiring editorial judgment or audience engagement predictions. While all models struggled with this task, prompting also makes a clear difference. For the GPT models, hand-crafted reasoning chains boosted accuracy more than in the code generation tasks, demonstrating that these workflows can be useful in editorial contexts. Chain-of-thought prompting again increased accuracy across most models, reinforcing its value as a versatile strategy for improving performance on reasoning-intensive tasks.
Conclusion
In constructing this evaluation, I tried to pick tasks that would benefit from o1’s reasoning capabilities — tasks that required combining multiple logical steps to produce a data visualization, or that required the model to reason about newsworthiness and audience engagement. But the results shown here echo 
OpenAI’s general guidance
: For most tasks, o1 should not be your go-to model. While it’s as good as GPT-4o in some areas, it lags behind in others (and does so at a 
steep premium
).
This evaluation highlights the importance of intentional prompting strategies. Chain-of-thought prompting, in particular, is a powerful tool — refining your prompts can significantly boost performance, even for smaller, local models. By guiding the model to reason step-by-step, you can elevate its ability to handle more complex tasks.
Equally important is task-specific evaluation. While newer models may excel in benchmarks, their performance doesn’t always translate to every task. The only way to determine which model is best for your use case is to develop a systematic, real-world evaluation tailored to the specific challenges you’re addressing. This requirement makes it hard for practitioners to find the best model for their needs, creating an opportunity for future work to develop journalism-specific benchmarks for evaluating LLMs.
There are limitations to consider. The o1 models are new, and as users learn how to prompt them more effectively, their performance may improve. OpenAI has also 
indicated
 that o1 will evolve rapidly, meaning these findings could change with future updates. And this evaluation covers only a small subset of journalistic tasks, and different models may excel with different datasets or tasks.
In their current form, o1 models show promise in 
some areas
 but are not yet the most reliable option for journalism tasks. Their reasoning abilities are impressive, but they often fall short compared to more established models like GPT-4o. The key takeaway is that success with any model hinges on thoughtful prompting and thorough evaluation. As o1 continues to evolve, reassessing its strengths and weaknesses will be crucial to understanding its real-world applications.
Appendix A: Data analysis evaluation
Fruit price scatterplot
Electric vehicle histograms
NYC collision heatmap:
Appendix B: A/B test evaluation]]></content:encoded>
</item><item>
<title><![CDATA[Anticipating AI’s Impact on Future Information Ecosystems]]></title>
<link>https://generative-ai-newsroom.com/anticipating-ais-impact-on-future-information-ecosystems-b97dffb9cc18?source=collection_home_page----df04cefca135-----29-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Foresight]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[AI]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2024-10-07</pubDate>
<atom:updated>2024-10-07</atom:updated>
<content:encoded><![CDATA[Anticipating AI’s Impact on Future Information Ecosystems
Developing foresight from hundreds of written scenarios
Nick Diakopoulos
Follow
12 min read
·
Oct 7, 2024
--
4
Listen
Share
The proliferation of generative AI technology has created a new drive in the media industry to understand how it can strategically adapt. What are the best ways to leverage the technology for efficiency in news production or to drive new experiences for audiences? But also: What are the threats from technology actors? Will legacy media suffer yet another wave of disintermediation from audiences? What happens to journalism ethics in all of this technological adaptation?
And so I was pleased to see the Open Society Foundations (OSF) running the 
AI in Journalism Futures project
 earlier this year. In the ambitious first phase of the project the organizers put out an open call for participants to write future-looking scenarios to explore the potential driving forces and implications of AI for the broader media ecosystem. What might happen with various stakeholders in 5, 10, or 15 years from now? As some of our 
own research
 on 
scenario writing
 shows, scenarios are an excellent way to capture a broad and diverse set of perspectives on an issue. No one is trying to predict the future here, but by understanding a range of plausible alternatives, this can inform strategic thinking 
now. 
In total, more than 800 people from about 70 countries around the world wrote and submitted short scenarios. The AIJF project went on to use these as a premise for a workshop and to refine ideas, which they 
detail in their report
, but in this post I want to go back to that very broad set of initial scenarios, which OSF was kind enough to share with me in anonymized form.
In particular, here I explore (1) the range of 
types of impacts
 that were identified in the scenarios, (2) the 
timeframe
 
of impacts
 in terms of whether certain impacts are associated with a short, medium, or long-term timeframe, and (3) 
global differences
 such as whether different parts of the world focused on different types of impact. Obviously there are many many more questions that you could ask of this data, such as the types of drivers of impacts, the final outcomes and their severity, the stakeholders mentioned, or the technical capabilities that scenario writers focused on. But in this analysis I’m going to focus on impacts.
Refining The Data
Starting from the 872 scenarios there was a bit of cleaning, filtering, transformation, and verification that I needed to do before analysis. For starters, a few of the scenarios were too short (< 50 words) to meaningfully analyze (20 of them). Filtering those out left me with 852 scenarios to analyze. 14 of the scenarios were not written in English and so I used the Google Translate function in Google Sheets to translate those to English. To be able to analyze geographic distribution and timeframe I also had to do some transformations, 
mapping
 the country of the scenario writer in the original data to continents, and manually converting the free text “timeframe” field, which respondents used to indicate how far in the future their scenario explored, into a number representing years in the future.
Next I had to extract the impacts from each scenario. To do that I used an LLM (GPT-4 in this case) that I prompted to analyze the text and output a set of discrete impacts that had been mentioned (see the Appendix for the system and user prompts). Getting the prompt right took a bit of iteration. Probably most important was a careful conceptualization of what an “impact” actually is. The definition I arrived at was “An impact is a significant effect, consequence, or outcome that an action, event, or other factor has in the scenario,” which I based on synthesizing a few dictionary definitions. Impacts, then, are not necessarily an “end state” of a scenario but could be intermediate outcomes as well. I also prompted GPT-4 to extract impacts that were distinct from one another, and there could be an arbitrary number (or none at all) extracted from any given scenario. Each extracted impact was represented by a one sentence description, e.g. “The proliferation of flawed AI systems leads to a compromised information ecosystem, causing a general doubt in the reliability of all information,” and a short label, e.g. “Compromised Information Ecosystem.”
To check that the impact extraction was working as expected I took a random sample of 5 scenarios and manually checked the 18 impact descriptions extracted from them in terms of (1) whether they were valid according to the definition, and (2) whether each extracted impact was indeed present in the scenario and was accurate in terms of how the impact was conveyed in the scenario. 100% of the impacts extracted passed these checks and so at that point I felt more confident and scaled up the analysis to the whole dataset. This resulted in the extraction of 3,445 impacts from the 852 scenarios.
To build a typology of impact types based on the 3,445 impact descriptions I turned to a new method for using LLMs to do qualitative thematic analysis introduced in a 
recent paper and embodied in a tool called LLooM out of Stanford University
. It works by distilling the input texts, clustering those representations, and then synthesizing concepts which capture abstract connections in the clusters. Each extracted concept also includes a definition that can be used in a prompt for an LLM to score each original input text according to whether it reflects the concept. So, for instance, for a concept detected like “AI Personalization” the process also outputs the scoring definition, “Does the text discuss how AI personalizes content or enhances user engagement?” Using these scoring definitions each impact description can be scored against each concept to tabulate the frequency of occurrence.
Impacts of AI on Media Ecosystems
Using LLooM as described above I arrived at a set of 19 impact themes and their associated scoring definitions:
AI Personalization
: Does the text discuss how AI personalizes content or enhances user engagement?
AI Fact-Checking
: Does the text mention the use of AI for fact-checking or combating misinformation?
Trust Erosion
: Does the text describe how AI or technology impacts trust in media, institutions, or information?
Political Manipulation
: Does the text explore the influence of AI on political campaigns, elections, or public opinion?
Verification Difficulties
: Does the text discuss challenges in verifying the authenticity of information due to AI technologies?
AI in Data Journalism
: Does the text highlight the use of AI for data analysis, insights, or enhancing data journalism?
AI Enhancing News Delivery
: Does the text describe how AI technologies improve the delivery, distribution, or accessibility of news?
AI Tools for Efficiency
: Does the text explore how AI tools are used to increase efficiency or productivity in journalism?
Media Adaptation
: Does the text discuss how traditional media adapts to digital or technological changes?
Journalism Business Models
: Does the text describe changes in journalism business models due to technology?
Audience Engagement
: Does the text explore how technology impacts audience engagement or consumption patterns in media?
News Production Shifts
: Does the text detail shifts in news production or reporting methods due to technological advancements?
Media Platform Dynamics
: Does the text discuss the impact of technology on the dynamics between different media platforms?
Content Accessibility
: Does the text address how technology affects the accessibility and distribution of content?
AI Content Production
: Does the text discuss how AI is utilized to automate or enhance content creation processes in media or journalism?
AI Ethics
: Does the text address ethical issues, biases, or dilemmas introduced by AI in technology or content dissemination?
AI and Journalism Careers
: Does the text explore how AI technology influences job roles, skills, and employment in journalism or media?
AI in News Production
: Does the text describe the impact and integration of AI in journalism practices, newsroom operations, or news production?
AI and Misinformation
: Does the text discuss the role of AI, including deepfake technology, in spreading misinformation or impacting the authenticity of information?
Somewhat unsurprisingly, scenarios are articulating impact themes related to how AI intersects with fact checking, trust, and misinformation, as well as ethics and labor concerns, business models, and several other issues. The concepts aren’t always as distinct as I’d like. For instance, if I were doing a manual thematic analysis I might not have separated “AI in News Production” and “AI Content Production”. I’m also not sure whether it’s worth separating “AI in News Production” and “News Production Shifts”. “AI Enhancing News Delivery” and “Content Accessibility” are clearly related if you look at the scoring definitions. I wouldn’t publish this typology in an academic forum, because there’s still some haziness around distinguishing some of these concepts. At the same time, I didn’t need to spend dozens or hundreds of hours to get a quick overview of concepts in the dataset, and the categories do seem meaningful, even if they are not entirely disentangled.
The next figure shows the prevalence of the impact themes based on the entire set of 3,445 extracted impact descriptions and the use of GPT-4 to rate each scenario. “Political Manipulation” is all the way at the bottom, indicating that not many scenario writers focused on it. “AI Fact-Checking” was only half as prevalent as “AI and Misinformation” suggesting that more scenario writers lean towards a future where AI is used to proliferate misinformation rather than fight it. On the other hand, themes like “AI Ethics”, “AI in News Production” and “Content Accessibility” as well as “AI Content Production” and “News Production Shifts” were quite prevalent in the scenarios. The most frequent themes connect to how AI will impact core tenets of journalistic activity: production, distribution, and ethics.
The distribution of 19 impact themes.
It’s also interesting to compare to the impact themes that were 
mentioned in the OSF report
. For instance, the OSF report highlights personalization and misinformation but these were, in fact, far less prevalent than other impact themes in this analysis. The report also highlights themes like the rise of AI agents and audience fragmentation which did occur in the underlying impact descriptions but were not prevalent enough to form large clusters. In this case, AI “agents” were mentioned in only 0.7% of impacts, and “fragmentation” was also only mentioned in 0.7% of impacts. (Feel free to examine the 
underlying extracted impacts
).
Because there could be interesting yet less prevalent impacts not captured by the 19 themes, I re-ran the clustering with a smaller minimum cluster size. This resulted in hundreds more concept themes, some of which provide more insight into long(er) tail issues. For instance, Language Barrier Reduction and Global Understanding are positive visions for how generative AI could facilitate more information flow between languages, while Deaf Community Support and Visual Impairment Aid point to how AI could help to make content accessible for marginalized audiences. Energy Consumption captures the idea that there is an environmental impact as a result of generative AI’s use. Not only could AI result in Societal Fragmentation, the Erosion of a Shared Reality, and Privacy Concerns because of AI-based personalization, but also there is a potential for Bridging Polarization and for enabling Hyper-Local Content. Here we see both positive and negative impacts of AI personalization, sometimes pulling in opposite directions. Another idea that popped out at me from the long tail was Historical Figures: “Does the text example mention AI avatars of deceased public figures?” — while it’s unclear that a conventional news organization would pursue this idea, it is intriguing nonetheless.
While looking at the prevalence of impacts in larger clusters says something about where the current consensus is in how AI will impact media, these more niche kinds of impacts are helpful for identifying a wider set of long-tail possibilities that could act as early indicators, suggest areas where additional empirical research may be warranted, or inspire innovation.
Impacts Over Time and Around the World
How do the impacts mentioned in scenarios vary by the timeframe the writer chose to focus on, or the geographic location of the writer? Answering these questions speaks to whether some impacts are perceived to be on the short, medium, or long-term horizon, and whether some parts of the world are more (or less) intent on particular impacts. This can have bearing on how we think strategically about which impacts to tackle through innovation or investment, and who around the world may perceive the most benefit.
To look at the timeframe dimension, I used a Chi-Squared test to compare the distribution for all impacts over short, medium, and long timeframes against the distribution for each individual impact over timeframes. This indicates if there’s an impact which is skewed toward any of the three timeframes in comparison to the overall distribution. I found that “AI Personalization” skews slightly towards being a long-term issue (> 10 years in the future), and that “Journalism Business Models” skews slightly towards being a medium and long-term issue (> 5 and > 10 years in the future). Both “AI Fact-Checking” and “AI and Misinformation” skew towards being short-term issues: more scenario writers mentioned these impacts in scenarios that were less than 5 years in the future. For instance, overall, 34.4% of impacts were from scenarios situated less than 5 years in the future, but 45.3% of “AI Fact-Checking” impacts were from that timeframe. Scenario writers recognized that these kinds of fact and misinformation impacts are near-term issues related to AI, likely because they see these impacts 
already
 playing out and so it’s not hard to extrapolate that these will continue to be issues in the near term.
I did a similar analysis to look at the distribution of impacts around the world by binning scenarios based on the continent where the writer was from. I found that, in comparison to the overall distribution, “AI Fact-Checking” was over indexed in scenarios with writers from Africa and Asia but was less common in scenarios with writers from Europe, or North and South America. At the same time, “AI and Misinformation” was mentioned less in scenarios from African writers, and more in those from Asian writers. So both African and Asian scenario writers appear to be optimistic about AI augmenting the fact checking process, but only Asian writers are more skewed towards describing misinformation impacts of AI in the media ecosystem. “Political Manipulation” was less common in scenarios from Africa and South America, but a bit more common in Asia. And “Journalism Business Models” was less mentioned in scenarios from Africa and Asia, but more in those from Europe.
Take Aways
Analyzing 852 scenarios from around the world, there’s a good bit of consensus on how AI will impact the media ecosystem over the next 5 to 10 or even 15 years: many of the core activities of news production and distribution are expected to change, and domain ethics will need to take account of those changes. Scenario writers thought AI could drive efficiency and turbo-charge data journalism, while also threatening journalism careers. Of course we already know these things as they’re happening in at least some media ecosystems 
now
. As William Gibson famously put it: “The future has arrived — it’s just not evenly distributed yet.”
In the longer tail, we see people writing about the potential for AI personalization, about the impact on business models, for enhancing activities like fact checking that are central to the accuracy mandate of journalism, and about the risk of political manipulation. And we see hints at positive potentials like reducing language barriers or enabling hyper-local content production. In future analysis it would likely be productive to look into the smaller clusters of impacts even more, running separate clusterings to generate smaller sub-clusters for each of the 19 high-level impact themes, or indeed to manually go one by one through the impacts to develop arbitrarily small clusters.
I did also find that there’s variance in terms of which impacts are perceived to be shorter vs. longer term. For instance misinformation-related impacts are considered more imminent threats, whereas personalization seems a bit more far-off. Personalization is interesting as it’s not the most prevalent theme, but also still not exactly rare, occurring in more than 400 of the impact descriptions. Combined with the longer term impact horizon this suggests it could be something to look out for and invest in now as a sizeable minority see it as a plausible path for AI to take in impacting media. Similarly, there are some differences around the world, which make sense given specific media ecosystem contexts and how different scenario writers see things evolving. For instance business models seems to be a topic that is particularly top of mind in Europe, but less so elsewhere.
These findings show that visions of the future impacts of AI are not entirely homogeneous, but are rather related to the individual perspectives and specific ecosystems of habitation that scenario writers bring to the task. And that’s an important finding, since one of the goals of such broad participation is to try to capture diverse perspectives, augmenting our ability to see how the impact of AI may play out in different media contexts worldwide.
Appendix
System Prompt:
“You are an expert data analyst specialized in the analysis of written scenarios. Your responses are always grounded in the specific written scenario provided to you.”
User Prompt:
“An impact is a significant effect, consequence, or outcome that an action, event, or other factor has in the scenario.
A driving force is something in the macro environment such as a social, economic, political, environmental, technological, or other societal factor that creates influence or impact in a scenario. In this scenario a driving force MUST be explicitly related to functionality available from AI, either current or potential.
Analyze the following scenario and extract 1 or more impacts that are present and easily apparent in the scenario. An impact may be caused by a driving force or another prior impact described in the scenario text.
For each impact provide a 1 sentence detailed description and a 2–4 word label that captures the essence of the description. Make sure the impacts you identify are distinct from each other and not closely related. If no impacts are present, output an empty list. Please respond ONLY with a valid JSON in the following format: {“impacts”: [{“description”: “<IMPACT_1_DESCRIPTION>”, “label”: “<IMPACT_1_LABEL>”}, {“description”: “<IMPACT_2_DESCRIPTION>”, “label”: “<IMPACT_2_LABEL>”}, … ]}.
###Scenario###”]]></content:encoded>
</item><item>
<title><![CDATA[The Copyright Battle between Authors and Generative AI Models]]></title>
<link>https://generative-ai-newsroom.com/the-copyright-battle-between-authors-and-generative-ai-models-b02fcc89aff8?source=collection_home_page----df04cefca135-----30-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Fair Use]]></category><category><![CDATA[Copyright]]></category><category><![CDATA[Copyright Infringement]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Nina Brown]]></dc:creator>
<pubDate>2024-10-01</pubDate>
<atom:updated>2024-10-01</atom:updated>
<content:encoded><![CDATA[The Copyright Battle between Authors and Generative AI Models
An overview of legal arguments from the idea/expression dichotomy to the fair use defense
Nina Brown
Follow
10 min read
·
Oct 1, 2024
--
Listen
Share
Image from Unsplash.
“
Rain beat against the courthouse windows as attorney Rachel Summers stood in front of the jury. The air in the courtroom was tense, thick with anticipation. Her client, Paul Jefferson, a mild-mannered software engineer, sat silently at the defense table, his hands clasped together, the weight of the trial etched deeply into his face
.”
This is the opening paragraph from 
The Silent Witness
, a short story about an attorney who defends a man accused of murder, only to uncover a deeper conspiracy that threatens to silence the truth. This gripping legal thriller was written by ChatGPT. And truly, it wasn’t half bad. The characters were compelling, there was a strong plot with an intriguing conflict, and satisfying conclusion. It might have been a story written by a student of John Grisham’s work, or maybe someone who’s read a lot of David Baldacci or Michael Connelly.
And in a sense, that’s exactly what this story is: a product written by a student of great legal writers. By this point, much has been written about how Large Language Models (LLMs) like ChatGPT 
work
. Central to that understanding is the premise that generative algorithms are only as good as their datasets. A large and robust dataset enhances an LLM’s ability to understand and generate human-like text, making it more accurate, versatile, and capable of handling a wide range of queries and topics. So it probably isn’t surprising that books by John Grisham, David Baldacci, and Michael Connelly — among other writers of legal fiction — are 
among the works
 in the training corpus for ChatGPT. That corpus allowed the LLM to learn the sequence of words and structure of language used in legal stories so that it could generate one itself. Thus, the inclusion of these works and others like them may be one of the reasons that ChatGPT was able to write such a compelling courtroom drama. Unfortunately, it has also led to a real-world courtroom drama.
The Scraping Claims
John Grisham, David Baldacci, and Michael Connelly are among 18 plaintiffs suing OpenAI (maker of ChatGPT) for copyright infringement. Their claim is simple enough: OpenAI “scraped” the web for data to train ChatGPT and copied works of the plaintiffs for inclusion in this set. “Scraping” is the term used to describe the process of automatically extracting data from websites to create or augment datasets used for training generative AI models. Because the right to copy is an exclusive right of the copyright owner, the plaintiffs (all authors and the Authors Guild) claim that OpenAI violated their rights when it copied their protected works during the training process.
These authors are not alone. This lawsuit is one of several in the same vein — the rise in generative AI programs has given way to 
more than a dozen lawsuits
 alleging copyright infringement so far in the past two years on the same basis. News organizations, music publishers, authors, and other content creators have all made essentially the same claim against OpenAI, Stability AI, Microsoft, Google and other AI developers: those companies have scraped large amounts of the web, and in doing so made copies of the plaintiffs’ works, for the purposes of training their models. They assert that this violated their copyrights.
[1]
Is there merit to these lawsuits? Absolutely. Is it as clear cut as the plaintiffs’ claim? Not one bit. Sometimes copyright infringement is pretty clear cut. A musician uses another artist’s melody as the basis of their song, or a company uses downloaded images from the internet for promotional purposes. But often the determination of whether there has been infringement is more complicated. Not all copying is infringement, even when a defendant 
makes a profit
 from their use of a plaintiff’s work, or 
uses a large quantity
 of it. Whether there has been a copyright infringement is a case and fact-specific inquiry, and rapid innovation in this field has created uncertainties in how copyright law applies in this context in the first instance. In fact, the United States Copyright Office is undertaking a 
study
 of the copyright law and policy issues raised by artificial intelligence which includes an examination of the legal implications of training AI models on copyrighted works.
This won’t be the 
first time
 that copyright law has had to respond to a 
new technology
, and we’ll get a sense of how the law will adapt to this area as the cases make their way through the federal court system. Understanding some of the nuances of copyright law will help both understand and predict how these cases are likely to pan out.
The Battle Over What Is Copied
The plaintiffs paint this as an open and shut case of copyright infringement. The 
defendants
 so far have denied that their uses of plaintiffs’ works constitute infringement and instead claim that they are fair uses. The fair use argument is without question the strongest for the defendants, and one that we will return to shortly. But there’s another defensive position that OpenAI has taken that also merits mentioning here.
In both its court filings and in response to the United States Copyright Office’s request for public comments about the copyright law and policy issues raised by artificial intelligence, OpenAI has suggested that the idea/expression dichotomy should bar plaintiff’s claims. In copyright law, only the expressions of ideas are protected. Copyright law 
doesn’t protect facts or concepts
. This is understood as the idea/expression dichotomy: the expression of an idea can be protected by copyright law, but 
not the idea itself
. This explains why there can be 20 (or 200) books about the same exact topic. The way the ideas and facts within the book are communicated can be protected, but not those underlying facts and ideas themselves.
OpenAI has argued that in fact it 
hasn’t 
made duplicate copies of plaintiffs’ protected works. Instead, it argues that the model stores only the information from copyrighted works that is necessary for it to “learn” how to respond to prompts with either text or an image. Specifically, 
it has argued
 that the LLM underlying ChatGPT stores only the structure of the language itself, such as the syntax and grammatical function of words (e.g. pronouns, verbs, adjectives, etc.) as well as the probabilistic data about the chronology of words so that its model can accurately predict the next set of words in a given sequence. In other words, OpenAI has argued that it 
copies only the information from the expression and not the copyrightable expression itself
. It has been 
clear
 that “[d]espite a common and unfortunate misperception of the technology, the models do not store copies of the information that they learn from,” but instead as the models “learn” during the training process and become better at predicting the next word, that information is incorporated into the model itself. OpenAI compares it to a person who has read a book and set it down, 
explaining
 that its “models do not have access to training information after they have learned from it.”
It’s true that generative AI programs don’t copy protected works during the training process in order to reproduce those works. (At least not intentionally.) That’s 
not how these generative AI products work
. Instead, they are copying the data from the work. How words are arranged to form poems. How pixels are arranged in relation to one another to form an image of a horse. And the argument that copyright does not protect that information — the way words or pixels are arranged — is on the money. But what this argument misses is that 
at some point
 during the scraping process, the copyrighted work must be copied at least temporarily in order to process it. Even if only the unprotectable components of the work are retained thereafter, the entire work must be copied in the first instance in order for the LLM training process to be carried out. And that copying is prohibited under copyright law.
There may be valid normative arguments that this 
type
 of copying is very different from what copyright law has traditionally prohibited and thus should be permitted under the law. For example, when someone duplicates a photograph from the internet to a printed piece of paper, the medium changes but the photograph remains the photograph. When a musician uses a melody someone else wrote, it may be accompanied by different words and instruments, but it’s the same melody. When a work is copied as part of the training process for an LLM, the output of the LLM is not essentially the same as the original. Depending on the prompt, the response may bear no relation at all to the works used during training.
This argument appears in early court filings by defendants in generative AI cases, and it’s something the courts will need to address in the near term. Either way, OpenAI and the other defendants have a much better defense.
The Fair Use Defense
Even if the courts agreed with plaintiffs that scraping their content constituted making a copy, the defendants can still win based on the fair use defense. Fair use allows limited use of copyrighted material without permissions under certain conditions in an effort to balance the rights of creators with the public’s interest in accessing and using creative works.
The fair use doctrine is 
codified
 in the Copyright Act, and directs courts to consider four factors in making the determination of whether a defendant’s use is a fair use. It is a balancing test that is designed to be fact and context specific and doesn’t give way to bright line rules. And ultimately, only a federal court can make the determination of whether a particular use is a fair use.
This means that even though legal experts can opine on whether something will or won’t be a fair use based on the four-factor test and relevant precedent, it’s only just a guess until a federal court issues a determination on that specific question. In the context of LLMs, each LLM may be different in the ways it scrapes and incorporates bits of data for training. Those differences may be significant in a fair use inquiry, meaning that scraping or training as a general process won’t necessarily be seen as a fair use (or not) but rather the court will determine whether a particular defendant’s scraping 
process
 constituted a fair use.
OpenAI has 
argued
 that its practice of copying a work to extract its unprotectable aspects constitutes fair use. In support, it has pointed to cases
[2]
 where courts have permitted the intermediate copying of a protected work under this doctrine. However, the copying in these cases is distinguishable from scraping other forms of content in important ways.
Each case cited by OpenAI involved the copying of computer programs. While computer programs themselves are protected by copyright, they often contain many functional elements and ideas that are not protected. (This is not dissimilar from books, where the individual words used are not protected by copyright, but the selection and arrangement of those words may be.) What distinguishes computer programs from books, however, is that they have what one court described as “unique problems” because in order for someone to gain access to the unprotectable elements in the computer programs at issue, they need to copy the 
entire
 program. In other words, in the cases cited by OpenAI, there was 
no other way
 for the defendants to gain access to the ideas and functional elements embodied in the copyrighted computer program other than through intermediate copying.
This isn’t the case with scraping content such as books or news articles. In the cases against OpenAI and other generative AI companies, the plaintiffs are creators of works of literature and art that one can easily access without first making a copy of the entire work. This makes them fundamentally different than computer programs that store unprotectable data.
The critical question is whether courts are willing to treat scraping the same way despite these differences. There is a policy argument that the generative AI defendants will likely make that supports such an outcome: in order for an algorithm to parse the unprotectable elements of these works, it must make a copy of the entire thing. (The fact that it can access the work without copying the work is not, in and of itself, enough to allow it to meaningfully use the unprotectable elements.)
It’s possible that Congress or the Copyright Office could buy this argument and, in an effort to further innovation, determine as a matter of policy that scraping for model training does not constitute an infringement so long as a copy of the work isn’t retained in the model. (This would track with the aim of the United States copyright laws, which is to stimulate artistic activity and innovation for the greater good.) However, it’s just as likely that these government entities could establish a licensing system for scraping works not in the public domain.
Either way, there is a stronger fair use argument the generative AI defendants will make. There are certain uses, known as “transformative uses,” that have a dramatic impact on the fair use analysis. A use is considered transformative if it adds to or alters the original work with a different purpose, meaning, or message, rather than simply replicating or adding onto it. Transformative works often provide commentary, criticism, or parody, or offer new insights or understanding that are beneficial to the public. And importantly, highly transformative uses are more likely to be deemed fair.
Arguably, even if a court agrees with plaintiffs in these cases against LLM’s that scraping includes copying their work in order for the LLM to learn information, the defendant’s use is undeniably transformative. In the case against OpenAI brought by authors referenced above, for example, the defendant trained its LLM not only on the words written by the plaintiffs, but 
trillions
 
of other words. And the LLM used all of those works — including plaintiffs’ — to help it understand how words fit together and work to form higher-level ideas. In other words, it learned from the facts, not the protected expression of those ideas.
And thus, in this defense of transformative fair use lies a neatly packaged argument: the purpose of an LLM is so transformative that it should not be possible to recognize the plaintiff’s work in the LLM’s output. Almost like they never copied it at all.
—
Nina Brown is an attorney and professor who teaches at the Newhouse School of Communications at Syracuse University. Her research focuses on the intersection of media law and technology.
[1]
 It is important to note that some plaintiffs have made additional claims of copyright infringement based on the 
output
 of generative AI products. This article focuses only on the scraping claims and not on those that depend on AI-generated content.
[2]
 Assessment Technologies of Wi, LLC v. Wiredata, 350 F. 3d 640, 644–45 (7th Cir. 2003); 
Sony Computer Entertainment, Inc. v. Connectix Corp.,
 203 F.3d 596, 602–08 (9th Cir.2000); 
Sega Enterprises Ltd. v. Accolade, Inc.,
 977 F.2d 1510, 1520–28 (9th Cir.1992).]]></content:encoded>
</item><item>
<title><![CDATA[How Teams of AI Agents Could Provide Valuable Leads For Investigative Data Journalism]]></title>
<link>https://generative-ai-newsroom.com/how-teams-of-ai-agents-could-provide-valuable-leads-for-investigative-data-journalism-ac48ece1fdab?source=collection_home_page----df04cefca135-----31-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Investigative Journalism]]></category><category><![CDATA[Generative Agents]]></category><category><![CDATA[Journalism]]></category>
<dc:creator><![CDATA[Joris Veerbeek]]></dc:creator>
<pubDate>2024-09-24</pubDate>
<atom:updated>2024-09-24</atom:updated>
<content:encoded><![CDATA[How Teams of AI Agents Could Provide Valuable Leads For Investigative Data Journalism
ChatGPT hasn’t quite hit the mark as an investigative reporting assistant — could an Agentic AI workflow offer a better solution?
Joris Veerbeek
Follow
9 min read
·
Sep 24, 2024
--
Listen
Share
Note: This post was co-authored with 
Nick Diakopoulos
.
In investigative data journalism, the final product is rarely the work of a lone reporter. Instead, it’s the outcome of a collaborative effort where data analysts, reporters, designers, developers, and editors combine their skills to turn data into a compelling story. Data journalism almost inherently requires teamwork — few individuals possess the range of skills needed to collect, interpret, and present data in a way that truly resonates with readers. It’s through this collective effort that the most impactful stories are brought to life.
Over the past year, there’s been growing interest in adding at least one more member to the team: generative AI. The Markup, for instance, 
experimented with using ChatGPT as a reporting assistant 
earlier this year. More recently, The Pudding even tried Anthropic’s Claude
 to produce an entire story from start to finish
 and graded it in the process.
The results were underwhelming. In The Markup’s case, the AI provided inaccurate information, lacked transparency in sourcing, and required frequent corrections or highly specific instructions. The AI used by The Pudding seemed to fare slightly better but still earned only a C+ as a reporter.
Both experiments relied on interacting with generative AI through chat interfaces. When we think of generative AI today, it’s often tools like ChatGPT that come to mind — simple interfaces where you ask a question and get an immediate response. But this kind of back-and-forth exchange may be too simplistic for the demands of investigative data journalism, which requires more than just quick answers.
The future of generative AI might be headed toward something a little more ambitious. Just as human journalism benefits from collaboration, AI could also advance through teamwork. Increasingly, research in AI is shifting toward what’s being called “agentic AI” — teams of AI agents that don’t just answer questions but actively plan, strategize, and collaborate on complex tasks. Both OpenAI and Google are
 reportedly
 actively working on it.
We’ve already seen early signs of what’s possible in other areas, like literary translation. 
In one recent study
, an agentic AI system operated with a senior editor, a proofreader, a localization specialist, and even a virtual CEO. The results were very promising, with translations from the agents often being preferred over those produced by human writers.
Inspired by
 
these developments, we’ve been experimenting with a similar agentic workflow for investigative data journalism over the past few months. Not to replace the newsroom or substitute for the expert judgment of journalists, but to provide reporters with interesting leads from large datasets that they could assess and choose to pursue if they felt further investigation was warranted.
Specifically, we developed a prototype system that, when provided with a dataset and a description of its contents, generates a “tip sheet” — a list of newsworthy observations that may inspire further journalistic explorations of datasets. Behind the scenes, this system employs three AI agents, emulating the roles of a data analyst, an investigative reporter, and a data editor. To carry out our agentic workflow, we utilized GPT-4-turbo via 
OpenAI’s Assistants API
, which allows the model to iteratively execute code and interact with the results of data analyses. In this blog post, we’ll provide a general overview of the system; more detailed information can be found in the 
pre-print of the short paper
 that will be presented at 
Computation + Journalism 2024
.
How it works
Basically, our system consists of a series of prompts organized into an orchestrated workflow (see the diagram below), divided into two main types of prompts: 
role-specific
 and 
task-specific
. The role-specific prompts act as the job descriptions of our AI agents, designed to replicate the roles of key players in a data journalism team — the analyst, the reporter, and the editor. Task-specific prompts, on the other hand, guide each agent through the nitty-gritty steps required to get the job done.
In our setup, the analyst is made responsible for turning journalistic questions into quantitative analyses. It conducts the analysis, interprets the results, and feeds these insights into the broader process. The reporter, meanwhile, generates the questions, pushes the analyst with follow-ups to guide the process towards something newsworthy, and distills the key findings into something meaningful. The editor, then, mainly steps in as the quality control, ensuring the integrity of the work, bulletproofing the analysis, and pushing the outputs towards factual accuracy.
Overall, these three agents collaborate through four stages:
Question Generation: 
First, a dataset and its description are provided to the reporter agent, which is tasked with brainstorming a set of questions (with the number adjustable) that could be answered using the data.
Analytical Planning:
 For each question, the analyst drafts an analytical plan detailing how the dataset can be used to answer the question. The editor provides feedback on the plan and the analyst redrafts.
Execution and Interpretation
: Each analytical plan is executed and interpreted by the analyst. The editor and reporter provide feedback, which the analyst incorporates, and the reporter then summarizes the final results in bullet points.
Compilation and Presentation:
 All bullet points from the previous step are compiled, and a subset of the most significant findings is presented to the user in the tip sheet.
Throughout these stages, the agents don’t just passively use each other’s outputs as inputs but actively have to incorporate each other’s feedback, particularly during the analysis phase. After the analyst completes its work in the third step, for example, the reporter steps in to assess these findings. The reporter is then prompted to choose between three choices: 1) give a green light for “publication” which signals that the insight should be bulletproofed and potentially shared with the journalist supervising the agents, 2) suggest further analysis to try to develop other angles, or 3) decide the findings aren’t newsworthy enough to pursue.
A complete overview of the pipeline. Each box represents a single prompt assigned to one of the three agents
If the reporter agent thinks the analysis is ready, the process moves ahead. But if more work is needed, which can often be the case, the reporter provides specific feedback — whether it’s a follow-up question or a new angle that needs exploring. The analyst then revisits the data, refines the analysis, and addresses the reporter’s concerns. This back-and-forth can happen several times, creating an iterative process. A complete list of the prompts used in the pipeline is available
 in the project’s GitHub repository.
How we tested it
Evaluation can be a difficult problem for generative AI — 
general benchmarks often don’t tell the full story
, and merely interacting with the system doesn’t provide a structural evaluation of its capabilities. That’s why we approached the evaluation of our system with a focus on real-world applicability. We tested our generative agents on five actual investigative data journalism projects, detailed in the table below. All the projects were nominated for either the Sigma Awards or the Philip Meyer Journalism Award, and we prioritized diversity in publication location, methodologies, and types of insights. Although we selected relatively complex projects because of our focus on award-winning work, we also had to exclude projects that required extensive computational resources, large datasets, visual image analysis, or a focus on geographical data, due to the constraints of the OpenAI Assistants API.
Investigative data journalism projects used for evaluation.
To measure the benefits of the agents approach, we further compared the tips they produced to a baseline model that lacked the collaborative setup of our agent system. Our evaluation focused on three metrics: 
validity
, 
potential
 
newsworthiness
, and 
precision
.
Validity is about ensuring that the insights generated by our system are sound and logically derived from the provided data. Newsworthiness assesses whether these insights have the potential to become compelling stories that would grab a journalist’s attention and warrant further investigation and development into a story. And finally, precision measures how closely our system’s tips match the key findings in the original published articles. In other words, did the system identify insights similar to those presented in the original article?
What we found was promising: overall, the results affirmed the benefits of our agents pipeline, with higher average scores across all three metrics — validity, newsworthiness, and precision. Particularly the improvement in newsworthiness was notable, as our agents consistently outperformed the baseline model across projects, with about two-thirds of the suggested leads being potentially newsworthy.
Validity was relatively strong, with overall scores roughly between 80% and 90%. However, the system encountered difficulties with the more complex tasks, such as analyzing datasets that required expertise in multilingual text analysis methods.
Finally, while the agents demonstrated some improvements in precision compared to the baseline, the average precision remained relatively low at 34%. This means that generally about one in three tips found by the agents also appeared in the final article. Considering the broader scope of these tips, this may also indicate the influence of editorial decisions, with certain leads aligning more closely with editorial priorities than others.
Even with somewhat lower precision, when taken together with the newsworthiness results, the results show that the process was surfacing leads with news potential which weren’t included in the original reporting. This means there is potential to inform avenues of investigation for new coverage. Moreover, the validity ratings, while reasonable, indicate that having a human in the loop to check is as necessary as ever.
As an example to illustrate the findings further let’s look at
 the story we included published by Readr,
 which utilized the Facebook Ad Library to analyze the ad spending and targeting strategies of political parties in Taiwan. In this case, we found that while the agents’ setup exhibited higher newsworthiness overall, it had lower precision, meaning the baseline tips were more closely aligned with the original article. Both models provided basic statistics from the dataset, such as which party spent the most on ads (e.g., “The Democratic Progressive Party was the top spender on Facebook ads during Taiwan’s 2020 elections”) and details on targeted demographics and regions.
However, in several iterations, the agents’ setup shifted focus to also explore the relationship between ad spending and impressions, thus laying more emphasis on the platform’s role in disseminating the content. For instance, one agent returned a tip highlighting that an independent candidate achieved the highest campaign efficiency. While such insights are potentially valuable and similar analyses have been covered in other journalistic pieces, the original article centered primarily on the role of Taiwan’s political parties — a focus better reflected in the baseline tips.
What’s next
While the results of our evaluation were promising, there’s still a lot of ground left to cover. For one, the scope of our evaluation was limited by the constraints of OpenAI’s Assistants API, which doesn’t support executing complex code or using external packages. That meant we had to exclude certain types of projects from our evaluation. To really unlock the potential of these agents, we need to look at integrating more flexible, non-proprietary models that can handle a wider range of tasks.
There’s also a need to dig deeper into the various parts of the pipeline itself. How do the system prompts, knowledge bases, and feedback loops contribute to the final outcome? And how robust are they to small changes? Understanding these components more fully could lead to even more powerful and nuanced iterations of this tool for journalists. As we improve the system, we’re also keen to collaborate with newsrooms that are interested to try it out on their data-driven investigations.
Our current setup offers a lot of automation, but it leaves limited room for human input during the early stages. In a real-world newsroom, it’s crucial that journalists have more control, perhaps by shaping the initial questions or guiding the analysis in real-time. We’re interested in thinking through broader frameworks for integrating humans into the processing loop of agentic systems, including by exploring interaction and interface paradigms for effective ways to support journalistic agency, supervision, and expert judgment in the process.
The system we’ve developed shows a lot of promise — it’s a tool that can help uncover valuable leads and provide new angles on complex stories. But it’s also just that: a tool. The insights generated by these agents are a starting point, but the real work of journalism, the craft of telling a story that matters, remains firmly in human hands.
—
Joris Veerbeek is a PhD candidate in the Department of Media and Culture Studies at Utrecht University. He focuses on the application of AI in investigative journalism and, as part of his PhD, works on investigative data journalism projects with the Dutch weekly De Groene Amsterdammer.
Nick Diakopoulos is a Professor in Communication Studies and Computer Science (by courtesy) at Northwestern University where he is Director of the Computational Journalism Lab (CJL).]]></content:encoded>
</item><item>
<title><![CDATA[The Current State of GenAI Copyright Lawsuits]]></title>
<link>https://generative-ai-newsroom.com/the-current-state-of-genai-copyright-lawsuits-203a1bd0f616?source=collection_home_page----df04cefca135-----32-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Copyright]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Julia Barnett]]></dc:creator>
<pubDate>2024-09-18</pubDate>
<atom:updated>2024-09-18</atom:updated>
<content:encoded><![CDATA[The Current State of GenAI Copyright Lawsuits
Parsing the cases and how they may play out
Julia Barnett
Follow
17 min read
·
Sep 18, 2024
--
Listen
Share
Image generated using Midjourney.
In this series, I 
first went over some important background
 on the issues at hand in the generative AI copyright cases happening right now. I then discussed some landmark cases in the 
long history of technological media innovation resulting in lawsuits
 about alleged copyright infringement. This was all intended to give a more comprehensive understanding of the current state of lawsuits concerning big tech companies that have deployed generative AI models like OpenAI and Microsoft and the copyright holders of the content used to train these models.
At the time of writing this post, there are at least 
30 major copyright lawsuits
 against generative AI companies. For convenience, I have listed them all in the 
appendix
 alphabetically within subject groupings (combining when appropriate) with attached docket links and a short snippet describing the major players and models the cases are about. In the main content of this post I have highlighted the most important cases in
 journalism, books, images, 
and
 audio
. I also include the location of the lawsuit (by district), which is significant because it has implications for how the case may play out (i.e., Northern California may be more tech friendly than Delaware) as well as potentially increasing the likelihood of the Supreme Court picking up a case later down the line if the same topic has been battled out in multiple districts.
If you want to skip the specifics of the lawsuits and jump to the commentary, click 
here
.
📰 Journalism
The New York Times Company v. Microsoft Corporation 
— 
The NYT v Microsoft and OpenAI concerning Copilot and GPT Models
Easily the most widely watched case.
The NYT claims that OpenAI and Microsoft unlawfully used millions of their articles to train GPT models and Copilot, threatening the very existence of journalism.
All other cases in this category are extremely similar to this landmark one.
📍Southern District of New York
Daily News v Microsoft Corporation
 — 
8 newspaper publishers v OpenAI and Microsoft concerning GPT models and Copilot
Similar to the NYT case, but instead of one plaintiff it’s many news organizations (organized by the parent owning organization Alden Global Capital), and the plaintiffs focus on the threat to local news specifically.
The New York Daily News, The Chicago Tribune, The Orlando Sentinel, San Jose Mercury-News, and four others allege that OpenAI and Microsoft used millions of copyrighted articles to train their models.
Judge Stein has shown no interest in consolidating this with the NYT lawsuit.
📍Southern District of New York
Other cases (more details in Appendix):
All have the same underlying concern; simply different plaintiffs and defendants/model trainers.
The Center for Investigative Reporting, Inc. v. OpenAI, Inc. 
— 
GPT models and Copilot
The Intercept Media, Inc. v. OpenAI, Inc. 
— 
GPT models
Leovy. v. Alphabet Inc.
 — 
Gemini (previously “Bard”)
Raw Story Media, Inc. v. OpenAI Inc. 
— 
GPT models
📚 Books:
Authors Guild v. OpenAI Inc. 
— 
Authors v OpenAI concerning GPT models (ChatGPT)
Authors allege that OpenAI (and Microsoft) used their work without permission during the training of GPT models and resulting products like ChatGPT, thus allegedly infringing their copyrights.
Now just one consolidated case but was previously three separate cases: (1) Authors Guild v OpenAI, (2) 
Alter v OpenAI
, and (3) 
Basbanes v Microsoft
.
📍Southern District of New York
Tremblay v. OpenAI, Inc.
 — 
Authors v OpenAI concerning the GPT LLMs (ChatGPT)
Authors allege OpenAI infringed upon their copyright during the training process of GPT models and ChatGPT, They originally also alleged output infringement but that was thrown out by Judge Martinez-Olguin because she did not see enough similarity between original works presented and the ChatGPT outputs.
To throw this out at the summary judgment stage, the judge thought they were so dissimilar that no reasonable jury could have found them to be similar. Taking away from the jury a factual issue like that is the kind of thing that routinely gets challenged on appeal.
Consolidated three cases: (1) Tremblay, (2) 
Chabon
, and (3) 
Silverman v. OpenAI
.
📍Northern District of California
Other cases (more details in Appendix)
All have the same underlying concern; simply different plaintiffs and defendants/model trainers
Bartz v. Anthropic PBC
 — 
Claude
Huckabee v. Bloomberg
 — 
BloombergGPT
Kadrey v Meta
 
— LLaMa
Nazemian v. NVIDIA Corporation
 
— Nemo
ONan v. Databricks, Inc. 
— 
MosaicML MPT LLMs
🎨 Visual Art and Images:
Getty Images (US), Inc. v. Stability AI, Inc.
 
— 
Getty Images v Stability AI concerning Stable Diffusion and DreamStudio
Getty Images alleges that Stability AI used more than 12 million copyrighted photographs to train their models for resulting products Stable Diffusion and DreamStudio, thus infringing on their copyright.
They also allege trademark infringement due to the models allegedly recreating the watermark in the outputs of the model.
📍Delaware District Court
Andersen v. Stability AI Ltd.
 — 
Visual artists v Stability AI concerning Stable Diffusion and DreamStudio, v Midjourney concerning their eponymous tool, and v DeviantArt concerning DreamUp
Visual artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) allege that Stability AI, Midjourney, and DeviantArt all acquired copies of billions of images without permission to train their generative AI tools, thus allegedly infringing on artists’ copyright
Different from the Getty case in that Getty’s economic model is vastly different from an individual painter’s economic model. Getty wants $$$, and thus may have no issue settling for a large payout. The individual artists might be protecting their livelihoods and future careers, thus may have more incentive to continue for the long haul.
📍Northern District of California
Other case (more details in Appendix):
Zhang v. Google LLC
 
— Imagen
🎵 Music and Audio:
Concord Music Group, Inc. v. Anthropic PBC
 — 
Music publishers v Anthropic concerning the LLM Claude (lyrics, not audio)
Many large music publishers (Concord Music Group, Universal Music, Capitol CMG, etc.) sued Anthropic specifically for allegedly infringing their copyrights for use of lyrics to train Claude.
📍Northern District of California
UMG Recordings, Inc. v. Uncharted Labs, Inc.
 — 
Music publishers v Unchartered Labs concerning Udio
Many huge players in the music industry (UMG Recordings, Capitol Records, Sony, Arista, Atlantic, etc.) are alleging that Unchartered Labs (creator of Udio.com) unlawfully used their recordings to train their genAI music model, thus allegedly infringing their copyrights.
Udio “effectively conced[ed] in pre-litigation correspondence that it copied Plaintiffs’ copyrighted sound recordings” (quote from main complaint document).
📍Southern District of New York
Vacker v. Eleven Labs
 — 
Voice actors v Eleven Labs concerning ElevenLabs TTS
Filed just recently, this lawsuit comes from a group of professional voice actors who allege that Eleven Labs unlawfully trained on copyrighted recordings the actors recorded for various materials (e.g., audio books), thus infringing their copyrights.
📍Delaware District Court
What’s going on in these lawsuits?
The reason I only highlight a few cases is that the crux of the issue in almost every lawsuit is quite similar: some copyright holder of content alleges that a big tech company unlawfully used their copyrighted content to train their model. This content could be a book, a piece of journalism, a piece of art, lines of code, song lyrics, song audio, or almost anything else you can think of that copyright law protects.
If you recall from my 
first post of this series
, the 3 elements that need to be established to constitute an infringement of copyright are (1) the plaintiff (e.g., an author) needs ownership of a valid copyright, (2) the defendant (e.g., a tech company) needs to have used the plaintiff’s work, and (3) the defendant needs to have made an exact copy or something “substantially similar” to the plaintiff’s work. It is unlikely there will be too much dispute over whether the companies used this data to train (most models train on a large chunk of the internet; or in some instances such as the 
Udio case
 the plaintiffs immediately conceded to having trained on the copyrighted data), so what will really be the main issue here is was it okay (read: legal) for them to do so.
The majority of these cases are alleging infringement both during training and in outputs. Some judges have already ruled (such as 
Judge Martinez-Olguin in the Silverman lawsuit
) that the claim of output infringement is not valid since they cannot prove substantial similarity between the copied works and the generative AI model outputs. But in cases like these they may still allow the training infringement claims to proceed — the rejection of the output infringement claim does not necessarily indicate how they will evaluate the training infringement.
Issues of infringement during training will be much more interesting since most of the plaintiffs have solid and concrete evidence (sometimes including immediate admissions) that the defendants are using their content without permission. As a result, the spotlight will turn to the tech companies’ defenses of fair use. There are multiple angles they can take here, and as I discussed in the last post there is a 
long precedent of technological innovation resulting in copyright cases
. We know that courts sometimes rule for copyright owners and almost always take their interests very seriously, but courts are also sensitive to the value of technology (i.e., judges may favor the tech companies for the claims of infringement occurring during the outputs of generative models). Where it becomes interesting is when there are no disputes over the usage of the copyrighted content, but an argument about the necessity of the usage in order to create something for a legitimate non-infringing use. This will be the case in disputes concerning infringement during the training process.
A major argument the tech companies will make is that they had to use the copyrighted content to train their models to the accuracy and efficacy level they are at now, and that it would have been prohibitive to receive approval from each and every copyright holder when the records they’re training on is in the count of billions if not trillions. They will argue, similar to 
Sega
, 
Google Books
, and 
Google Android
 that they 
had
 to use the content in order to create something that serves a public good. They will likely argue that yes, there may be one-off cases of infringement here and there but these are negligible compared to all the other goods they think their products provide such as easy translation, new educational access, and aiding the public with arduous tasks. The crux of these cases will hinge on the question of whether the use of the copyrighted data 
constitutes fair use or not
.
What’s actually going to happen in these cases?
Tl;dr: 
we don’t know.
It is important to note that there will likely continue to be more cases levied against big tech for these alleged instances of copyright infringement. Some of them will get dismissed quickly due to poor construction of the legal arguments. It is imperative to highlight that even though media outlets may make panicked cries that these are wins for big tech and generative AI, it actually will have no bearing on the final resolution of the copyright issues. The well constructed cases (like the ones highlighted in this post) will stick around, with well-seasoned, experienced lawyers representing both sides. This speaks to the elitism of the legal system; these lawyers/law firms need to be in it for the long haul (multiple years) and thus have the deep pockets to support it. Let’s describe chronologically how this will play out.
First these cases will get battled out at the district level (primarily New York and California, with some in Delaware). Then they will inevitably be appealed regardless of which side wins — remember, there are 
billions of dollars at stake here
. Importantly, appellate courts have to weigh in on these appeals (either granting them or not). When they decide to weigh in, appellate judges will have the prerogative to remake the legal standards that the trial courts had set out, potentially rendering any conflicting district opinions obsolete. There are 13 federal appellate courts across the U.S., and as such there may well be conflicting opinions among those 13 courts on extremely similar cases. Inevitably after this, these cases will be appealed again to the Supreme Court. The Supreme Court 
does not have to weigh in
. The Supreme Court is asked to review 
~7,000 cases per year
, of which they tend to review 100–150 (1.5–2%). If the Supreme Court does not decide to review a case, then the appellate decision will stand. The Supreme Court can decide to combine cases, they can decide to only review certain elements of cases, and they can decide to not review them at all. We will ultimately not know the final ruling until we hear from the Supreme Court. Now let’s talk about what could happen in court.
In a Binary World, Two Possibilities:
1. Side with the Tech Companies
If the courts largely side with the tech companies and affirm fair use for model training, then we will likely see no consequences for ingesting copyrighted data to train models. In instances where these models pose major risks to industries such as 
voice acting
 and 
music creation
, we may see stark economic and labor impacts on current members of these communities. Within journalism, many of these lawsuits bring into question the potential impact on journalism as a whole. The 
main complaint in the NYT case
 notes “Defendants’ unlawful use of The Times’s work to create artificial intelligence products that compete with it threatens The Times’s ability to provide that service [journalism]”, and the 
Daily News case
 echoes the sentiment “…local news is the bedrock of democracy and its continued existence is put at risk by Defendants’ actions.”
If the tech companies win, nothing will really change from the current state of affairs. Even before these lawsuits have been resolved, plenty of copyright owners have opted to grant licenses to (at least some) AI companies. These licensing agreements will likely continue even if the AI companies win big. The terms will be more favorable to AI because they won, compared to right now (and especially favorable compared to what the terms would be if they lose).
We will also continue to see more and more models come about, and model creators will worry far less about scraping content to train their models. These models will continue to become better and better as the technology improves and corporations throw increasing amounts of computation and energy into training on ever larger datasets.
2. Side with the Copyright Holders
If even one of these cases ultimately sides with the plaintiffs (i.e., through a Supreme Court decision or, lacking that, an appellate decision), then we will see some legally mandated changes. Since all of these cases are at such an early stage, there is 
little attention being paid to potential remedies sought
 in these cases.
One remedy is quite obvious: the tech companies pay money to cover damages. This could be in the form of a settlement (which would ultimately amount to a license), or result in 
copyright statutory damages
 and/or expenses related to the loss of 
copyright management information (CMI)
. CMI means that when they use the copyrighted content and then repurpose it (such as curating it in a dataset), the original copyright notice gets lost in translation somewhere; each infringement of this case has a minimum damage awarded of $2,500 per instance. More general copyright statutory damages have a wider range, but when stacked up in the millions of violations these can add up to extremely large amounts. These costs could be so prohibitive that the models have to pivot away entirely from using copyrighted content.
A second remedy being sought in some of the cases (including 
the NYT v OpenAI
) is destruction of the models that were trained on the copyrighted works. Three potential issues in this case (that 
Pamela Samuelson
 has pointed out) are that (1) the training datasets are different entities than the models — you can destroy the dataset without impacting the model, (2) some of the datasets these models train on are open source and thus exist prolifically online; destroying one version of the dataset will not eradicate it from the web, and (3) the courts get to decide whether they want to mandate destruction of the model independent of the ruling in favor of the plaintiff.
Concluding Thoughts
Now we know what is at stake, we know what the claims are, we know the precedents, and we know the likely arguments both sides will make. We do not, however, have any inkling of what will 
actually 
happen — it is impossible to say today which side the courts (and ultimately, the Supreme Court) will favor. It is even hard to assess the probability of the outcomes in these cases, although the parties and various financial backers have well-informed guesses that they are refining. We do know it will likely be a long battle, and there will likely be many more cases filed until we get any answers.
Vital to highlight here is that these lawsuits are not the only opportunity for AI reform — there are other avenues such as legislation, advocacy, and academic research that can hopefully shape these generative AI models into a more equitable future trajectory. These should all be pursued regardless of any legal outcome.
Appendix
A full list of current cases, listed alphabetically within Journalism, Books, Visual Art/Images, Music/Audio, and Other:
📰 Journalism:
The Center for Investigative Reporting, Inc. v. OpenAI, Inc. 
– 
Nonprofit news organization v OpenAI and Microsoft concerning GPT models and Copilot
The center for investigative reporting has filed their own lawsuit against OpenAI and Microsoft for allegedly using their articles during training of their genAI models, thus infringing on their copyright.
📍Southern District of New York
***
Daily News v Microsoft Corporation
 – 
8 newspaper publishers v OpenAI and Microsoft concerning GPT models and Copilot
Substantially similar to the NYT case, but instead of one plaintiff it’s many news organizations and the plaintiffs focus on the threat to local news specifically
The New York Daily News, The Chicago Tribune, The Orlando Sentinel, San Jose Mercury-News, and four others allege that OpenAI and Microsoft used millions of copyrighted articles to train their models.
Judge Stein has shown no interest in consolidating this with the NYT lawsuit.
📍Southern District of New York
The Intercept Media, Inc. v. OpenAI, Inc. 
– 
Nonprofit investigative news v OpenAI concerning GPT models
The Intercept alleges that OpenAI used “thousands, if not millions” of articles unlawfully in their training of GPT models, thus infringing upon their copyright.
📍Southern District of New York
Leovy. v. Alphabet Inc.
 – 
Individual author and journalist v Google concerning Gemini (previously “Bard”)
Investigative journalist and non-fiction author (Jill Leovy) alleges that Google used over a decades worth of her investigative journalism work and her award winning non-fiction book without consent to train Gemini, thus allegedly infringing on her copyright
Formerly J.L. v Alphabet Inc, but an 
amended complaint
 cut the length from 143 to 32 pages and dropped a bunch of plaintiffs to just be one
📍Northern District of California
***
The New York Times Company v. Microsoft Corporation 
– 
The NYT v Microsoft and OpenAI concerning Copilot and GPT Models
Easily the most widely watched case out of all 30.
The NYT claims that OpenAI and Microsoft unlawfully used millions of their articles to train GPT models and Copilot, threatening the very existence of journalism.
All other cases in this category are extremely similar to this landmark one.
📍Southern District of New York
Raw Story Media, Inc. v. OpenAI Inc. 
– 
Alternative media company v OpenAI concerning GPT models
Raw Story Media files its own lawsuit against OpenAI and Copilot for allegedly unlawfully using their content during training, thus infringing on their copyright.
📍Southern District of New York
📚 Books
***
Authors Guild v. OpenAI Inc. 
– 
Authors v OpenAI concerning GPT models (ChatGPT)
Authors allege that OpenAI (and Microsoft) used their work without permission during the training of GPT models and resulting products like ChatGPT, thus allegedly infringing upon their copyright.
Now just one consolidated case but was previously three separate cases: (1) Authors Guild v OpenAI, (2) 
Alter v OpenAI
, and (3) 
Basbanes v Microsoft
.
📍Southern District of New York
Bartz v. Anthropic PBC
 – 
Authors v Anthropic concerning the Claude LLMs
Authors (Andrea Bartz, Charles Graeber, and Kirk Wallace Johnson) allege Anthropic pirated their books to train their model and infringed on their copyright.
📍Northern District of California
Huckabee v. Bloomberg
 – 
Authors v Bloomberg concerning finance LLM BloombergGPT
Author Mike Huckabee (former governor of Arkansas) and others and allege that Bloomberg used datasets containing hundreds of thousands pirated books to train their model, thus allegedly infringing on their copyright
Previously Hucakbee v Meta, Bloomberg, Microsoft and EleutherAI Institute, but the case against Meta and Microsoft has been transferred to California, and the EleutherAI case was dropped without prejudice
📍Southern District of New York
Kadrey v Meta
 –
 Authors v Meta concerning the LLM LLaMa
Authors (Richard Kadrey and Sarah Silverman lead plaintiffs) allege Meta infringed upon their copyright to train the LLM LLaMa
Related: 
Chabon v Meta
; 
Huckabee v Meta
📍Northern District of California
Nazemian v. NVIDIA Corporation
 – 
Authors v NVIDIA concerning the LLM Nemo
Authors (Andre Dubus III and Susan Orlean) allege NVIDIA infringed on their copyright during training of Nemo Megatron-GPT
Now combined with the same plaintiffs in the ONan v. Databricks, Inc. case below (Abdi Nazemian, Brian Keene and Stewart O’Nan)
Combined with 
Dubus v NVIDIA
📍Northern District of California
ONan v. Databricks, Inc. 
– 
Authors v Databricks concerning the MosaicML MPT LLMs
Authors (Abdi Nazemian, Brian Keene and Stewart O’Nan) allege Databricks and Mosaic ML infringed on their copyright during training of MosaicML Pretrained Transformer (“MPT”)
Related: 
Makkai v. Databricks, Mosaic ML
📍Northern District of California
***
Tremblay v. OpenAI, Inc.
 – 
Authors v OpenAI concerning the GPT LLMs (ChatGPT)
Authors allege OpenAI infringed upon their copyright during the training process of GPT models and ChatGPT, Theyoriginally also alleged output infringement but that was thrown out by Judge Martinez-Olguin because she did not see enough similarity between original works presented and the ChatGPT outputs.
To throw this out at the summary judgment stage, the judge thought they were so dissimilar that no reasonable jury could have found them to be similar. Taking away from the jury a factual issue like that is the kind of thing that routinely gets challenged on appeal.
Consolidated three cases: (1) Tremblay, (2) 
Chabon
, and (3) 
Silverman v. OpenAI
.
📍Northern District of California
🎨 Visual Art and Images
***
Andersen v. Stability AI Ltd.
 – 
Visual artists v Stability AI concerning Stable Diffusion and DreamStudio, v Midjourney concerning their eponymous tool, and v DeviantArt concerning DreamUp
Visual artists (Sarah Andersen, Kelly McKernan, and Karla Ortiz) allege that Stability AI, Midjourney, and DeviantArt all acquired copies of billions of images without permission to train their generative AI tools, thus allegedly infringing on artists’ copyright
📍Northern District of California
***
Getty Images (US), Inc. v. Stability AI, Inc.
 
– 
Getty Images v Stability AI concerning Stable Diffusion and DreamStudio
Getty Images alleges that Stability AI used more than 12 million copyrighted photographs to train their models for resulting products Stable Diffusion and DreamStudio, thus infringing on their copyright.
They also allege trademark infringement due to the models allegedly recreating the watermark in the outputs of the model.
📍Delaware District Court
Zhang v. Google LLC
 – 
Visual artists v Google concerning Imagen
Visual artists (Jingna Zhang, Sarah Andersen, Hope Larson, and Jessica Fink) allege that Google trained on billions of labeled copyrighted images multiple times throughout their model training, thus allegedly infringing on their copyright
📍Northern District of California
🎵 Music and Audio
Concord Music Group, Inc. v. Anthropic PBC
 — 
Music publishers v Anthropic concerning the LLM Claude (lyrics, not audio)
Many large music publishers (Concord Music Group, Universal Music, Capitol CMG, etc.) sued Anthropic specifically for allegedly infringing their copyrights for use of lyrics to train Claude.
📍Northern District of California
UMG Recordings, Inc. v. Uncharted Labs, Inc.
 — 
Music publishers v Unchartered Labs concerning Udio
Many huge players in the music industry (UMG Recordings, Capitol Records, Sony, Arista, Atlantic, etc.) are alleging that Unchartered Labs (creator of Udio.com) unlawfully used their recordings to train their genAI music model, thus allegedly infringing their copyrights.
Udio “effectively conced[ed] in pre-litigation correspondence that it copied Plaintiffs’ copyrighted sound recordings” (quote from main complaint document).
📍Southern District of New York
Vacker v. Eleven Labs
 — 
Voice actors v Eleven Labs concerning ElevenLabs TTS
Filed just recently, this lawsuit comes from a group of professional voice actors who allege that Eleven Labs unlawfully trained on copyrighted recordings the actors recorded for various materials (e.g., audio books), thus infringing their copyrights.
📍Delaware District Court
💻 👩🏻‍⚖️ Other: Code and Legal Content
DOE 1 v. GitHub, Inc.
 – 
Anonymous plaintiff v GitHub, Microsoft, and OpenAI concerning Codex and Copilot
Anonymous set of plaintiffs allege GitHub Microsoft, and OpenAI used copyrighted code to build their products Codex and Copilot, which they claim violates contracts for open-source software licenses
📍Northern District of California
Thomson Reuters Enterprise Centre GmbH v. ROSS Intelligence Inc. 
– 
Reuters v ROSS Intelligence concerning Legal LLM
Especially interesting because it is currently in trial
Thomas Reuters alleges that ROSS Intelligence took content without permission from the Reuters legal research platform “Westlaw” in order for ROSS to train its legal oriented AI platform, thus allegedly infringing on their copyright
📍Delaware District Court
— — —
Disclaimer
: The information in this post does not, and is not intended to, constitute legal advice and is for general informational purposes only.
Acknowledgements:
 The author would like to thank 
Dr. Peter DiCola
 for his invaluable discussion and feedback on this work, as well as 
Dr. Nick Diakopoulos
 for his helpful feedback.]]></content:encoded>
</item><item>
<title><![CDATA[Do People Want to Get Their News from Chatbots?]]></title>
<link>https://generative-ai-newsroom.com/do-people-want-to-get-their-news-from-chatbots-8a806495b65d?source=collection_home_page----df04cefca135-----33-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Chatbots]]></category><category><![CDATA[Audience]]></category>
<dc:creator><![CDATA[Nick Hagar]]></dc:creator>
<pubDate>2024-09-12</pubDate>
<atom:updated>2024-09-12</atom:updated>
<content:encoded><![CDATA[Do People Want to Get Their News from Chatbots?
What data from real-world LLM interactions tells us
Nick Hagar
Follow
9 min read
·
Sep 12, 2024
--
1
Listen
Share
Visualization of Wildchat user messages, generated using 
Nomic Atlas
The existential threat that large language models (LLMs) seem to pose for news organizations is hard to ignore. If chatbots can answer user news-related queries directly — without sending them to news websites — they could crater what remains of outlets’ search traffic. While LLMs aren’t yet reliable sources of news and information (
hallucinations
 are 
still an issue
), products like 
SearchGPT
 and Google’s 
AI Overviews
 are gradually paving the way for this shift. The recent surge in news publisher 
lawsuits
, 
licensing deals
, and 
scraping restrictions
 suggests that the industry is bracing for a paradigm shift that further weakens newsrooms relying on digital engagement to survive.
But while tech companies have a clear incentive to push this transformation, it’s far less obvious whether news audiences actually 
want
 to use AI chatbots as a news source. Do people really ask OpenAI’s ChatGPT for the latest election headlines? Are they using Google’s Gemini to check scores from last night’s NBA games?
In the real world, are LLMs truly becoming a go-to source for reading news? Much of the impact of these chat interfaces on news and journalism will ultimately hinge on whether there is an appetite from users and audiences.
To explore this question of how (and whether) people use LLMs for news, I analyzed data from 
Wildchat
, a dataset collected by researchers at the 
Allen Institute for Artificial Intelligence
, a non-profit focused on research to advance artificial intelligence. Wildchat contains ~1 million anonymized interactions between users from ~200 countries and two OpenAI models — GPT-3.5 and GPT-4 — from April 2023 to April 2024. Users had unrestricted access to these models in exchange for sharing their chat histories, providing a snapshot of real-world LLM usage.
When 
journalists
 have 
analyzed
 the Wildchat dataset, they’ve found an overabundance of conversations about sex and requests for help on homework. Researchers from the University of Washington 
mined the dataset
 for instances of journalists using an LLM in the process of reporting and writing stories, and found several examples of queries geared toward generating article drafts. Anecdotally, from my initial 
exploration
 and inspection of the data, other major categories include: requests for prompts to use with image generator 
Midjourney
; text translation; questions about coding, statistics, and machine learning; and lots of people working on creative writing projects such as scripts and short stories.
Clearly there is a wide range of use cases and behavior around ChatGPT, but here I want to focus more specifically on identifying news oriented behavior
. 
In particular I examine 
news-related queries 
to help answer two key questions:
How often do users ask LLMs about the news?
What types of news questions do they ask?
Analytic Approach
To answer these questions, I followed a multi-step approach to identify news queries in the dataset.
Initial filtering:
 I narrowed the full Wildchat dataset down to English-language messages from U.S.-based users, and excluded all LLM responses to focus just on the initial query. This resulted in a subset of ~265,000 messages across ~146,000 conversations.
Manual annotation:
 I manually reviewed a random sample of 1,000 messages, classifying them as either 
news queries
 or 
non-news queries
. My definition of 
news queries
 included questions about specific events, general requests for news, or inquiries about news sources.
Examples of non-news queries in this sample included:
write me 50 jokes
Can you show me how to make a program that allows the user to draw and animate on transparent 3d planes that allows their animations to appear like they’re moving in 3d space
Quantify and characterize viral pathogens’ adsorption on micro- and nano-plastics and desorption from the plastics
And examples of news queries included:
If SAG-AFTRA goes on strike, why do news reporters and anchorpersons stay on the air?
What major news happened on January 23, 2023? Rate using a scale of 1 to 10 your confidence that your answer is correct.
What has president Biden said about the Russian people and their relation to their president Vladimir Putin?
In this sample, I found only eight news queries.
Targeted search:
 To supplement the random sample with more relevant queries, I conducted targeted keyword searches (e.g., “news,” “breaking,” “New York Times,” “Ukraine”). This produced 58 additional queries that I labeled as news queries. After reviewing the results, I ended up with a sample of 1,058 messages: 992 non-news queries and 66 news queries.
LLM evaluation:
 Identifying news-related messages was challenging, as users might ask about the day’s headlines, ongoing national stories, or specific local events. Given the dataset’s size and diversity, I couldn’t rely on a single set of search terms or manual review. I therefore relied on LLM annotation for more nuanced review at scale. I first used two LLMs — GPT-4o and GPT-4o-mini — to replicate the categorization on my manually annotated sample. Both models performed well:
LLM annotation:
 Finally, I used GPT-4o-mini via OpenAI’s batch API to categorize the entire dataset of 265,000 messages (prompt in appendix).
News-Related Queries with ChatGPT
Based on these annotations of Wildchat user interactions with ChatGPT, it’s clear that 
news queries are rare
. 1.88% of all messages in the sample were classified as news-related, about 5,000 in total. These messages came from ~1,000 unique IP addresses (7.15% of IP addresses in the sample). This scarcity is in line with how people generally consume news online — time spent with journalism makes up 
a small portion
 of most people’s day, and 
only a fraction
 of what’s shared on social media is news.
Yet the diversity within the news queries that do exist is significant
. Users approach LLMs for help in navigating various parts of the news consumption process. By manually reviewing the 5k messages identified, I found 11 types of news queries, described in the table below:
These queries reveal several important patterns in how news consumers interact with LLMs, in ways that traditional platforms cannot easily replicate. These behaviors suggest opportunities for future news-oriented products that might more directly serve user needs, even if only a small fraction of users overall currently exhibit them.
Active News Assistance
Based on the typology of user behaviors uncovered, it’s clear that many users weren’t just looking to consume news passively; they actively engaged in more complex tasks like summarization, translation, and analysis. This signals a shift in how some users are interacting with news content, using AI not simply to read the news, but to process, interpret, and personalize it in ways that meet their individual needs.
In addition, some users incorporated news articles as part of broader conversations about politics, science, or local events, treating AI as a medium for “conversational news.” This represents a significant departure from traditional one-way news consumption, enabling readers to discuss, unpack, and collaborate around news stories interactively. To the extent that users want to include news content as part of the context of their interactions, this raises questions for news organizations about copyright and control of their coverage.
Together, these behaviors point to ways that LLMs can assist readers by summarizing articles or offering context for ongoing stories. They can help explain complex concepts in simpler terms or respond to targeted follow-up questions. Or they can offer insight into the reporting process itself, such as explaining what “on background” means or offering tips on evaluating source credibility.
From a design perspective, these use cases also highlight a tension in how many newsrooms approach generative AI. As publishers experiment with LLM capabilities like 
translation
, they often don’t personalize these outputs to the individual reader. Yet the queries seen here suggest that readers may get more value out of controlling the scope and context of an LLM’s assistance. Newsrooms must therefore consider where they want their generative AI products to fall on the spectrum between user and publisher control.
Fulfilling Specific Information Needs
ChatGPT seems to support highly contextualized and specific information seeking behavior, much as a search engine does. And as with a search engine, users leveraged the LLM for a wide range of news and information requests — some users asked the model directly for a broad overview of news headlines, while others wanted information about a particular local event, or from a trusted source. This blend of querying for news and seeking personalized angles suggests that pairing robust search with generative outputs could benefit news consumers.
In addition to tailoring responses based on the unique details of a given query, this behavior also opens up the potential for LLMs to deliver personalized news experiences, whether by focusing on stories that matter most to the reader, translating content into a preferred language, or adjusting the tone and style to suit their preferences, based on learned associations for how a user tends to use the product. AI could adapt news delivery to individual tastes and needs in this way, further enhancing the user experience.
Supporting Media and AI Literacy
A notable subset of users turned to LLMs for help in evaluating the credibility of news sources and understanding media bias. This behavior underscores the potential for LLMs to play a crucial role in fostering media literacy. LLMs could be developed to offer users guidance in evaluating the credibility of news articles, providing context on the perspectives of different outlets, and highlighting the characteristics of trustworthy journalism. This also extends to generally helping users understand both the underlying process of news production and capabilities and limitations of the model for helping to deliver it. By helping users better understand the media they consume, LLMs could empower individuals to make more informed decisions about the information they rely on.
Limitations
While these findings offer valuable insights into how users are interacting with LLMs for news consumption, it’s important to recognize the limitations of this analysis. As the Allen Institute researchers 
note
, their chat interface “predominantly attracts developers or individuals connected to the IT sector,” so these users are likely not representative of the broader population. Moreover, this analysis is limited to English-language, U.S.-based users, meaning the characteristics and volume of news-related queries might differ in other cultural or linguistic contexts. And unlike more advanced LLM systems like ChatGPT, the software used for Wildchat lacks web-browsing capabilities, which may have limited the scope of news queries. Whether and how behavior may be different with the full ChatGPT product, or with new search-oriented products like SearchGPT or Perplexity remains unclear.
In Closing
These findings nonetheless highlight significant opportunities for the future of LLMs in news consumption. The data underscores a clear use case for more personalized and interactive news experiences — needs that LLMs are well-positioned to meet.
The threat that LLMs pose to news organizations is real, but it also opens up an unprecedented opportunity. While LLMs have the potential to disrupt traditional news consumption, this very capability could be harnessed by news organizations to better serve their audiences. The data shows that users are not just looking for quick answers; they want deeper engagement, personalization, and credible sources they can trust. By leveraging generative AI thoughtfully, news organizations can meet these evolving needs — offering interactive, personalized news experiences that keep readers coming back.
Appendix: LLM classification prompt
You are an AI assistant designed to classify user messages as either news-related (1) or not news-related (0). Your task is to analyze each message and determine if it pertains to news, current events, or information seeking about news sources.
Classification Guidelines:
Classify as news-related (1):
Messages seeking information about news sources
Questions about current events or recent happenings
Requests for updates on any topic that could be considered news
Inquiries about reputable news outlets or publications
Messages mentioning specific news events or asking for news summaries
Requests for information that would typically be found in news articles
Any message that mentions news, even if it's not the primary focus (err on the side of including ambiguous cases)
Classify as not news-related (0):
Personal questions or statements unrelated to current events
Requests for advice on personal matters
Fiction-related queries or creative writing prompts
Technical questions unrelated to news (e.g., coding, math)
General knowledge questions that aren't tied to current events
Output:
Provide only a binary output: 1 for news-related, 0 for not news-related
Include a brief explanation for your classification, focusing on the intent and content of the message
Important Notes:
Consider all news topics equally relevant (politics, sports, entertainment, etc.)
If a message contains both news-related and unrelated content, classify it as news-related (1)
Focus on the intent and content of the message, not on the specific sources mentioned
The message you are meant to classify will be wrapped in these tags: [MESSAGE]…[/MESSAGE]
Examples:
INPUT: [MESSAGE]Please list ten websites where I can find bilingual magazines both in Chinese and English for free download. The magazines should be as well-known as 'New York Times' and 'The Economist', and the information therein should be based on authoritative and reliable sources.[/MESSAGE]
OUTPUT:
1
This is news-related because the user is asking for information about news sources.
INPUT: [MESSAGE]Where to get FHA loan.[/MESSAGE]
OUTPUT:
0
This is not news-related because it's a personal finance question unrelated to current events.
INPUT: [MESSAGE]Name a few major events in the middle east from 2020 from the BBC.[/MESSAGE]
OUTPUT:
1
This is news-related because the user is asking about specific events from a news source (BBC) and a particular time period.
INPUT: [MESSAGE]Make Season 1 Episode 14 about Tommy and The Girls hanging out again, and Momo comes close to Tommy hugging him and caring for a him, and giving him a soft kiss on the lips[/MESSAGE]
OUTPUT:
0
This is not news-related because it appears to be a creative writing prompt or a request for fictional content.]]></content:encoded>
</item><item>
<title><![CDATA[Opportunities for Research on Generative AI Use in Journalism]]></title>
<link>https://generative-ai-newsroom.com/opportunities-for-research-on-generative-ai-use-in-journalism-fa15ccba6e6c?source=collection_home_page----df04cefca135-----34-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Journalism Innovation]]></category><category><![CDATA[Journalism Research]]></category><category><![CDATA[Ai Research]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Charlotte Li]]></dc:creator>
<pubDate>2024-09-03</pubDate>
<atom:updated>2024-09-03</atom:updated>
<content:encoded><![CDATA[Opportunities for Research on Generative AI Use in Journalism
Charlotte Li
Follow
6 min read
·
Sep 3, 2024
--
Listen
Share
Image by upklyak
 on Freepik
I recently attended 
SRCCON
, an annual gathering of news professionals, and wasn’t surprised to find that generative AI technologies and their applications to journalism continue to be at the center of many conversations. Aligned with 
our earlier report on generative AI in newsrooms
, more and more newsrooms are exploring applications of generative AI to individual tasks, such as researching, fact-checking, audience analysis, or translation. However, uncertainty about the ethics and impacts of these generative AI tools continues to loom over the industry at large, deterring further incorporation of AI technologies into a workflow.
Some concerns regarding adopting generative AI technologies in newsrooms include lack of ethics and safety standards around AI uses, difficulties with allocating already limited resources and integrating AI technologies, biases against generative AI uses among readerships, potential job losses, environmental impacts, and imbalances in power dynamics with technology companies. I noticed that, while many news professionals attending SRCCON were working on addressing some of these concerns with strategies within their newsroom or within the community of newsroom developers, these concerns also serve as a great source of inspiration for meaningful research questions in computational journalism. To help bridge the practitioner-researcher gap, in the rest of this article I explore a few of the major concerns that came up at the event and suggest some opportunities for research to contribute to addressing them.
Ethics and Safety Guardrails
Discussions about ethical concerns with generative AI technologies often came up early on when prompted about risks of adopting generative AI. At SRCCON the session on 
the intersection of AI and visual journalism
 and the session on 
AI failures
 both involved discussions about AI ethics. Specifically, practitioners were concerned with issues such as obtaining copyrights for content generated with generative AI technologies and unintentionally and unjustly including copyrighted materials through employing technologies trained on them. Issues with implicit biases embedded in AI models and misinformation were also brought up in these discussions.
In response to these concerns, news organizations like 
AP
 and 
Poynter
 created and continue to improve guidelines surrounding ethical use of AI technologies. Indeed, 
a growing number of newsrooms recognize
 the need for such guidelines. Such guidelines cover how AI should be reported on and how AI might be used in newsrooms. Yet, while they offer a solid foundation for creating standards of AI usage in individual newsrooms, they do not address the underlying legal and ethical implications of AI usage for journalism. Additionally, issues with transparency from large technology companies and slow advancements of policies on AI make it hard to design for ethical uses of generative AI.
Correspondingly, within academia, ethics of AI systems has been in discussion in papers like 
Ethics of AI: A Systematic Literature Review of Principles and Challenges
. Some research projects, such as 
The Ethical Implications of Generative Audio Models: A Systematic Literature Review
 
and 
Anticipating and Addressing the Ethical Implications of Deepfakes in The Context of Elections
 anticipate some implications of these ethical issues within specific types of generative AI like audio generation and deepfake generation. These research papers are a step forward in the direction of making generative AI more ethical. In the future, more work could be done collaboratively between researchers and practitioners to better understand ethical and legal implications of existing generative AI uses within newsrooms and how (and whether) concerns can be mitigated.
Audiences’ Perception of AI Use in Newsrooms
Another concern that’s present with the adoption of generative AI technologies in newsrooms is the fear of biases against the use of generative AI and the erosion of trust among audiences due to these biases. Some of these negative perceptions by audiences are spelled out in 
recent focus group research
 at the University of Minnesota. In addition to negative perceptions among audiences, some practitioners also expressed concerns about readers’ inability to distinguish between valuable journalistic information and misinformed generated information.
Both journalism practitioners and researchers are working on addressing issues of trust and bias against AI content. At SRCCON, 
a session presented by Bay City News
 attempted to address this concern with 
their experiments
 on transparency with AI usages in their newsrooms. During the session on 
the intersection of AI and visual journalism
, strategies surrounding watermarking images were also introduced. In terms of research, an article was recently published on this blog: 
“This Article is AI-Generated”: AI Disclosure and Labeling for News Content
, 
calling for further research on the design of disclosure and labeling of AI usage in news. 
A Watermark for Large Language Models
 introduces a language model that generates textual content that can be later detected as generated with relatively simple keys.
In addition to designing for disclosure of AI usage, another line of research that might help address these concerns relates to 
improving AI literacy
 among news readerships and the general public, whether this is achieved through incorporation of AI education in formal education or through more informal channels for education, such as news.
It is also worth noting that both lines of research work mentioned in this subsection focus on communicating and designing interactions for existing uses of generative AI tools, as well as changing the public perspective of them. This is in line with many other investigations surrounding generative AI within newsrooms.
Internal Difficulties for Incorporation of AI
Concerns about appropriately allocating resources to AI technologies are becoming more prevalent among management in newsrooms too, especially with the emergence of commercial AI products that market specifically to newsrooms, such as 
Nota.ai
, on top of the already highly competitive AI chatbot technologies scene. It is important to remember that acquiring and adopting AI technologies into use in newsrooms not only come with monetary cost, but also time and opportunity costs. Another concern that is in line with concerns about opportunity costs is the difficulty of integrating some AI use cases into the existing journalism workflow.
Existing solutions to some of these concerns in the industry include grant opportunities to test and build out AI use cases initiated by some tech companies like the 
Product & AI Studio
 in theAmerican Journalism Project. Nevertheless, adoptions and integrations are more reliant on individual engineering teams within each newsroom rather than offering a broad solution for all. While a project like Product & AI Studio brings together several newsrooms to discuss their individual use cases of generative AI, it would be helpful to understand and publicize common themes in how a newsroom chooses to incorporate a technology into their workflow, which can then become a less costly resource for many more newsrooms.
A research topic that may address these concerns includes how to design and develop 
domain specific evaluation metrics for AI tools
, which we’ve begun working on. Conversations I had with practitioners at SRCCON provide directions to expand on our approach with emphasis on designing for human-centered evaluation of potential evaluation metrics and establishing structures for value alignment in evaluation. This is also in line with the emphasis on understanding existing uses and properties of generative AI models instead of developing more uses. In the end research on such evaluation approaches could help lower barriers (and costs) around knowing when (and how) to adopt AI technologies.
Closing Thoughts
Above I present some thoughts on how to tackle a few concerns that are stopping practitioners from embracing AI technologies in their practice, though many more concerns remain unanswered. Research on the 
impact of generative AI on the labor market
, 
environmental impact 
and on 
an already imbalanced power dynamic
 between big tech and society are active, but few have looked specifically at these impacts in journalism.
As mentioned above, one of the main take-aways for me after attending SRCCON 2024 this year is that instead of developing larger, more powerful language models and defining new use cases of generative AI in newsrooms, the more pressing problems for research are on how to make the current usages and experimentations of generative AI in news safer, more usable, and more transparent.]]></content:encoded>
</item><item>
<title><![CDATA[The Long History of Technological Media Innovation and Copyright Legal Battles]]></title>
<link>https://generative-ai-newsroom.com/the-long-history-of-technological-media-innovation-and-copyright-legal-battles-1f283c97c6f7?source=collection_home_page----df04cefca135-----35-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Copyright]]></category><category><![CDATA[Generative Ai]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Julia Barnett]]></dc:creator>
<pubDate>2024-08-26</pubDate>
<atom:updated>2024-08-26</atom:updated>
<content:encoded><![CDATA[The Long History of Technological Media Innovation and Copyright Legal Battles
Julia Barnett
Follow
12 min read
·
Aug 26, 2024
--
Listen
Share
Photo generated using Midjourney.
To understand the 
current spate of lawsuits
 and the arguments both sides in the generative AI copyright debate will make, it is helpful to go over some foundational cases in the United States involving technological innovation and the copyright-owning industries (book publishing, music, movies and TV, etc.). There is a rich history of new technology disrupting the media industries followed by copyright holders responding with lawsuits in an attempt to protect their intellectual property.
The cases we are going to explore below also suggest a long and arduous process ahead — none of these generative AI lawsuits are likely to be decided in a quick turnaround especially if the major case(s) proceed to the Supreme Court. For each historical case I will explain the background, what the decision was, and what it means in the context of current copyright cases. Below is a timeline of every case I’ll discuss in this post, which I will do chronologically within three broad subject areas.
Timeline of the cases discussed in this post.
Chronologically we’ll go through a selection of key cases demonstrating (1) how the court has handled creative derivative works, then we will discuss (2) cases involving technologies that can enable subsequent copyright infringements, and finally (3) cases involving the use of copyrighted content for legitimate uses. This is not an exhaustive list of landmark cases, but it does include some fundamental rulings that will be cited in future cases. We’ll look at how US courts handled 
creative derivative works
 to shed some light on how alleged infringements during 
training
 may be evaluated, cases involving technologies enabling
 subsequent infringement
 by users to understand how alleged infringement during 
output
 may be evaluated, and finally examine some instances where the courts examined the use of copyright content for 
non-infringing purposes
 to explain the main defense on which the technology companies will rely.
How Courts Apply Copyright to New Technologies and New Uses
🎵 Player Piano — 
Decided: 1908
White-Smith Music Publishing Co. v. Apollo Co.
Player pianos
 were self playing pianos popular in the early 1900s that played paper or metallic piano rolls — continuous rolls of music with punched holes indicating to the piano when to play what notes (for audio enthusiasts, modern versions use 
MIDI
 files). The crux of this lawsuit centered around the question of whether these piano rolls constituted copyright infringement of the original compositions — did manufactures have to pay royalties to the composers? The distinction in this case back in 1908 was that these piano rolls were mechanical parts not intelligible to humans in the same way sheet music would be, but rather mechanical parts, and the case ruled in favor of the player piano manufacturers.
This was almost immediately superseded by (because it prompted) the Copyright Act of 1909 (and subsequent current 
copyright law §115
 in 1976) which mandated that you cannot prevent others from making piano rolls, vinyl records, or other ‘mechanical’ reproductions of musical works.
Derivative works
 are defined as works that are in some way derived from existing works such as a movie adaptation of a book or a musical arrangement of an existing song. In other words, you don’t have to ask Taylor Swift for permission to make derivative works of her songs, but you do have to pay her royalties if you make money off of those works. This is exclusively for 
music
 — I will discuss two examples in film and texts that have different results.
🎬 Seinfeld Trivia Book — 
Decided: 1998
Castle Rock Entertainment, Inc. v. Carol Publishing Group Inc.
In 1994, Beth Golub (publisher: Carol Publishing Group) created a trivia book called “
The Seinfeld Aptitude Test
” containing 634 trivia questions about the television show Seinfeld. The copyright holders of the show (Castle Rock Entertainment) were extremely selective about what products they allowed to be created in curating their specific brand for the show, and did not like the creation of this product. The two questions at the heart of this case were whether the trivia book infringed on Seinfeld’s derivative work in the first place, and if so, whether using protected aspects of Seinfeld to create the trivia book constitutes fair use.
The courts determined that this was 
not 
fair use, because it was commercial and not transformative (no commentary or analysis of Seinfeld, as a research or critique work would be). Castle Rock did not have any trivia books themselves so it was not in direct competition with any products they owned, but the courts determined that “
copyright law must respect [the] creative and economic choice of plaintiffs not to exploit a particular market
,” which differs from the mandate for compulsory licenses for musical works.
🎬
📚 
Harry Potter Lexicon
 — Decided: 2008
Warner Bros. Entertainment Inc. v. RDR Books
In 2000, Steve Vander Ark created an online 
Harry Potter Lexicon
 which was a non-profit online encyclopedia of events in the Harry Potter series, which even 
J.K. Rowling acknowledged using
. In 2007, RDR books approached Vander Ark to publish a for profit-version in
 book form
, which is what this lawsuit concerns. Rowling (plaintiff for the books) and Warner Bros. Entertainment (plaintiff for the films) took issue with their copyrighted content being used to produce this for-profit book, especially considering Rowling had wanted to create her own encyclopedia. The courts decided that this was 
not fair use
, because 
“although the Lexicon has a transformative purpose, its actual use of the copyrighted works is not consistently transformative
.” The original non-profit work was not an issue because reference works are generally encouraged under copyright law, but the for-profit nature of the book actually competed with the original copyright holders. This is an instance of infringement without it technically being a derivative work; even though the court didn’t think the encyclopedia was a derivative work, the encyclopedia did violate Rowling’s reproduction right for taking excessive verbatim quotes.
Takeaway for Generative AI Copyright Court Cases:
These three cases together indicate that there is a special case for music copyright that mandates you cannot prevent certain specific derivative works (cover versions) from being made (in exchange for royalties), but all other copyright holders maintain more creative control over their copyrighted content. Fair use and other exceptions limit the rights of copyright holders, but, especially in cases of creative property, the courts have tended to protect copyright holders.
Technologies Enabling Copyright Infringement
📼
 
VCR
 — Decided: 1984
Sony Corp. of America v. Universal City Studios, Inc.
Popularly known as the “Betamax” case, this case was so divisive that it was an 8-year litigation filed in 1976 and first argued in front of the Supreme Court in January of 1983 that then had to be re-argued in October, ultimately decided in January of 1984. The case concerned the legality of making home recordings of content broadcast on television for personal use and ability to rewatch at a later time (
time shifting
), at the time with the use of 
Betamax
 or other VCRs. Specifically, this case concerned whether liability fell on the manufacturer of these devices for enabling what could be considered prolific instances of copyright infringement by the users.
This case was ultimately decided by borrowing logic from 
patent law (35 U.S. Code § 271)
 that confines “
contributory infringement
”–think of it as aiding and abetting infringement by another person–to the knowing sale of a component made for use in connection with a patent. Extended to this case, they decided that if devices were sold for a legitimate purpose and had substantial use that was 
not
 copyright infringement (e.g., non-infringing home videos), then the manufacturer will 
not be liable
 
for copyright infringement committed by its users.
This avenue of interpretation could potentially lead to a favoring of the technology companies creating generative AI models (e.g., OpenAI, Microsoft) in terms of the output infringement (not relevant to training infringement). They could argue that the majority of uses of their technology were for legitimate purposes and thus should not be held liable for individual acts of infringement by the users.
🛜 ISP v. Scientology — 
Decided: 1995
Religious Technology Center v. Netcom On-Line Communication Services, Inc.
In 1995, the Religious Technology Center (a corporation that oversees the copyrighted and trademarked content of the Church of Scientology) got upset that a former Scientology Minister (allegedly) posted confidential copyrighted Scientology material online. As a result, they sued everyone they could think of: the former minister for posting the material, the California 
computer bulletin (message) board
 service that he posted on, and Netcom, the internet service provider through which the message board accessed the internet. The aspect we care about here is specifically the ISP case: a federal trial court decided that an internet provider could not be directly or 
vicariously liable
 for infringement of a user, but it might be 
contributorily liable
 and allowed this allegation to continue to litigation. The courts distinguish between (at least) two secondary types of liability–vicarious liability is important when the relationship between two parties is such that someone (or an entity) can be accountable for the infringement of another (e.g., employer and a supervisor) which is applicable here, whereas contributory liability is a more general case of enabling others to infringe using technology you provide.
This has implications for technologies that purely enable copyright infringement by users and are far removed from the act — internet service providers are certainly further removed from the acts of infringement than the owners of models are, but model providers may try to make the argument they are closer to ISP than they are to the end user.
▶️ 
Youtube
 — Decided: 2011
Viacom International Inc. v. YouTube, Inc.
Though YouTube may appear to be a bit 
stricter today
 on users who upload copyrighted content, back in the early 2000s the site was rife with countless videos containing recordings of popular media. In 2007, Viacom (an entertainment company) filed a lawsuit against YouTube (and owner Google) for $1 billion claiming that YouTube allowed users to upload 
more than 150,000 videos Viacom owned
 (e.g., clips of television shows like SpongeBob and The Daily Show). More than this, they allege that YouTube encouraged this behavior in order to increase site traffic.
Decided by the Second Circuit (not The Supreme Court), the court ruled that the safe harbor provisions (
Section 512
) of the 
Digital Millennium Copyright Act
 passed in 1998 shielded online platforms from copyright infringements of their users. For those familiar, this is quite similar to 
Section 230
 which shields all social media sites from liability of any content their users post by not treating providers of these sites as publishers of the content. In the case of Youtube vs. Viacom, Youtube was 
not held liable
 for the copyright infringement of its users.
📺 
Aereo
 — Decided: 2014
American Broadcasting Cos., Inc. v. Aereo, Inc.
In 2012, Barry Diller funded a technology company called 
Aereo
 that leased individual antennas to their subscribers which enabled them to watch live and time-shifted content over the internet (streaming before Netflix figured it out). Notably separate from other cases in this section, the alleged infringement here was done solely by the company (Aereo), not the users watching content through the service. This seemingly blatant copyright infringement was exploiting a loophole in copyright law that attempted to prevent the streaming from being considered a “public performance” since it was shown individually to subscribers. In 2014, the Supreme Court ruled in a 6–3 decision that Aereo 
was infringing
 because it acted with an “
overwhelming likeness to cable companies … [and] performs petitioners’ works ‘publicly’
.” They also stated in their decisions that they did not want to discourage the emergence of new technologies (enter: Netflix, Hulu, HBO Max, Amazon Prime…). Notably, the dissenting three cited the pro-technology spirit of the Sony VCR case as a reason for Aereo to not be held liable.
This case is important especially in light of the VCR and YouTube decisions because it speaks to the court’s distaste for actors who are clearly exploiting loopholes in existing statutes rather than pursuit of legitimate non-infringing means.
Takeaway for Generative AI Copyright Court Cases:
These cases all demonstrate the tendency of the courts to not hold liable creators of technology that enable users to potentially infringe upon copyright if the larger purpose of the technology is for non-infringing means. This has large implications for how the court may treat the infringement of outputs of generative AI models. However, none of these are perfect comparisons with generative AI models — in order for the users to infringe upon someone’s copyright during output, the model would necessarily have had to ingest that copyrighted content during the training phase (barring infringement due to the integration and replication of content via internet search features). The users who would then be infringing upon copyright during the outputs are not acting of their own volition to infringe, but rather do so unknowingly by using a tool that was engineered to sometimes infringe upon copyright. This could move the needle away from models simply being “enablers” of copyright infringement and shift the liability towards the creators of these tools.
Using Copyrighted Content for Legitimate Purposes
🎮 
Sega v. Accolade
 — Decided: 1992
Sega Enterprises Ltd. v. Accolade, Inc.,
In 1988, Accolade (a video game publisher that was at the time competing with Sega) wanted to utilize Sega’s licensing deal that allowed for third-party developers to release games on Sega consoles. In order to ensure that their games were compatible with Sega’s systems, Accolade 
reverse-engineered
 Sega’s source code (in this instance it copied copyrighted source code in order to understand how it worked). An appellate (not Supreme) court ruled that this 
constituted fair use 
and was 
not infringing
 because Accolade used the copies of source code for a legitimate purpose. This is a relevant case since the fair use argument only stands due to the legitimate nature of the reverse-engineering: if they had reverse engineered to copy stylistic elements of Sega games and infringe upon trade secrets in this nature, then the fair use argument would not have worked.
Though this case is typically situated in copyright law in terms interoperability of competing technology platforms, this can be thought of analogously to the use of competing copyrighted works (e.g., music, text, visual art) in order to train AI models that will then produce content that competes with them. The courts will have to decide if the AI companies are using this content for legitimate purposes.
📚 
Google Books
 — Decided: 2015
Authors Guild, Inc. v. Google, Inc.
Google Books
 is a service provided by Google that digitizes millions of books in order to allow users to search for any book present in Google’s digital library, and then Google provides a snippet (a preview) of the book for free to the public. The Authors Guild (along with many other copyright holders) sued because Google scanned all of these books without asking permission or paying any license fees. Google argued 
fair use
 as this was a transformative action, and the 
courts agreed
 stating that Google Books made available information about the books without constituting a substantial substitute for the book.
👩‍💻 
Java
 — Decided: 2021
Google LLC v. Oracle America
Java is a widely used programming language that allows programmers to build things such as mobile apps. When Google created Android (the platform used on Google mobile devices), it copied roughly 11,500 lines of code from Java in order to enable programmers who were already familiar with Java to use the platform (for context, Android has 
12–15 million lines of code
 total; 11,500 is less than 1%). Oracle (copyright owner of the Java computer platform) sued for $8.8 billion in damages because Google did not obtain a license to use Java source code. The Supreme Court decided (in a 6–2 majority) that Google’s use of Java 
constituted fair use 
in part because it promoted creative progress by enabling millions of programmers to continue making new creative content.
Takeaway for Generative AI Copyright Court Cases:
These cases all indicate that there are instances when the court finds no issue with using copyrighted content, as long as the resulting product is of legitimate use. These are especially relevant when the resulting product serves as a means towards a positive outcome for the public, which is an argument that generative AI companies will make when they argue for fair use. They will argue that their generative AI models have enabled so much good in the world (e.g., translations, creative outputs, accessible educational content) that they needed the copyrighted content to enable these positive capabilities.
Concluding Thoughts
In this post we went over several foundational cases that may all come up during the upcoming 
slew of lawsuits
 about copyright infringement by generative AI companies. We know the courts tend to favor protecting creative works from derivative outputs, but do not tend to hold accountable creators of the technologies that enable infringement which will hold relevance when evaluating whether the tech companies will be liable for infringing outputs of these AI models. Finally, the crux of these decisions may come down to the question of whether these companies used copyrighted materials for a legitimate lawful and non-infringing purpose for the potential betterment of the public.
—— —
Disclaimer
: The information in this post does not, and is not intended to, constitute legal advice and is for general informational purposes only.
Acknowledgements:
 The author would like to thank 
Dr. Peter DiCola
 for his invaluable discussion and feedback on this work, as well as 
Dr. Nick Diakopoulos
 for his helpful feedback.]]></content:encoded>
</item><item>
<title><![CDATA[A Right to Personal Use of Content with LLMs?]]></title>
<link>https://generative-ai-newsroom.com/a-right-to-personal-use-of-content-with-llms-7bbad71c13de?source=collection_home_page----df04cefca135-----36-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[News]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Terms Of Service]]></category><category><![CDATA[New York Times]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2024-08-15</pubDate>
<atom:updated>2024-08-15</atom:updated>
<content:encoded><![CDATA[A Right to Personal Use of Content with LLMs?
Nick Diakopoulos
Follow
6 min read
·
Aug 15, 2024
--
2
Listen
Share
When it comes to catching up on news I’ve recently been reading less and listening more. The proliferation of news sites that offer audio versions of their articles — oftentimes read by a generated AI voice — has been helpful while I rock and go for long walks. But in the event a publisher doesn’t offer an audio version of an article I go to the 
Reader app 
from 
ElevenLabs
, a generative AI service which can take a link to a web page and read it out in any number of generative voices of my choosing.
That’s nice when it all works. But some news publishers appear to be blocking services like ElevenLabs from accessing their content this way. I recently tested ten national publishers and three of them weren’t functional with the service [1]. Several 
recent
 
news industry reports
 on generative AI have touted the potential of the technology to improve accessibility of content, so it’s frustrating to see content get so buckled down that end-users can’t use generative AI to adapt and make it accessible according to their own needs.
To take one example, New York Times content is blocked in ElevenLabs’ Reader app. I get an “unknown error” (see Figure 1a, left) if I drop a link to a NYT article in the app. Using 
WebProxyTool
, which intercepts web traffic originating from apps on the phone, I was also able to view the underlying error message provided by the ElevenLabs API when a NYT article link was provided (see Figure 1b, right). This doesn’t appear to be a technical block given that the 
New York Times robots.txt file
 does not block the User-Agent of the app. It’s unclear whether this could be the result of a chilling effect from the NYT lawsuit against OpenAI, or if perhaps the New York Times has explicitly communicated to ElevenLabs that they were not to download their content.
Figure 1a. (left) A vague error message is provided by ElevenLabs Reader app when trying to access a New York Times article link. Figure 1b. (right) The detailed status message provided by the ElevenLabs API when requesting a NYT article in the Reader app.
Navigating Conflicting Terms of Use
As a user this is all a bit frustrating. Things should just be easy and work, right? In their copyright notice, the New York Times graciously 
provides me permission
 to “download material from The New York Times on the web (one machine readable copy and one print copy per page) for your personal, noncommercial use only”. Thank You! I would like to seamlessly use that machine readable copy to have a generative AI voice of my choosing read the article for my personal use.
The NYT 
Terms of Service
 go into more detail, saying “You may not modify, publish, transmit … create new works from … any of the Content”. This would seem to block my use case, but there’s an exception “for your personal use”. In my opinion, using ElevenLabs to read out an article for me is arguably personal use and should be permitted. Other banned uses explained in the Times’ ToS reflect reactions against tech companies scraping their content: I am prohibited to “use, access, or collect the Content, data or information from the Services using automated means,” and training, fine-tuning, or grounding of AI models using the content is verboten.
As legal scholar Pamela Samuelson has 
argued
, “There are several reasons why 
private and personal uses
 [emphasis mine] of copyrighted works should either be given a broad scope of fair use, or exempted from copyright control. For one thing, personal uses of copyrighted works, such as reading, viewing, listening, and otherwise enjoying them, generally do not interfere with commercial exploitations of protected works.” So, at least one highly-informed interpretation of fair use, as well as the Times’ own exception in its contract would seem to support personal use of their content to read it aloud with ElevenLabs’ Reader app. Note, annoyingly, that 
contracts can foreclose fair use
, and so the fact that the New York Times provides an exception for personal use seems relevant.
To be sure, there are ways around this block. My ability to easily access a link pasted into the ElevenLabs app is impeded, but with some additional effort I can “print” my personal copy of an article into a PDF and then share that with the app to read. These kinds of loop holes are commonplace in web browsers, where it appears that on-device manipulation of content (presumably for personal use) is largely permitted, or is at least not policed. For instance, in Safari on mobile, I can transform New York Times’ content into a “reader” mode which strips out ads, I can have the website content translated to another language, or, in fact, I can use Apple’s inferior text-to-speech technology to read the article aloud. The Arc browser can 
summarize a webpage
, or let me 
ask questions of content on a page
 I’ve loaded. And 
newer versions of Chrome experimentally embed Google’s Gemini Nano model
 for “local processing” of content on the page.
Unlike the ElevenLabs use case, manipulating content in the browser doesn’t require sending content to a third-party cloud service, or having that service download the content on your behalf. And therein lies the crux of the issue. As restrictive as the New York Times’ Terms of Service are with respect to AI use-cases, it seems to be a clear reaction against the even more egregious data-grabbiness of AI providers like ElevenLabs. When you input data to ElevenLabs’ services their 
Terms of Service (non-EEA) stipulate
 that “You hereby grant to ElevenLabs alicense [sic] to use, reproduce, modify, adapt, publish, translate, create derivative works from, distribute, publicly or otherwise perform and display, and use your Input to provide the Services (including the trust and safety features therein), to improve the Services, and to develop new services and products.” No wonder the New York Times blocks them — I don’t blame them for not wanting their content used to train another company’s AI model.
Balancing Fair Use and Corporate Control
So we have two companies that are fighting over terms of service for the content coming out of one organization, and going into another. It’s alienating for a consumer of either organizations’ services. The whole unfortunate situation leaves the end-user who simply wants to exercise their fair use and contractual exception for personal use of the content out cold. What could the media system look like if we were to take the user experience and fair use rights more seriously? In that world consumers would be able to easily and frictionlessly exercise their rights to fair use (i.e. personal use), including by using third-party services to access the content in a way that respects the personal use context.
An obvious solution here would be for AI companies to be less aggressive with their policies about training on any data submitted to their services. Sometimes users will be exercising their fair and personal use of copyrighted materials, and this needs to be reflected in contractual Terms of Service. ElevenLabs does provide an opt-out for use of Content “to improve the Services” however it’s unclear that this extends to using the Content for developing new services and it requires filling out a form by the user anyway, so it’s yet more friction. Moreover, ElevenLabs puts all of the responsibility on the end-user to ensure they have “all the rights necessary to grant us the license described”, so it’s effectively not ElevenLabs fault if any copyrighted content from the NYT ends up getting input to their services.
I don’t understand how we can have a data regime that can both respect fair use for an individual, but have that fair use result in another entity claiming they can then use the content in any way they want (unless they also think it’s fair use?!). That’s like me recording a TV show for my personal consumption and having Samsung decide that they can now train whatever AI they want using that TV show content because I “input” the content to their device. It’s high time for AI policy and governance (dare I say legislation) to address the issue of Terms of Service. Terms of Service could be made machine-readable and interoperable, with different rights respected and enforced in code. A request to ElevenLabs authenticated as personal use would then preclude ElevenLabs from training on any inputs. Whether this, or some other route, we should be looking for ways to protect end-user rights, including the right to fair and personal use of content with LLMs, whether local or in the cloud.
—
[1] I tested the ElevenLabs Reader app with 10 national publishers and found that The New York Times, Wall Street Journal, and Politico (.com but not their .eu site) were not functional, but that CNN, Fox News, USA Today, Washington Post, AP News, The Atlantic, and Time, were functional.]]></content:encoded>
</item><item>
<title><![CDATA[How to Query Your Data Using Natural Language with LlamaIndex?]]></title>
<link>https://generative-ai-newsroom.com/how-to-query-your-data-using-natural-language-with-llamaindex-8988904e6d25?source=collection_home_page----df04cefca135-----37-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Llamaindex]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Large Language Models]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Mowafak Allaham]]></dc:creator>
<pubDate>2024-08-07</pubDate>
<atom:updated>2024-08-07</atom:updated>
<content:encoded><![CDATA[How to Query Your Data Using Natural Language with LlamaIndex?
Mowafak Allaham
Follow
13 min read
·
Aug 7, 2024
--
Listen
Share
Image was generated using GPT-4o: “Generate an image for a blog post that provides an overview of an AI tool called “LLamaIndex” for data analysis in journalism.”
Many journalists can benefit by being able to extract insights and find answers to questions in the heaps of documents and data they may collect in the course of reporting. How can generative AI help facilitate this? Previous GAIN blog posts have illustrated how less technical journalists can analyze data by integrating 
open source Large Language Models (LLMs) into Google Sheets
 or by building an 
agentic
 solution that relies on the 
ReAct
 (Reason + Act) prompting framework to answer questions written in natural language. But what if data journalists want to build a system for the newsroom to enable their colleagues with less technical expertise to query data from files using natural language, without the need for deep expertise in prompt engineering?
In this post, I present an approach to work with data using 
LlamaIndex
 — an open source framework for connecting custom data sources to Large Language Models (LLMs). In particular, I outline a query pipeline that enables journalists to query semi-structured data, such as tabular data stored in 
.csv
 files, using natural language without using any database or query languages. Data journalists/scientists may re-configure this pipeline and potentially deploy it as a 
web application
 in the newsroom. Accordingly, this post is oriented towards data journalists and scientists who are familiar with coding and data analysis. It aims to illustrate how LlamaIndex works and how it can be leveraged to set up a query pipeline as a tool for data analysis.
The following sections provide a brief description of the data source I chose to illustrate this post and how LlamaIndex works on this dataset. Later, I delve into the details of the Text-to-Pandas architecture, powered by a query pipeline, that enables the functionality of querying data in natural language. Finally, I conclude with a sample evaluation of LlamaIndex and its current limitations in the context of querying a 
.csv
file for data analysis.
Data Source
The query pipeline I will set up in the following sections can be leveraged in conjunction with LlamaIndex 
Data Connectors
 to query sources of unstructured data such as documents stored in Microsoft Word or PDF formats (as illustrated by these 
examples
), 
structured
, and semi-structured data (e.g, tabular data saved in a 
.csv
file).
To demonstrate the capabilities of LlamaIndex on semi-structured data, specifically data saved in
.csv
files, I chose a 
dataset
 describing the county-level votes for U.S presidential elections from 2000 to 2020 that was retrieved from 
MIT Elections Data and Science Lab
. As we are approaching the U.S presidential elections in November, I reasoned that journalists might use this data for understanding previous voter turnout and patterns across states as they are covering the elections.
What is LlamaIndex?
LlamaIndex
 is an open-source framework that can be used to 
ingest
 (i.e., load data), 
index
, and query your data when building generative AI applications powered by LLMs such as 
Retrieval Augmented Generation (RAG) pipelines
 for question answering, 
chatbots
, and 
agents
. As mentioned earlier, one of the appealing and key features of LlamaIndex is its 
Data Connectors
 that are hosted on 
Llama Hub
. This feature offers a range of plug-and-play solutions to connect your application to various data sources such as 
Twitter
, 
Notion
, 
database
, or 
webpage
s to build 
query pipelines
 — which I will elaborate on in the next section — that enables 
chatting
 with the content of these data sources in natural language. In the following section, I will demonstrate how query pipelines can be used to convert user queries from text to Pandas expressions in Python that can be executed on a dataframe containing voting data loaded from a 
.csv
 file.
Text-to-Pandas Architecture
Query pipelines
 are objects used to create workflows that combine and connect different stages of a data pipeline in a sequential order that can be run end to end. This pipeline enables users to query various types of data sources using Large Language Models (LLMs) such as GPT-4o. The details of this pipeline can be broken down into three stages (as outlined below) and the corresponding prompts for some stages that GPT-4o will use to complete the task. Although LlamaIndex 
supports open-source models
, I decided to proceed with GPT-4o for its easy integration with LlamaIndex.
Stage 1: Convert questions to executable python expressions. 
This stage entails prompting GPT-4o to convert users queries to executable Python expressions that can run on the dataframe containing the loaded data from the 
.csv
 file. Depending on the type of query users are requesting, GPT-4o may return python expressions that could alter the data such as modifying values or removing rows and columns. Because the LLM has access to the 
eval()
 function in Python via LlamaIndex, this poses a 
security vulnerability
 that LlamaIndex 
warns
 about due to its potential impact on both the data and the system where the pipeline is deployed. It’s recommended to use the query pipeline to query the data for insights instead of relying on it to transform data, and to run the code in a sandbox environment where there is no chance it could interfere with your primary systems. To convert user queries to Python expressions, I will use the following prompt in this stage and pass it to GPT-4o via the query pipeline setup:
pandas_prompt = (
    "You are working with a pandas dataframe in Python.\n"
    "The name of the dataframe is `df`.\n"
    "This is the result of `print(df.head())`:\n"
    "{df_top_5_records}\n\n"
    "Follow these instructions:\n"
    "1. Convert the query to executable Python code using Pandas.\n"
    "2. The final line of code should be a Python expression that can be called with the `eval()` function.\n"
    "3. The code should represent a solution to the query.\n"
    "4. PRINT ONLY THE EXPRESSION.\n"
    "5. Do not quote the expression.\n"
    "Query: {user_query}\n\n"
    "Expression:"
)
Note: variables in the curly brackets will be replaced with actual values resulting from each step of the query pipeline. For more details, read about 
f-strings
 in Python.
This prompt includes four key pieces of information:
df
 : is the name of the dataframe I would like GPT-4o to consistently use when referencing it, instead of making one up.
df_top_5_records
: refers to the output of 
df.head()
 which includes the first 5 rows of a dataframe. Including this information in the prompt will inform GPT-4o about the column names, data types, and the overall formatting of the data.
Five instructions elaborating for GPT-4o the task of converting queries to python expressions as outlined in the prompt above.
user_query
: refers to the submitted question by a user to query the data and receive an answer.
To execute this stage and retrieve the Python expression from GPT-4o, I first had to create a prompt template object that has the aforementioned prompt along with its values. Prompt templates serve as an interface between user prompts and LlamaIndex modules. In addition, I had to instantiate an output parser to parse out the output from GPT-4o and retrieve a well formatted answer that can be used as an input to the following stage. To this end, I used the custom Pandas output parser supported by LlamaIndex which parses pandas syntax and makes it accessible for LLMs.
# Prompt template object containing pandas_prompt and the first five rows of a dataframe
pandas_prompt_object = PromptTemplate(pandas_prompt).partial_format(df_top_5_records=df.head(5))
# Prompt template object containing response_synthesis_prompt  
response_synthesis_prompt = PromptTemplate(response_synthesis_prompt)
# Output parser
pandas_output_parser = PandasInstructionParser(df)
# Instantiate GPT-4
llm = OpenAI(model="gpt-4o",api_key="YOUR_API_KEY",max_new_tokens=300)
Next, I created a query pipeline consisting of modules (i.e, components) and links (i.e. steps) controlling how modules are connected together.
qp = QP(
modules={
"input": InputComponent(), # parse and process inputs.
"pandas_prompt": pandas_prompt, # prompt to convert user queries to Python expressions.
"expression_to_llm": llm, # LLM used for processing user query.
"pandas_output_parser": pandas_output_parser, # parser to render the output in a readable format.
"response_synthesis_prompt": response_synthesis_prompt, # prompt to generate the final answer.
"pandas_to_answer_llm": llm, # LLM synthesizing the findings from the dataframe into a single answer based on the data.
},
verbose=True,
)
The 
modules
parameter of the query pipeline includes the necessary components (i.e., building blocks) for the pipeline to function. This includes defining the objects that will pass the input to the pipeline, the relevant prompts at each stage, and the LLM (GPT-4o) that will be used as part of the pipeline.
Link
is another data structure that LlamaIndex supports to connect modules together to create various stages of a query pipeline at a more granular level. A link describes the module that will be executed next, the input to the module, the prompt to be used in that module, and its output.
Returning back to the task of converting user queries into Python expressions, I created three links each performing a subtask. The first link passes a user query to 
query_to_python_expression
 prompt to populate the place holder 
user_query
 with the query entered by the user. Next, the 
query_to_python_expression
 prompt with the populated user query will be passed as an input to GPT-4o, per the second link. Lastly, the output of GPT-4o will be passed to 
pandas_output_parser
 to get a well formatted response that can be used in the subsequent stage pertaining to executing the Python expression on the dataframe.
qp.add_links([
    # First chain of processing
    Link("input", "pandas_prompt",dest_key="user_query"),
    Link("pandas_prompt", "expression_to_llm"),
    Link("expression_to_llm", "pandas_output_parser"),
)]
Stage 2: Executing python expressions on pandas dataframe. 
To execute the Python expression from Stage 1, I extended the set of links from the previous stage to include a few additional links that leverage the following prompt:
# Response synthesis prompt (response_synthesis_prompt)
response_synthesis_prompt = (
    "Given an input question, synthesize a response from the query results. Always include numbers in your answer to support your claims. \n"
    "Your response must ALWAYS include three components: 1) Response, 2) Python expression to query the dataframe, and 3) brief explanation of the Python expressoin. \n"
    "If the answer requires a full dataframe, display it in the results."
    "Include in your answer the column names you used to filter data on \n"
    "Query: {python_expression}\n\n"
    "Pandas Instructions (optional):\n{pandas_instructions}\n\n"
    "Pandas Output: {pandas_output}\n\n"
    "Response: "
)
Similar to the previous stage, this prompt also has three key components: 
python_expression
referring to the query obtained from the previous stage, 
pandas_instructions
 referencing the output after locally executing the Python expression on a pandas dataframe, and 
pandas_output
 referring to the final output that is synthesized by GPT-4o using the above prompt after populating all the aforementioned placeholder variables.
Using 
Links
, the Python expression in stage 1 gets executed locally on the pandas dataframe based on the 
response_synthesis_prompt
, and then the results are forwarded to the 
pandas_output
 field in the same prompt. The updated set of links looks like this:
qp.add_links([
    # First chain of processing
    Link("input", "pandas_prompt",dest_key="user_query"),
    Link("pandas_prompt", "expression_to_llm"),
    Link("expression_to_llm", "pandas_output_parser"),
    # Links to response synthesis prompt
    Link("input", "response_synthesis_prompt", dest_key="python_expression"),
    Link("expression_to_llm", "response_synthesis_prompt", dest_key="pandas_instructions"),
    Link("pandas_output_parser", "response_synthesis_prompt", dest_key="pandas_output")
])
Stage 3: Synthesize a final answer. 
To synthesize the final answer I will pass the output from stage 2, which includes the 
response_synthesis_prompt
 with its populated values (from stage 2) and the result, to GPT-4o. To do so, I added one more link to account for this stage 
Link("response_synthesis_prompt","pandas_to_answer_llm") 
that forwards the populated 
response_synthesis_prompt
 to GPT-4o. Accordingly, the final set of links in the query pipeline is:
qp.add_links([
    # First chain of processing
    Link("input", "pandas_prompt",dest_key="user_query"),
    Link("pandas_prompt", "expression_to_llm"),
    Link("expression_to_llm", "pandas_output_parser"),
    # Links to response synthesis prompt
    Link("input", "response_synthesis_prompt", dest_key="python_expression"),
    Link("expression_to_llm", "response_synthesis_prompt", dest_key="pandas_instructions"),
    Link("pandas_output_parser", "response_synthesis_prompt", dest_key="pandas_output"),
    # Final link to the LLM
    Link("response_synthesis_prompt", "pandas_to_answer_llm")
])
Executing the Query pipeline
To put this query pipeline into action, I call the run method supported by LlamaIndex query pipelines. Under the hood, the user query is converted into a Python expression that gets executed on the pandas dataframe, LlamaIndex then parses the output, and then forwards the results to GPT-4o to synthesize a final answer.
response = qp.run(query_str="Name the counties with the highest votes for democrats and compare them to counties that voted for republicans.")
Evaluation
The utilization of LLMs for developing generative AI applications should always be paired with an evaluation process to confirm the validity of the generated output by the LLMs and assess the performance of the applications relying on these outputs. In the context of this blog post, it is recommended for data journalists intending to use LlamaIndex to 1) ensure the alignment of the intermediary steps between users’ queries and the generated expressions by the LLM, and 2) continuously evaluate whether the generated response by the LLM is factual with respect to the content of the file being queried.
To evaluate the generated Python expressions of the model and confirm that the query pipeline is executing the expression correctly, I instructed the LLM as part of the 
response_synthesis_prompt
 to always include in the output the query expression generated by the LLM and also provide an explanation of the generated expression so I understand the logic behind it. This is most useful when the editor has the requisite technical knowledge to understand and critique these expressions and explanations as needed to ensure the response is accurate.
On a high level, the LLM is behaving as expected by generating the relevant expression to my query that is also executable and well-explained. For instance, when querying the data using the query pipeline to “
Name the top counties that voted for a democratic presidential candidate in 2004
”, I get the following response:
The top counties that voted for a Democratic president in 2004 were Los Angeles, Cook, Montgomery, Orange, and Jefferson. Los Angeles had the highest number of votes with 1,907,736.
Python expression:
df[(df['year'] == 2004) & (df['party'] == 'DEMOCRAT')].groupby('county_name')['candidatevotes'].sum().nlargest(5)
Explanation:
This Python expression filters the dataframe to include only data from the
year 2004 and where the party is Democrat. It then groups the data by county
name, sums the candidate votes, and returns the top 5 counties 
with the largest total candidate votes.
As shown in the response, the model correctly picks the right set of columns to generate an expression, LlamaIndex then executes the expression, and returns a synthesized response with the correct data. However, the model is prone to hallucinate when submitting complex queries which emphasizes the need to continuously validate the generated answers by AI. For example, when asking “
Who were the democratic candidates in 2004 and who were they competing with? and who won?”
 the model generated an expression that correctly addresses the first part of the question 
df[(df['year']==2004)&(df['party']=='DEMOCRAT')]['candidate].unique()
. However, that same expression is not sufficient to address the remaining questions about “
who the democratic candidate was competing with and who won the elections”.
 Yet, the LLM relied on its own knowledge from its training data to specify that George W. Bush was the Republican candidate and he won the 2004 election despite the absence of such information from the generated python expression by the LLM as shown below:
In the 2004 election, the Democratic candidate was John Kerry. He was competing against the Republican candidate George W. Bush, who won the election.
Python expression:
df[(df['year'] == 2004) & (df['party'] == 'DEMOCRAT')]['candidate'].unique()
Explanation: This Python expression filters the dataframe to only include rows 
where the year is 2004 and the party is Democrat. 
It then selects the unique values in the 'candidate' column for these filtered rows.
In light of these examples, it is recommended for data journalists to have an evaluation criteria that is grounded in assessing the relevance of the generated Python expression in the final answer with respect to their original queries. This may also entail some prompt engineering by refining the phrasing of the query submitted to the pipeline so the LLM can correctly parse the request. Furthermore, assuming that the generated expression by the LLM is relevant, it remains crucial to assess the factuality of the answer. This can simply be verified by running the resulting Python expression on the dataframe to confirm whether the LLM is leveraging the right sample of data to conclude the answer. Without such evaluations it becomes challenging to determine the efficacy of emerging tools, such as LlamaIndex, that are integrated with AI and their applications in the newsroom.
Limitations
By extensively testing the query pipeline across various queries that vary in complexity, I noticed that the model may occasionally make up column names from its knowledge base instead of the data file. One such case was evident when requesting to “
Compare the voting pattern between the republican and democratic candidate in the last election in 2020.
” The LLM responded with “
There is no data available in the dataset for the last election in 2020 to compare the voting pattern between the republican and democratic candidate
.” even though the data was there. This issue was due to the model hallucinating the formatting for the names of “JOE BIDEN” and “DONALD TRUMP” instead of preserving the way these names are formatted in the data file as “JOSEPH R BIDEN JR” and “DONALD J TRUMP”, respectively. Including unique values in the 
pandas_prompt
 in Stage 1 for each column in the dataset could mitigate against this issue.
Python expression:
df_2020 = df[df['year'] == 2020]
winner_2020 = df_2020.groupby(['state', 'candidate'])['candidatevotes'].sum().idxmax()[1]
df_2020[(df_2020['candidate'].isin(['JOE BIDEN', 'DONALD TRUMP'])) & (df_2020['candidate'] == winner_2020)]
Explanation:
The Python expression filters the dataset to only include data from the year 2020.
It then identifies the winning candidate in each state based on the candidate with
the highest total votes. Finally, it attempts to display the details of the winning candidate 
(either JOE BIDEN or DONALD TRUMP) in the last election in 2020, but since there is no data available,
the output is an empty DataFrame.
In addition, I observed that LlamaIndex doesn’t support complex queries that would require multi-indexed dataframes. For instance, re-running the previous query with the adjusted candidate names resulted in the following LlamaIndex error: “
can’t unstack multi-indexed data
” . This creates a disadvantage for adopting LlamaIndex for more complex data analysis use cases. One potential way to resolve these performance bottlenecks, but requires technical expertise, is to leverage 
function calling
 so the LLM can make explicit calls to functions that perform multi-indexing of the dataframe and use the resulting dataframe to execute user queries to generate the final answer. This solution enables more control over the implementation of the intermediate steps in comparison to LlamaIndex. This issue could also be addressed in future 
releases
 by LlamaIndex. Accordingly, it is recommended for data journalists/scientists to regularly check the releases page of the LlamaIndex GitHub 
repository
 for bug fixes and feature enhancements.
Closing Thoughts
Open source AI tools such as LlamaIndex present an opportunity for journalists to engage with the open source community and potentially request feature 
enhancements
 for these tools to account for use cases in journalism. As generative AI tools continue to evolve, we invite data journalists to consider how these tools can enhance reporting and boost productivity for journalists in the newsroom. Could this approach be useful, for example, to index and query a large corpus of documents in an investigation? Or to query transcripts or reporting notes? Whether you are seeking an in-depth analysis of your data or further exploring it to unlock fresh insights, these tools could contribute new ways of unlocking the value of data for journalists.
—
The data and code for this blog post are available on a 
Notebook
 hosted on Google Colab.
Acknowledgements: The author would like to thank 
Dr. Nick Diakopoulos
 for his helpful feedback.]]></content:encoded>
</item><item>
<title><![CDATA[“This Article is AI-Generated”: AI Disclosure and Labeling for News Content]]></title>
<link>https://generative-ai-newsroom.com/this-article-is-ai-generated-ai-disclosure-and-labeling-for-news-content-73d6e6cb6d50?source=collection_home_page----df04cefca135-----38-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News Media]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Transparency]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Jessica Zier]]></dc:creator>
<pubDate>2024-08-01</pubDate>
<atom:updated>2024-08-01</atom:updated>
<content:encoded><![CDATA[“This Article is AI-Generated”: AI Disclosure and Labeling for News Content
Jessica Zier
Follow
12 min read
·
Aug 1, 2024
--
2
Listen
Share
This past spring, Hoodline, a hyper-local news outlet based in various US cities, was found to be using AI for article generation. That alone is not worth getting upset about, but the lack of 
transparency
 about the outlet’s AI use 
drew criticism from readers and journalists
 alike. Particularly troubling was the misleading use of AI-generated personas, which included bylines and headshots for the supposed journalist authors. Hoodline does not have a masthead showing who’s a person and what’s an AI, leaving readers to figure that out for themselves.
Hoodline has since replaced the AI-generated bios and headshots with a small “AI” icon that now appears next to fake author personas on the site (see the icon just before “By Nina Singh-Hudson” in Figure 1). But this AI label — a visual cue that signals AI use — is a hollow gesture which conveys little actionable information to the reader. This and other cases, such as
 CNET’s “journalistic disaster” using AI
, have reignited discussions about transparency when using AI in journalism. In Hoodline’s case, the damage to any credibility the site had was already done and retroactive labels did little to fix audience concerns about the outlet’s deceptive practices and fake personas. But, does that mean AI labeling is always doomed as a way to enhance transparency?
Figure 1. A screenshot of a Hoodline article containing an AI label in the byline
Generative AI & Transparency
While the Hoodline saga paints a dramatic tale of deception with AI, the use of generative AI in the newsroom is becoming increasingly common, as evidenced by this blog — and is not inherently a bad practice though it 
can certainly be mismanaged
. Either way, the increased use of generative AI in journalism heightens the need to figure out how to communicate AI use to audiences, especially given the 
ethical concerns in the industry around non-disclosure
. Recent policy developments around the world are also driving attention to the potential of AI transparency to mitigate AI risks. The 
European Union’s AI Act is a risk-based approach
 which stipulates that AI generated content must be marked as such, unless the “AI systems perform an assistive function for standard editing.” In the US, Congress has proposed legislation along similar lines, such as the 
AI Disclosure Act of 2023
 and the 
AI Labeling Act of 2023
, while a number of states 
have introduced AI bills
 or have already adopted AI-disclosure legislation.
Transparency is routinely touted as a deeply held normative value that aims to guide the behavior and conduct of journalists. Again and again, current AI use guidelines raise the idea of transparency and highlight its importance when laying out rules and suggestions for incorporating AI in the newsroom. 
A recent analysis of 37 AI guidelines across 17 countries
 reveals key themes including transparency, accountability, and preservation of journalistic values. For example, 
Partnership on AI’s Responsible Practices for Synthetic Media
 explicitly identifies “transparency via disclosure” as an emergent best practice. The 
Paris Charter on AI and Journalism
 supports clear disclosure of AI use and suggests that significant use should be quantified, demonstrating the journalistic commitment to fairness and trustworthiness. The underlying beliefs reflected appear to be that being transparent about the use of AI is one component of a system of accountability with the potential to increase credibility, remedy lost trust, and enhance fairness. The guidelines appear to take a relational rather than dictating approach and this flexibility seems to favor rational and situational decision-making over hard rules, with each news organization having different goals and reader needs.
But despite all of the legal headway and normative consensus, it’s somewhat less clear what effective AI transparency looks like in practice. AI transparency can be both
 direct
 and 
indirect.
 While indirect disclosure, such as 
watermarking
 can offer a reader a chance to learn about the AI nature of content, it can take a bit of effort. In contrast, audience-facing direct disclosure through 
labeling 
offers the reader information as part of their consumption experience by default. Direct disclosure, in the form of labeling, is becoming increasingly common on social media platforms (TikTok, Meta and LinkedIn have all launched AI labeling strategies). Perhaps this could be a viable strategy for news media too, especially since the public might become more accustomed to labels after interacting with them on social media platforms. Outlets are constantly updating their AI policies, and some include content labeling as part of this approach.
AI labeling can be as simple as providing a visual cue that signals to the audience that AI has been used to create a piece of content. It offers contextual information about a piece of content that is not immediately apparent from the content itself. Direct disclosure through labeling is presumed to empower readers to critically evaluate information and make informed decisions, mitigating concerns around misinformation and misleading content. If journalists aim to increase transparency and maintain trust with their audiences, such labels may offer a solution.
Labeling as a Transparency Strategy
Labeling has been used in various other media contexts besides AI, particularly as an attempt to mitigate misinformation. Fact-checking labels are not quite the same as AI labels but the underlying purpose is similar insofar as they are both about informing readers about the epistemic basis of a piece of content (i.e. how what is known is known). Looking at the efficacy of fact-checking labels offers some hints about the effectiveness of labeling strategies in general.
Researchers have found that in some contexts, 
fact-checking labels do reduce sharing intentions of fake news
. Studies have also found a 
link between the perceived effectiveness of labels and trust in news media
– perhaps because people who already trust news media are more likely to have faith in the validity of the labels. Trust in institutions is one of the strongest predictors for support of such misinformation interventions– if audiences trust the outlet, they might trust the label more.
However, 
a recent meta-analysis
 found that fact-checking messages has limited effects on changing public attitudes toward political issues. While not beneficial for influencing credibility perceptions of news posts, 
fact-checking labels seem to improve judgments
 of a (social media) site’s overall quality. Maybe because the presence of a label signals that the site has made an effort to verify its content. Regardless, the presence of a fact-checking message could still act as an accountability tool when used as a journalistic practice.
Aside from the binary outcome of labels working or not working, there is another possibility: labels could backfire. Labels could deviate from their intended purpose and have unwanted effects by 
exacerbating selective exposure
 when used for stance and credibility purposes. Also, the practice of labeling only a subset of content on platforms may result in an 
“implied truth effect”
 where content that is not labeled is more likely to be perceived as accurate, even if false. Reduced perceptions of false positives can mitigate negative reactions toward the labeling intervention, but this requires familiarity with labeling and trust in the source. Increased exposure to labels, the visibility of labels, and audience familiarity with the intervention are thought to influence labels’ effectiveness. While a new warning system might work initially, given its novelty and attention-grabbing potential, 
readers might become accustomed to the labels
 and end up ignoring them over time.
In the case of applying these kinds of labels to AI content, this disclosure is meaningless if readers are unsure of how to interpret the labels. This approach could backfire if there is a mismatch between the well-intentioned goal of labeling for transparency and reader interpretations of what these labels actually signal. The potential backfire effects must be considered when deciding which journalistic content to AI-label: if only AI-generated content is labeled, it might signal that unlabeled content is of a different quality, even if both have had considerable editorial oversight. Indeed, AI-labeled content has already been found to 
reduce perceived accuracy
 and to 
lower trust
 in “AI reporters.” Explicit labeling may 
reflect negatively on the content creator
, reducing trust in journalists rather than helping readers to judge the credibility of the content itself.
Open Questions Moving Forward
Labeling shows promise and very well could be a viable approach which meets journalistic normative commitments. But outstanding questions remain around the specificity needed, design options, and other user expectations that would make it an effective approach for audiences.
Specificity
Deciding which “flavor” of transparency to provide is one of the main challenges. What level or type of explanation is expected? Do audiences want to know the full details of how exactly the content was generated? Or do they care more about the general context of AI being somehow involved?
Would a simple tag saying “AI” suffice as a meaningful transparency approach? Hoodline’s very missable “AI” badge that now appears next to fake author personas on the site has been criticized for being an empty gesture, so what more do audiences expect? 
A recent study tested nine variations of AI labels
, and found substantial differences when different AI labels are used. “AI-generated” connotes something quite different than “AI-manipulated,” and other variations such as “AI-assisted” or simply just “manipulated” could further obfuscate the boundary between misleading content (such as Pope Francis gallivanting in a fighter jet in Figure 2) and purely editorial uses of AI.
Figure 2. An AI-generated image of Pope Francis in a fighter jet, labeled as “Generated by AI” and published by the New York Times
Platforms and outlets are already dealing with the ambiguity of labeling. Just recently, on July 1st 2024, Meta changed their label from “Made with AI” to “AI Info” (see Figure 3) because the original label (launched in April 2024) did not align with audience expectations, nor did it provide enough context about the content. 
Meta cites
 minor modifications (e.g., retouching tools and filters) as being ill-suited for an AI label, so the new label will link audiences to more contextual information rather than a static superimposed label. Meta’s hope is that this modification will better match the company’s intent of informing audiences of manipulated media on their platforms. We can expect additional variations and modifications to labeling strategies as outlets discover more about their audiences’ expectations and how labels are received in the real world.
Figure 3. To the left, Meta’s AI label, launched April 2024. To the right, 
Meta’s revised AI label, launched July 2024
Another puzzle concerns whether there is a certain threshold of “AI involvement” that would require content to have a label. Should journalists be letting audiences know that they used AI for every (potentially small) task behind their published work? Should they have to describe which components AI helped with? Do they have to provide the prompts used? Would an audience want to know that AI gathered background info? What if it was only used for grammar/style? Determining how the degree and type of AI used relate to the label (and the audience’s understanding) will be important factors.
Design
The exact label design, its placement, size, etc. can all influence its effectiveness.
User experience insights will be valuable here, but the labels will probably be best suited to the byline of an article where audiences can have immediate insight into who authored the content they are consuming.
Designs will also need to vary by medium: a label for a written article will take a different form than for a video, or audio piece.
Audience expectations around AI use
A recent 
survey
 from the Reuters Institute found that a large majority of respondents want at least some disclosure when AI is used for news content creation, though there is less consensus on which use cases should be labeled. The study found that around one-third of respondents think back-end tasks such as AI-assisted grammar should be labeled, while around half think AI text creation and data analysis require disclosure. The general consensus on generative AI’s impact on society also varies by region. Countries such as Japan, Argentina and the US seem to have a more optimistic outlook, whereas countries such as France and the UK skew more pessimistic, underscoring that a one-size-fits-all approach will not be effective.
Understanding audience expectations around how journalists use AI, and communicating this use, will ultimately determine labels’ effectiveness. The most valuable insights will come from understanding audience expectations. What information do they need or want to know that will increase their agency in evaluating news sources? And how might those expectations shift in different contexts, such as a news website versus encountering some information on social media?
Aside from curbing misinformation, labeling can also impact the relationship between journalists and their audience. Audience perceptions are vital if the goal of labeling is to increase trust.
Public attitudes towards AI use in journalism vary
 (and will continue to fluctuate), reflecting a range of opinions and that one size will not fit all. While some embrace AI’s potential, others fear its implications for journalistic integrity, fearing a lack of human oversight and the perpetuation of misinformation. Balancing the benefits and drawbacks of generative AI remains an ongoing ethical, practical, and societal challenge. Labels are not temporally distinct interactions, but rather, are related to broader societal attitudes around AI use. These public opinions will change over time as audiences become more accustomed to AI, and any labeling strategies put in place will need to adapt.
What Can Be Done Now?
While the industry evolves and adapts to audience needs around transparency, there are some things journalists can do in the meantime including educating the public about AI, adapting as norms and understandings around AI shift, and seeking to understand and research what will work best.
Educate
Educate the public about how AI is used (and limited/checked/overseen) in the newsroom. Given the dynamism and ever-evolving technology, news media has the opportunity to demystify the role of generative AI as a tool, rather than provoking fear. Journalists’ framing decisions in their coverage of generative AI could feed into public attitudes, which in turn might influence how they use AI themselves.
Ensure that audiences are aware of both what the labels mean and why they are being provided. This will require aligning reader interpretations of labels with journalistic goals, and modifying the labels to align with audience expectations. Do not expect this process to be static, but rather adapt to your audience needs and find ways to effectively communicate any changes to labels.
Adapt
Without a specific and consistent labeling mechanism, it is writers’ responsibility to build audience trust in their work. Adaptability will prove to be the paramount skill in these developments, not only to changing guidelines but also to audience needs as the AI hype wanes and people potentially grow acclimated to these technologies. The public’s perceptions of AI will be dynamic and transparency strategies will need to be responsive.
Understand
In the absence of a dominating “correct” approach, studying reader expectations in more depth might be a fruitful next step in solidifying label implementation. Different outlets will also have different approaches. Perhaps directly asking audiences what information is most valuable to them can guide how outlets approach labeling.
Conclusion
Overall, labeling has been shown to reduce misinformation belief, increase trust in the journalistic process, and mitigate the harms of automated content production. It can be a valuable strategy to increase transparency. However, the exact design of the labels could impact their efficacy as accountability tools, such that framing AI’s role as the “generator” of the content could potentially serve to dodge accountability for the human journalist. Inadequate labeling can perpetuate harm, and excessive (or inaccurate) labeling can backfire; there really is no one-size-fits-all approach.
There are still outstanding questions and more research is needed — if your organization is interested in running studies with your audience, please get in touch with us, or find your nearest university collaborator. Emerging best practices have focused on reader autonomy and journalistic integrity at the forefront of these decisions, but there are still more questions to be answered around the practical implementation of labeling. What to label, label specificity, and label design are all still outstanding issues. Understanding exactly what needs to be communicated and disclosed to meet user needs will further the ability to design and implement effective transparency disclosures moving forward. In the longer run, some kind of norms or standards for such labeling could help the industry approach the issue from a unified perspective while creating more uniform expectations from audiences.
—
Jessica (Jessi) Zier is a Media, Technology and Society PhD student at Northwestern University. She has a background in cognitive neuroscience (BSc, Minerva University) and digital communication (MA, University of Salzburg/Vrije Universiteit Brussel), which has influenced her research interests on the psychology of persuasion, misinformation, and information seeking in the context of AI and other algorithmic systems.]]></content:encoded>
</item><item>
<title><![CDATA[The Impact of Generative AI on Journalistic Labor]]></title>
<link>https://generative-ai-newsroom.com/the-impact-of-generative-ai-on-journalistic-labor-e87a6c333245?source=collection_home_page----df04cefca135-----39-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Labor Market]]></category><category><![CDATA[News]]></category><category><![CDATA[Jobs]]></category><category><![CDATA[AI]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2024-07-23</pubDate>
<atom:updated>2024-07-23</atom:updated>
<content:encoded><![CDATA[The Impact of Generative AI on Journalistic Labor
Nick Diakopoulos
Follow
12 min read
·
Jul 23, 2024
--
1
Listen
Share
A recent study 
published in the journal Science
 argues that current state-of-the-art LLMs like ChatGPT could make a person at least 50% faster for an average of 14% of tasks in each of the hundreds of occupations studied, all while maintaining or even increasing quality. When considering additional software on top of the LLM that might adapt it for specific tasks, the average share of tasks per occupation that might similarly benefit jumps to 46%.
How do these numbers look for a couple common editorial jobs? Well, for 
News Analysts, Reporters, and Journalists
 the researchers 
estimate
 that only 16.7% of common tasks in the occupation can’t be helped by LLMs, but that 43.3% of tasks will benefit from current LLMs, and the remaining 40% of tasks will benefit from LLMs when additional software is provided to adapt to the task. And for 
Editors
, the estimates are 23.8% of tasks not benefiting, 52.4% benefiting from current LLMs, and 23.8% only benefiting when additional software is included.
The proportion of common tasks with estimated efficiency benefits from LLMs and LLMs with additional software for two common editorial occupations.
These estimates suggest AI has the potential to significantly change how these jobs are done. They can also be critiqued: I am skeptical, for instance, of whether the annotators really captured the right notion of “quality” for rating tasks in journalism. And our own 
survey research with the AP
 suggests that, as much as generative AI may offer efficiency gains, it can also create 
more
 work in the form of prompting and editing. I may do a deeper dive on this in a future post, but if we set these issues aside for now and just go with it, the estimates do paint a plausible picture: core editorial jobs in news production are 
exposed and susceptible
 
to being made drastically more efficient by LLMs
. Once software is built to adapt the LLMs to specific tasks and integrate it into systems and workflows, considerable efficiency gains could be realized for 83% of tasks for reporters and 76% for editors. It’s important to emphasize that these figures are fundamentally about 
augmentation
 rather than automation — we’ll still need people around but they’ll just be much more efficient. The implications for small scale and local media are tantalizing: imagine an entire community that could be effectively covered by 1–2 people.
To unlock the value of generative AI from an augmentative efficiency standpoint, news organizations will need people (1) who have the skills to use the current generation of LLMs and (2) who can develop the bespoke software to unlock their full potential, particularly if building new in-house tools. Other 
labor economists studying AI have also suggested
 that generative AI could result in “the decrease in difficulty of existing content-creating jobs” which could mean that other production skills become less important as the use of generative AI tools grows.
In the rest of this post, I explore what we can learn about these issues by analyzing data about news organizations’ hiring patterns over the last few years and how this relates to skills, both AI and non-AI. In particular I use job postings data in the US from 
Lightcast
, a company that comprehensively scrapes online job ads and standardizes them by occupation, industry, and skills. Prior 
research from the OECD
 has used this data to examine labor patterns related to AI, but not with specific attention to the news industry.
I focus the analysis on three segments of the US news industry as defined by NAICS codes (Newspaper Publishers, Television Broadcasting Stations, and Radio Broadcasting Stations), as well as four occupations defined by 
ONet
 codes that reflect editorial work within these industries (News Analysts, Reporters, and Journalists; Editors; Producers and Directors; and Writers and Authors), and two occupations that reflect technical work in these industries (Data Scientists; and Software Developers). AI-related skills are tagged based on the 
Lightcast Open Skills Taxonomy
 subcategory of “Artificial Intelligence and Machine Learning (AI/ML)” which consists of 
172 related skills and certifications
. I compare data for the 19 months after ChatGPT launched (12/22 to 6/24, 
post-ChatGPT
) with an equal number of months before then (5/21 to 11/22, 
pre-ChatGPT
) to see how the rapid proliferation of (and hype around) generative AI after the ChatGPT launch may have shifted the picture.
Job Posting Trends in the Face of (Generative) AI
After a post-COVID hiring bounce in late 2021 to mid 2022, the number of open postings for the editorial occupations studied settled back down to around 950 per month, which is roughly where it was pre-COVID (see next figure). Something to keep in mind for the rest of the analysis here is that in the pre-ChatGPT period, there was a lot more hiring going on in general (28,566 postings) in comparison to the post-ChatGPT period (18,156 postings).
Number of unique postings for 4 editorial occupations in 3 media industry segments.
The number of postings calling for at least one AI / ML related skill is a tiny fraction of the overall hiring in these industries and for these positions, reflecting just 0.3% of jobs over the entire period (see next figure). The post-ChatGPT period does show a pronounced uptick in demand for positions with AI / ML skills though, more than doubling from 46 pre-ChatGPT (0.16% of all positions) to 108 post-ChatGPT (0.59% of all positions). Whatever the cause, whether part of the AI hype that’s been more pronounced since ChatGPT launched, or a well-warranted realization of the need for AI skills in the organization, this is considerable growth proportionally-speaking even if it’s still only a tiny fraction of the overall labor demand.
Number of unique postings for 4 editorial occupations in 3 media industry segments calling for at least one AI / ML skill.
The number of positions mentioning “freelance” for the editorial occupations studied also increased as a proportion, from 3.7% of positions pre-ChatGPT (1056 postings) to 4.7% after ChatGPT launched (861 postings). In contrast to 
research finding that
 freelance job postings for automation-prone jobs have taken a dive since the launch of ChatGPT, the data here show that freelance positions increased proportionally by a small amount. Rather than eroding it, demand for freelancers in the news industry seems robust.
What about when we look at technical roles like Data Scientists and Software Developers? These are more likely to be the folks building that additional software needed to unlock the power of generative AI for domain-specific tasks. Here we see absolute demand for people with AI skills slightly down, with 861 postings pre-ChatGPT and 832 post-ChatGPT, a 3.4% drop. Though, it’s important to point out that there was a 30.0% drop in demand for data scientists and software developers overall (from 4,509 postings pre-ChatGPT to 3,161 post-ChatGPT). In other words there is robust and 
proportionally
 
growing
 
demand
 for data scientists and software developers with AI skills in the news industry even if overall absolute demand for these positions is slightly down.
So, from the pre-ChatGPT period to the post-ChatGPT period we see overall growth in the proportion of postings calling for AI skills both amongst editorial positions (3.7x proportional growth) as well as for technical positions (1.4x proportional growth). The total number of technical positions calling for these skills dwarfs the editorial side by about 11x. Surprisingly, demand for freelancers in the news industries appears stable. 
The bottom line is that news organizations are investing in growing the number of people with the critical skills needed to both use existing LLMs and build the software to better leverage them in the future.
Jobs & AI-Skills
Organizations in the news industry are indeed hiring people with AI skills. But what does that look like in terms of specific roles and jobs? To look at this I read through a sample of the 154 editorial job descriptions from the period that matched on at least one AI-skill. Interest in these positions is highly concentrated in relatively few firms, just 35. Most of the companies looking to hire are bigger ones such as NBC, Bloomberg, McClatchy, as well as Gannett, Boston Globe, and the New York Times.
I roughly saw about four different clusters of jobs being posted, which range from the more technical to the less technical and which employ AI skills for different purposes: AI-doers, AI-users, AI-strategizers, and AI-reporters. The AI-doer positions involved tasks like testing and recommending tools, building out workflows for automation, or designing AI-driven experiences. These positions typically called for the highest level of technical AI skills in implementing and designing AI solutions. Next were the AI-user positions, which called for more general “proficiency in utilizing AI tools” or “using AI digital image manipulation software” as well as “enthusiasm for learning” about AI technologies. AI-strategizer positions were about horizon scanning and staying updated on industry “trends like AI” or having a “strong understanding of AI and its potential uses” for the sake of developing strategy. Finally, the AI-reporter positions reflect the editorial mission of news organizations for 
reporting on AI in society
. These positions reflected a need to know about AI and the AI industry, how it was impacting particular beats like education, and knowledge of how to direct coverage of AI, but did not emphasize any particular technical understandings or use of AI.
One of the roles that’s received some hype in the media as being a “
job title of the future
” as generative AI grows is 
prompt engineer
. Demand for people with the “prompt engineering” skill began to take off in March 2023 and it has rapidly grown, with 3,793 positions total since May 2021 across all industries that Lightcast indexes. In June 2024 alone there were 469 unique positions calling for the skill. Much of the interest is concentrated amongst “Consulting Services” industries, with only a small fraction of postings calling for this skill (12 total across any occupation) coming from within the News Publishers, Television, and Radio industries I focus on here.
Overall, we see a range of editorial positions calling for AI skills being advertised including those that are more technically intense to those that are about strategy or framed as being able to report on the topic more broadly. There is little demand in the industry for hiring people with prompt engineering skills.
Skill Shifts
Changes in the skills required for a job can be indicative of how new technological opportunities or demands are shaping the nature of work. To look at this I compared the prevalence of skills for all pre-ChatGPT and post-ChatGPT editorial positions and report statistically significant differences.
In addition to the increase in AI / ML skills noted above, other technical skill categories are also showing signs of increased demand amongst editorial positions, including Computer Science moving from 3% in the pre-ChatGPT period up to 5.4% of postings in the post-ChatGPT period, and Data Analysis moving from 6.4% up to 7% of postings. But some other core skill categories are also up, including Journalism (73.4% to 76.2%), Critical Thinking and Problem Solving (16.5% to 18%), and Initiative and Leadership (53.2% to 55.2%). The technical skill categories are called for less often overall, but their proportional increase is also greater.
In terms of specific skills demanded we see that Ethical Standards and Conduct has grown from being present in 12.7% of editorial postings pre-ChatGPT to 16.8% of postings in post-ChatGPT. Other skills for which the demand has grown include Writing (45.5% to 48.9%), Storytelling (12.7% to 14.5%), Creativity (4.2% to 5.2%), Fact Checking (1.2% to 1.6%), and Proofreading (2.1% to 2.5%). Although correlation isn’t causation it’s easy to see several of these skills as complementary to the weaknesses of generative AI, as would be expected to drive increased need for proofreading, fact checking, creativity, and especially ethical standards and conduct.
There are also some notable drops in skill category demands, including from technical and design areas such as Graphic and Visual Design Software (11.4% to 10%), Web Design and Development (5.2% to 3.9%) as well as more general skills related to Communication (55.5% to 53.5%) and Media Production (47.4% to 41%). More specific skills that have seen a drop include Punctuation and Capitalization (6.5% to 4.6%), Information Gathering (7.8% to 6.6%), Content Editing (4.7% to 3.7%), Research (24.7% to 23.8%), Memos (2% to 1.3%), Grammar (14.8% to 14%), Copywriting (4.8% to 4.2%), and Image Editing (1.2% to 0.9%). And not that it was prevalent in the pre-ChatGPT period, but Subtitles as a skill has gone away completely in the post-ChatGPT period. Again, we can see that the decrease in demand for some of these skills could reasonably be interpreted as related to the technical changes being driven by generative AI. While statistically significant, many of these shifts are minor, as we would expect them to be over the course of only 3 years.
Surely there are many other forces operating on the journalistic labor environment that may be shifting the nature of skills demanded in job postings. For example, demand for skills in TikTok have almost doubled between the two periods reflecting the rapid growth of that platform, and demand for bilingualism (Spanish/English) grew by about a third as some news outlets respond to demographic changes in communities. A growth of interest in fact checking could also reflect an increased concern and responsibility to counteract mis- and disinformation which has proliferated on platforms. And dwindling interest in grammar skills could reflect larger technological shifts around writing aids (e.g. grammarly or other text suggestion mechanisms integrated into common software), or reduced interest in grammatical precision due to lowered expectations from online audiences. In other words, I don’t think we can necessarily pin all these shifts to generative AI.
Overall we see that there are some skill shifts that could plausibly be related to changes in the growth in interest in AI and generative AI since ChatGPT launched.
 I make this statement cautiously as I think there are many forces beyond AI that are shaping skills reflected in job postings. But whatever the cause there are some trends clearly visible in the data here. Increased demand for complementary skills in ethics, critical thinking, creativity, fact checking, and proofreading would seem to address some of the concerns raised by generative AI’s weaknesses 
described in previous research
. Similarly, drops in interest for skills like content editing, punctuation, grammar, memos, copywriting, and image editing, show that the demand for those skills where generative AI has clear capabilities appear to be eroding. These shifts of course also only consider skills in new job postings and do not account for any of the upskilling around AI that many news organizations have been undertaking in 2023 and 2024 through various rounds of training.
In Closing
Based on the data analyzed, the news industry in the US seems to be slowly shifting its labor pool in ways that may help it realize the potential efficiency gains from generative AI. We see that news organizations are seeking to grow the number of people with the skills needed to use existing LLMs on the editorial side and to build the software to layer on top to enhance efficiency on the technical side. So far it seems like the freelancing roles in the industry have been spared. And heavy demand for prompt engineering hasn’t really hit the industry. But skill demands beyond AI skills also appear to be shifting in editorial roles. These shifts appear to be complementary to the weaknesses of generative AI towards skills such as ethical standards and critical thinking. To stay competitive in the shifting industry journalists may want to enhance their AI skills, as well as some of these other complementary skills. Likewise, news organizations would be wise to accelerate hiring and invest in upskilling their existing workforce (as some already are) if they want to leverage potential gains from generative AI.
Now, whether the people with AI skills at news organizations will be managed towards efficiency goals is another question, which depends on the organizational strategy employed. There are, of course, alternatives to the efficiency strategy, such as more product-oriented or differentiation approaches, as have been 
explored previously on this blog
. Labor unions in the industry are 
starting to get traction
 in protecting jobs that might be impacted by AI which could further shape who benefits from the potential efficiency gains. As much as augmentation can be spun in a positive way for small organizations that need to do more with less, large organizations with multiple people in a role may find themselves in a situation where increased output from efficiency gains may make less sense than maintaining a constant level of output with lower labor cost. The dynamics between labor, content quality, and organizational strategy and management are complex and beyond this post, but I hope to get more into that in the future.]]></content:encoded>
</item><item>
<title><![CDATA[Bringing Open-Source Models to Spreadsheets 🚀]]></title>
<link>https://generative-ai-newsroom.com/bringing-open-source-models-to-spreadsheets-c440fc4818b4?source=collection_home_page----df04cefca135-----40-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Spreadsheets]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Tools]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Florent Daudens]]></dc:creator>
<pubDate>2024-07-18</pubDate>
<atom:updated>2024-07-18</atom:updated>
<content:encoded><![CDATA[Bringing Open-Source Models to Spreadsheets 🚀
Florent Daudens
Follow
8 min read
·
Jul 18, 2024
--
Listen
Share
AI can help journalists do great things if only we can imagine how to use it. That’s why I created a simple but potentially useful tool: 
Hugging Face on Sheets
, to bring open-source AI models into spreadsheets. The idea is to meet people where they are and help them use AI as a feature in a tool they are already comfortable with.
In the arsenal of journalists and other specialists in media organizations, spreadsheets are an essential tool — not only to manipulate numbers but also text. And when you think about it, it’s a match made in heaven with AI since they are both tools designed to manipulate data.
This project germinated one day when I was sharing my wonder about AI with an economist friend at the playground (not necessarily the best place to brainstorm, I concede!). His thoughtful answer brought me back to Earth: “Yeah, AI is interesting. But as a PhD working in one of the most renowned statistical institutes, I can tell you we still do 95% of our work in spreadsheets.”
Yet, AI fundamentally changes one thing: we can talk to the machine in our own language. Or, as Andrej Karpathy puts it, “
The hottest new programming language is English
”. It can be challenging to talk to the spreadsheet, master the formulas, etc., something I’ve often observed when teaching data journalism to journalists and students.
Moreover, chatbots are consumer products, not necessarily the best tool for a specialized profession such as journalism. With spreadsheets, you can manipulate a lot of data, automate tasks, find needles in a haystack, and process datasets in a highly flexible and tailored way.
There are many advantages to using AI models in spreadsheets:
No code required
It’s a commonly used tool in journalistic workflows
You can manipulate various types of data: text, URLs, dates, numbers, images…
You can chain prompts to define complex processes
You can create reproducible and automated workflows
Free usage within the rate limits of the API
I’m probably forgetting some other arguments; feel free to add yours in the comments ;)
To help unlock these benefits I 
created a script
 you can run in your Google Sheets as an add-on (I’m not a good coder, so I got some help from AI to help build it).
This tool opens up a lot of possibilities to interact with your data, and enables analytic tasks such as: Classification, extraction, data cleaning, summarization, named entity recognition, translation, regular expression formulas, removing personal info, and you name it! You can consider chaining prompts in various columns or adding a 
few-shot prompt
 to improve your results.
This script allows you to call the model of your choice among those that run on the free API for inference on Hugging Face, such as Meta Llama 3 8B and 70B, Mixtral 8x7B, etc. (
see the complete list here
). For more advanced uses, you can also set up your own inference endpoint and access a wider variety of models.
Three Demonstrative Use Cases
Next, let’s look at three specific use cases to see how Hugging Face on Sheets works in action. You can find the sheet for each of the examples 
here
.
1. Classification
Let’s combine business with pleasure and see if season 3 of The Bear is worth watching, based on critics aggregated by Rotten Tomatoes.
I imported them into a nicely structured table with author, publication, and critic columns (thanks to an importxml formula that is out of scope for this post). Then, I created a new column in my sheet to classify them with the following cell formula:
=HF(C2, “meta-llama/Meta-Llama-3–70B-Instruct”, “Think step by step. 1. Classify this movie critic only by using these categories: ‘positive’, ‘neutral’, ‘bad’. 2. Remove special characters, tags, and capital letters in your answer. 3. Clean the final result to answer with one word only.”)
Here’s the breakdown of the formula:
HF
 is the name of the formula
C2
 is the cell where the text of the critic is located
Then, I call the model
And finally, my prompt.
You can see the result with these two examples:
The Bear
 Season 3 feels like an overlong exercise in experimentation that doesn’t fully pay off. Perhaps it would have benefitted from one of the non-negotiables Carmy preaches this season: Subtract. 
→ bad
The Bear
 still finds moments of transcendence in its characters’ pursuit of professional excellence and personal growth, yet the show remains more fallible than its rapturous acclaim may imply. 
→ neutral
You might wonder why not write a simple prompt like “classify this movie critic by using three categories.” It’s a question of trial and error, but after three iterations of the prompt, I got a clean result.
Pro tip: Ask an LLM to generate the prompt for you. It can sometimes be faster and better structured, and at the very least gives you a starting point for refining it further.
2. Extraction
To test how the tool performs with extraction tasks, I took a sample of the CNN Dailymail Dataset since it’s well suited for journalistic tasks. This English-language 
dataset
 contains just over 300k unique news articles written by journalists at CNN and the 
Daily Mail
.
As a first try, I wrote the following prompt:
=HF(A2, “meta-llama/Meta-Llama-3–70B-Instruct”, “List every person mentioned in the text, separated by commas. Answer only with the list of names. Exclude special characters and tags.”)
It worked okay, but several names were missing. Interestingly enough, I got better results by changing the wording to the following:
=HF(A2, “meta-llama/Meta-Llama-3–70B-Instruct”, “Extract the name of every person mentioned in the text, separated by commas. Answer only with the list of names. Exclude special characters and tags.”)
I also created another column “Daniel Radcliffe/Not Daniel Radcliffe” to ask the model if this name appears in the previously created column. The prompt was:
=HF(B2, “meta-llama/Meta-Llama-3–70B-Instruct”, “Is ‘Daniel Radcliffe’ mentioned in the text? Think step by step. 1. Answer only with ‘yes’ or ‘no’ in lowercase. 2. Remove everything else in your answer (tags, special characters). Text: ”)
You’ll note that I end with “Text: “ to make it explicit to the model that the text begins after this prompt. If we look under the hood, the line of code to call the prompt is the following:
const formattedPrompt = `<s> [INST] ${systemPrompt} ${prompt} [/INST] </s>`;
Which will translate to:
<s> [INST] Is ‘Daniel Radcliffe’ mentioned in the text? Think step by step. 1. Answer only with ‘yes’ or ‘no’ in lowercase. 2. Remove everything else in your answer (tags, special characters). Text: Daniel Radcliffe, Rudyard Kipling, Peter Shaffer, Harry Potter, Jack Kipling [/INST] </s>
for the input sent to the model for the first cell.
As with any generative AI tool, you’ll have to iterate with the prompts to get them just right.
3. Messy Data
In his 
article
 “What is dirty data and how do I clean it? A great big guide for data journalists,” Paul Bradshaw outlines a range of data issues often encountered in journalism including inaccuracy, incompleteness, inconsistency, and incompatibility. Could the Hugging Face on Sheets add-on help with some of these issues?
I gave it a try, and it performed well on cleaning strange HTML. It was also surprisingly good at removing a special character in a string, something LLMs sometimes struggle with (try asking your favorite model how many ‘r’ are in ‘strawberry’). I also added an extra step to normalize the capitalization of the letters.
=HF(C9, “meta-llama/Meta-Llama-3–70B-Instruct”, “You are a helpful assistant. I want you to remove all the characters ‘å’ in the following text. Think step by step: 1. Locate the characters ‘å’. 2. Replace them with a space. 3. Capitalize the words. 4. Give the final answer only. Don’t add any comments and remove special characters.”)
Obviously, the range of data issues can vary a lot, but I think that this approach could be really powerful for data journalists looking to clean up and munge their data before doing more standard analyses.
The Open-Source Approach
This code is based on open-source models and is open-source itself, which means that everybody is free to use it, but also to contribute to it!
Since it launched, I’ve seen the power of the community with two interesting examples. Louis Brulé Naudet improved it by adding the possibility of selecting a model and a prompt located directly in cells instead of in the script, giving more flexibility to this tool. Nick Diakopoulos, the editor of this blog, spotted an error in the chain of prompts and [/INST] tag, significantly improving the results.
I recently saw another interesting example developed by Jonathan Soma, a journalism professor at Columbia. His 
example
 integrates specific task-oriented models on the Hugging Faces Hub instead of foundational ones for tasks such as zero-shot classification, token classification, summarization, or text-to-image.
How to Get Started with Hugging Faces for Sheets
To get started, you need to:
Register or log in to 
Hugging Face
Get your API key in your Hugging Face profile settings (it’s free)
Select fine-grained token > generate a token
Copy your token
Set permissions > select “Make calls to the serverless Inference API”
You can also choose “Make calls to Inference Endpoints” if you plan to use dedicated endpoints.
Then, in your Google Sheets:
Click on Extensions > Apps Scripts
Replace the few existing lines of code with 
this script
 and save it.
Back in the sheet, you should see a new tab called “Hugging Sheets”.
Click on it and add your API key.
Voilà!
What’s Next?
I can think of many possible improvements and invite you to contribute to this project!
Adding task-oriented models such as summarization, image generation, translation, etc (
see the full list of tasks here
)
Sharing exciting use cases and efficient prompts 
in the community tab of the project
 to help everyone use it. (Please share examples with me if you have ideas!)
Publishing it as an official add-on in the Google Marketplace to make it easier to install. I’ve looked at the process, and it’s unfortunately quite cumbersome!
Most importantly, let’s imagine other tools together! I’m convinced that the news industry needs to take ownership of AI, develop products tailored to its needs, and foster the rise of AI builders instead of AI users.
So, feel free to 
reach out on LinkedIn
 and join the 
Journalists on Hugging Face community
!
—
Florent Daudens is the Press Lead at Hugging Face, the open-source AI platform. He also teaches Digital Journalism at the Université de Montréal and provides newsroom training. He previously worked for more than 15 years in the media as a journalist and managing editor for Canadian newsrooms such as Le Devoir and CBC/Radio-Canada.]]></content:encoded>
</item><item>
<title><![CDATA[Decoding US Copyright Law and Fair Use for Generative AI Legal Cases]]></title>
<link>https://generative-ai-newsroom.com/decoding-us-copyright-law-and-fair-use-for-generative-ai-legal-cases-507fdfd9956c?source=collection_home_page----df04cefca135-----41-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Copyright Infringement]]></category><category><![CDATA[Generative Ai]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Julia Barnett]]></dc:creator>
<pubDate>2024-07-11</pubDate>
<atom:updated>2024-07-11</atom:updated>
<content:encoded><![CDATA[Decoding US Copyright Law and Fair Use for Generative AI Legal Cases
Julia Barnett
Follow
9 min read
·
Jul 11, 2024
--
Listen
Share
Image generated using Midjourney.
Over the past couple of years there have been a 
spate of lawsuits
 concerning various big players in generative AI, with more than 20 cases filed against OpenAI, Meta, StabilityAI and others. At the core of almost every one of these cases is the question of copyright infringement: is it okay that these companies are using copyrighted content from creators and publishers in various ways without their permission?
In this post, the first of a series, I’ll dive into some important background and translate the legalese of these lawsuits into a more digestible narrative, outlining the technical background, copyright issues, and fair use arguments. (As deeper background readers may also be interested in this 
primer on copyright and generative AI for news media
).
The basic technical-legal issues
Generative AI tools in all domains function the same at a fundamental conceptual level. Whether it’s a large language model (LLM) tool like 
ChatGPT
, an image generator like 
Stable Diffusion
, or a music generator like 
Suno
 and 
Udio
, the same underlying approach powers them all, even if there are technical nuances in exactly how they function. These models take in a large amount of training data (think: as much of the internet as possible for ChatGPT, or millions of songs for a music generator), and then they take thousands of hours to train on this data to teach the models how language is used, what images should look like, or how music should sound. The outputs of these models are ostensibly new text, images, or songs, depending on what the model was trained to do.
Two main issues arise during this process that are prompting all these lawsuits:
Generative AI model creators often never had the permission to train on the data they used in the first place;
Sometimes, model outputs are not entirely novel, but actually exact copies of or extremely similar to items present in the training data. This can be entire works (e.g., a whole song) or partial elements (e.g., the chorus of a song).
Thus, these lawsuits typically break down into two main areas, both with the central question of copyright infringement: did copyright infringement occur in the (1) training phase of the model, and (2) outputs of the model.
How to establish copyright infringement?
As in all court cases in the United States, the 
burden of proof
 lies on the plaintiff. There are three core elements that need to be established to constitute an 
infringement of copyright
:
The owner of the (allegedly) infringed work needs to possess a valid copyright;
The defendant (e.g., OpenAI) needs to have actually used the plaintiff’s (e.g., the New York Times) work to make the defendant’s work;
The defendant either made an 
exact copy
 
of the work, or something 
substantially similar
 to this work.
As an illustration, let’s take the example of the 
New York Times vs. Microsoft and OpenAI
 concerning Microsoft’s Copilot and OpenAI’s ChatGPT.
Item 1
: The New York Times (NYT) claims to own a 
valid copyright
 for the 
work they publish
 (it is important to recognize that not everything the NYT publishes is copyrightable, such as 
facts
 in the stories they cover or 
any content generated by an LLM
).
Item 2
: The NYT alleges that OpenAI and Microsoft unlawfully used millions of NYT articles to train generative AI tools, specifically OpenAI’s 
ChatGPT
 and Microsoft’s 
Copilot
 (previously “Bing Chat”).
Item 3
: [
Training Infringement] 
The NYT cites the most highly weighted datasets used in the training of both ChatGPT and Microsoft Copilot (called “the 
Common Crawl
”) as containing at least 16 million unique records from the NYT, and points out that OpenAI even notes in their 
paper introducing GPT-3
 that the “datasets we view as higher-quality are sampled more frequently” during the training phase. These 16 million unique records are exact copies of the text of NYT articles.
Item 3
: [
Output Infringement] 
The NYT cites specific examples of using “minimal prompting” to ChatGPT and Copilot that result in paragraphs that are almost verbatim reproductions of NYT articles (with only a handful of words slightly altered), which may be construed as an exact copy or substantially similar.
This is an example of a strong 
prima facie
 (Latin legalese for having adequate evidence to proceed into trial) case due to the cut and dry nature of all three elements. In both training and output, there is evidence of both OpenAI and Microsoft having products that produce exact copies of the NYT’s work. In text (i.e., large language models), finding memorization of training data can be a relatively straightforward task (see 
Carlini et al.
 and 
McCoy et al.
 for research, or prior posts on this blog about 
finding memorized news content
 and 
analyzing why some articles may be memorized more often
) because you can find verbatim 
exact copies
 
of the training data present in outputs. In images and music, the memorization of training data is not as exact due to the exponentially larger “vocabulary” of pixels building images and musical elements (melody, rhythm, timbre, notes, lyrics, etc.) building music. In these cases, memorization is constituted as
 substantial similarity
 
(see 
Somepalli et al.
 for images and 
Barnett et al.
 for music), which is not as clear cut of a legal case for copyright infringement due to the fuzziness around what constitutes “substantial” in terms of similarity.
Though training and outputs are inherently intertwined — the model would not have been able to reproduce articles exactly if they were not originally present in the training data — the infringement during output and training are likely to be evaluated separately. The system (e.g., ChatGPT as opposed to the model GPT-3.5 or GPT-4) potentially could produce direct copies without them being present in the training data since it has access to search engines and could output copyrighted material in this way. This further supports the already established legal precedent that each act by the defendant (infringement during training and infringement during output) is a separate infringement that likely will be established separately.
Under the US copyright legal statute (17 U.S.C. § 106), the copyright holder has the 
exclusive right to various acts
 involving their copyrighted work (including but not limited to reproduction, derivative works, and distribution). These infringements tend to be evaluated separately such as in the case of 
Cartoon Network, LP v. CSC Holdings, Inc.
 (in which the defendant was found to be not guilty in all three cases). In this case, a group of copyright holders sued a cloud DVR service for copyright infringement in three ways: two cases of copies made during brief buffers and one case of playback over the internet — all three instances were evaluated separately. There are many cases like this one that lawyers will cite in generative AI copyright cases to support the separate evaluation of infringement in training and in output.
Fair use: What happens after a copyright infringement claim has been established?
After the plaintiff (e.g., the NYT) has successfully established their claim(s) of copyright infringement, the baton passes off to the defendant (e.g., OpenAI) to make a case for why they should still be allowed to use the copyrighted material. In these cases the defendants will claim that the material used was non-infringing due to 
fair use
.
Fair use
 is legal doctrine in the US that provides an exception to copyright infringement for various purposes such as criticism (e.g., a movie review), news reporting, or education. There are currently 14 sections in US copyright law pertaining to fair use exceptions (Title 17 Chapter 1 §108–122), but none of these apply to the current questions surrounding copyright infringement and generative AI. There is a rich history of technological innovators getting sued for copyright infringement and claiming fair use, (more on this in a future post) that set the precedent and tone for how these generative AI cases may proceed, such as the 
Google Books Case
 (Authors Guild, Inc. v. Google Inc.) and the 
Aereo streaming case
 (American Broadcasting Cos., Inc. v. Aereo, Inc.).
The argument made by these generative AI companies like OpenAI, StabilityAI, and Microsoft is (and will likely continue to be) that they should be allowed to use these copyrighted works without permission due to a 
furthering of research
, and due to the fact that they are transforming the outputs (for further details of an existing argument, see 
OpenAI’s comments about fair use
 on page 11). None of the cases have thus far had any significant rulings regarding whether the use of copyrighted works in generative AI can be considered fair use, and this is the key thing to watch going forward.
There are 
four important factors
 in the copyright statute enacted by Congress to consider when deciding whether something can be considered fair use:
The purpose and character of the work — was it for commercial use? For non-profit educational purposes?
The nature of the copyrighted work — was it an unpublished work? A creative piece? Something commercially available?
The amount or substantiality of the portion used in relation to the copyrighted work as a whole — was it one sentence of a published book? A whole chapter? A figure in a fraction of a painting?
The effect of the use upon the potential market for or value of the copyrighted work — could they have purchased or licensed this work? Did they make a lot of money off of this?
It is essential to note these are 
guiding factors
 
in determining fair use. There is no cut and dry answer for this, which is why these factors will be argued out in the courts. There are 
billions of dollars
 at stake for both the generative AI companies as well as some of the publishers — these battles will not quietly disappear.
There was a recent case decided by the Supreme Court in March 2023 concerning the following Andy Warhol works of Prince that were created using the accompanying black and white portrait of Prince taken by Lynn Goldsmith:
Photos obtained from the 
court documents
 of Andy Warhol Foundation for the Visual Arts, Inc. v. Goldsmith.
The Andy Warhol Foundation (AWF) had a licensing agreement to use the photograph for a “one-time use” for $400, but then made an additional 15 works that all amounted to a significantly large sum of money for the AWF — $10,000 for use in a Vanity fair magazine cover. When Goldsmith sued for copyright infringement, the AWF claimed that using this photograph was fair use due to the transformative nature of the work.
Evaluating in line with the four fair use factors listed above, and in accordance with Goldsmith and the Supreme Court majority (the AWF and dissenting justices had a different, but also plausible view):
There was undoubtedly a commercial stake here: the AWF made $10,000+, while Goldsmith only made $400.
Goldsmith’s photo was a published work — a creative project that was commercially available.
Warhol’s works used the central aspect of the photograph: Prince’s face altered creatively.
Goldsmith’s photo was able to be licensed — the AWF had even licensed it once — they just did not for the subsequent usage.
If the Supreme Court had ruled in favor of the AWF, that would have set precedent for many of the transformative works from (at least visual) generative AI to be well within the grounds of fair use. However, they ruled in favor of Goldsmith. This does not necessarily mean that the courts will side in favor of the plaintiffs opposing the generative AI companies. However, the Court’s decision does show some potential lines of argument for the courts favoring copyright owners (i.e. the original creators of the works used for training).
It’s likely that the generative AI cases that have already been filed will take several years to fully shake out, especially as defendants and plaintiffs alike appeal the rulings to the appellate and even Supreme Court level. There will likely be conflicting rulings and opinions in various cases across the country. We may not know the final implications until the Supreme Court weighs in, which will likely be quite a while from today — if the Court chooses to involve itself at all. By then, who knows how far the technology will have come and how many more copyrighted works will have been used in the training process. In the meantime, there’s plenty to dig into, from the details of the current cases and their potential outcomes to the various historical precedents around copyright infringement with respect to technological innovations. I’ll explore these issues (and more!) in future posts in this series.
Disclaimer
: The information in this post does not, and is not intended to, constitute legal advice and is for general informational purposes only.
Acknowledgements:
 The author would like to thank 
Dr. Peter DiCola
 for his invaluable discussion and feedback on this work, as well as 
Dr. Nick Diakopoulos
 for his helpful feedback.]]></content:encoded>
</item><item>
<title><![CDATA[Making Sense of Science: LLMs for Helping Reporters Understand Complex Research]]></title>
<link>https://generative-ai-newsroom.com/making-sense-of-science-llms-for-helping-reporters-understand-complex-research-f1292720a09c?source=collection_home_page----df04cefca135-----42-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Science Communication]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Large Language Models]]></category><category><![CDATA[Sensemaking]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Sachita Nishal]]></dc:creator>
<pubDate>2024-07-08</pubDate>
<atom:updated>2024-07-08</atom:updated>
<content:encoded><![CDATA[Making Sense of Science: LLMs for Helping Reporters Understand Complex Research
Sachita Nishal
Follow
7 min read
·
Jul 8, 2024
--
Listen
Share
This blogpost was written in collaboration with 
Eric Lee
.
During the news gathering process, reporters can often struggle with understanding complex, jargon-heavy documents, particularly in fields like science and technology. For instance, a tech reporter trying to write a story about a new study about AI may encounter difficulty making sense of specialized terms and concepts from the field. This can create friction during the process of reading the documents to determine what may be a newsworthy angle to cover about the study.
To address this challenge, we propose using large language models (LLMs) to identify and define jargon terms within scientific abstracts. These models can be leveraged to wrangle and transform textual input into different formats (e.g., news headlines or summaries based on news article text), and may lend themselves well to the use-case of simplifying complex terms, especially if a definition is available but needs re-writing into more easily understood words. Such systems could also support the contextualization of news articles for readers, as 
demonstrated by BBC News Labs
 a few years ago.
This blog post walks through how we built and conducted a preliminary evaluation of a prototype to test out this idea, and describes what we learned in the process [1]. We hope this can help others engaged in similar projects, and extend to systems that support journalists’ sense-making of documents in other domains, such as complex legal or medical documents.
Using Retrieval-Augmented Generation
Retrieval-Augmented Generation (RAG)
 is an approach that can enable this kind of simplification of complex terms — it allows a user to input a “query” (i.e., a prompt, a question, or even simply a jargon term), and then matches it against text in a reference document that could be used to help derive a simplified definition. For instance, a jargon term like “precision metric” from a scientific abstract about a novel AI model will likely be found in different sentences within the text of the whole scientific article (
Note
: In this post we use 
article 
to refer to a scientific article rather than a news article). RAG relies on finding these matching sentences or text snippets, and supplying them to an LLM, along with a prompt instructing how the snippets need to be used (e.g., to create a summary, to generate a readable definition).
Two assumptions we made when designing this prototype with a RAG approach were that: (1) the retrieved snippets will actually be informative for creating a definition of the jargon term, and (2) even if the RAG output has minor errors, a human will actually verify the supplied definition if it elicits their interest.
By employing such a RAG approach with 
GPT-4
, we designed a prototype system to provide reporters with clear, concise, and accurate definitions of complex terms. We also designed the prototype to personalize the identification of jargon terms based on a reader’s knowledge level, making it easier for journalists with differing levels of scientific knowledge to parse these articles (e.g. a general interest reporter might need something different than a seasoned science reporter working her specific beat). This prototype was constructed and evaluated in the lab using a sample of 64 peer-reviewed articles published on 
arXiv Computer Science
 in March 2024.
Preview of the Prototype
The prototype is built as a web app with a list of scientific articles, displaying each article’s metadata and abstract. Jargon terms are highlighted within the abstracts, allowing users to hover over them for instant definitions. Additionally, users can click to access a comprehensive list of all jargon terms from the abstract, along with their definitions. A search bar allows users to find articles of interest, and filters allow users to sift through specific categories of the articles as well (we focus on topics in AI, Human Computer Interaction, Computing and Society for now).
A screenshot of the prototype system to simplify complex jargon for science reporters. The user can hover over a word detected as jargon (“Anchors”) to receive a tailored definition.
Building this prototype entailed work on two main problems:
Identifying the jargon, based on a readers’ expertise
Defining the jargon, based on the text of the scientific article
Identifying the Jargon
We tackled the challenge of identifying jargon terms in scientific articles using GPT-4, with a 
prompt template
 that allowed users to specify their level of scientific expertise. We used this prompt to generate a tailored list of jargon terms for that user.
This was evaluated with two annotators who manually identified jargon terms in the articles in our sample dataset, and who also provided 
their own levels of expertise
 in the prompt templates by describing their knowledge in natural language. The overlap between the resulting sets of jargon terms — for each individual article, for each individual annotator — was used to understand how well GPT-4 performs at this task. Worth noting here is that both annotators had differing levels of scientific expertise, and this was visible with how one annotator consistently identified more jargon terms per abstract than the other.
We find that GPT-4 shows promise in identifying jargon terms, but does tend to identify more terms than the human annotators. It successfully captured most human-identified terms but also incorrectly labeled many words as jargon which the annotators didn’t think were jargon. In information retrieval jargon this equates to 
a high recall but low precision
.
Interestingly, GPT-4 does maintain the relative differences in expertise between the two annotators, identifying more terms as jargon for the less expert annotator. This suggests potential for personalization based on readers’ knowledge levels, and aligns to 
similar recent findings
 as well.
Defining the Jargon
Once the jargon terms were identified, the next challenge was to provide clear, concise, and accurate definitions. To support our RAG approach we sourced relevant snippets of text for a given jargon term from the complete text of the source article. These are obtained by calculating the similarity of the jargon term to individual snippets of text from the article, and returning snippets that exceed a certain similarity threshold. We use cosine similarity to capture semantic relatedness and chose a threshold of 0.3 (low to medium) to capture a wide range of relevant snippets while excluding irrelevant ones.
We then used GPT-4 [2] with a 
query prompt
 to generate simple and understandable definitions from the retrieved snippets. We also generated another set of definitions for comparison, solely based on providing the abstract to GPT-4, and asking it to infer a definition of a given jargon term based on the text of the abstract. Annotators rated both definitions for a given term on their accuracy, and then recorded a preference for one or the other (or a tie), based on their clarity and informativeness. Pairwise preferences such as these are often used in 
LLM
 
evaluations
.
The accuracy is calculated as a percentage over all definitions generated by a model. The preferences are measured by calculating a win percentage, based on the number of times one approach (Abstract vs. RAG) is preferred over another.
Surprisingly, GPT-4 with the abstracts performs a little bit better than GPT-4 with RAG over the article text, both in terms of accuracy (96.6% vs. 93.5%) and win percentage (29.2% vs. 27.8% — the rest were ties). This suggests that more context from the scientific article did not necessarily lead to higher accuracy or better understandability (i.e. our first assumption about using RAG here as described above was not met). A deeper investigation of the retrieved snippets and their actual relevance to the jargon term may help understand if this is an issue with the quality of the context, or if there may be other causes such as the similarity threshold we used. To apply RAG successfully, it’s essential to explore and test different parameters. Without this kind of 
careful experimentation
, RAG on its own might not provide the desired results.
It may also be the case that the large size of GPT-4’s pre-training dataset enables it to draw from other sources to generate definitions. This can be as much a concern as a benefit though — it can make it harder to override irrelevant information from the pre-training data, or eschew its limitations based on cutoff dates for model training.
We also found that the effectiveness of these approaches varied based on the reader’s expertise. For instance, the less experienced annotator found similar value in both methods (higher tie percentage), while the more expert reader noticed more differences. Further evaluation with a larger set of annotators may help to replicate and understand these differences.
Closing Notes
This blogpost demonstrates a small and scoped experiment in using GPT-4 with RAG to generate definitions of jargon terms, in service of improving the experience of reading complex documents during the news gathering process. We find that GPT-4 performs fairly well at identifying jargon based on a reader’s expertise, although it does tend to over-predict a bit.
Contrary to expectations, we also find that GPT-4 with RAG over an article’s text performs a little worse in terms of accuracy and clarity/informativeness of generated definitions, when compared to GPT-4 with just the context of the article’s abstract. This finding leads us to draw out more questions that are worth considering in the context of using LLMs to support text generation and transformation in the newsroom, including in terms of how to evaluate RAG oriented systems. In addition, our exploration of including expertise in the prompt to identify jargon suggests a potentially valuable pattern for journalists seeking to emulate this experience in their own workflow: providing a natural language description of expertise and knowledge in the domain can help steer the model to be more helpful in identifying jargon.
Ultimately, the utility of such a prototype is contingent on how users actually incorporate it into their workflows, and if it actually saves time and improves readers’ comprehension in practice. We would love to know if you have tested out such RAG-based tools in your own newsroom, and how they have been perceived, received, and used!
—
[1] Our dataset and web-app are publicly available to support others interested in experimenting with our approach: 
https://github.com/ericlee878/ScienceDeJargonizer
.
[2] We used GPT-4 Turbo in May 2024.]]></content:encoded>
</item><item>
<title><![CDATA[A Possible Future of News in 2043: FutureNewsCorp]]></title>
<link>https://generative-ai-newsroom.com/a-possible-future-of-news-in-2043-futurenewscorp-984c3dd00ff8?source=collection_home_page----df04cefca135-----43-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Future Of Journalism]]></category><category><![CDATA[Scenario]]></category><category><![CDATA[Ai Regulation]]></category><category><![CDATA[Values]]></category><category><![CDATA[Standardisation]]></category>
<dc:creator><![CDATA[Natali Helberger]]></dc:creator>
<pubDate>2024-07-02</pubDate>
<atom:updated>2024-07-02</atom:updated>
<content:encoded><![CDATA[A Possible Future of News in 2043: FutureNewsCorp
Natali Helberger
Follow
12 min read
·
Jul 2, 2024
--
Listen
Share
Note
: We cannot predict the future of news and AI, but we can work towards realising a particular vision, or avoiding undesirable futures. After all, our choices and decisions help to shape the future. 
Scenario methods
 can be a way of identifying driving factors for particular future developments, anticipating impacts and risks and pondering the consequences of policy decisions. For the 
OSF AI in Journalism Futures workshop
 I further developed a scenario that was part of a legal investigation into the potential 
impact of the European AI Act
 for the media, and the driving factors that can make this scenario far less sci-fi than we might wish for. And if you, dear reader, find the idea of a “FutureNewsCorp” dystopian, I close this post by offering some food for thought of how you might be able to help to avoid that particular future.
FutureNewsCorp
With over 20,000 employees, FutureNewsCorp is the continent’s most significant computational media company. It employs an army of data scientists, automation and machine learning experts, developers, user engagement editors, immersive journalism solution architects, network analysts and futurologists. Together, these experts worked relentlessly, day and night, to bring all the news worth reading. Well-established in the news world, FutureNewsCorp is one of Europe’s longest-existing computational media companies. It is a pioneer in offering a service platform to all the deployers that rely on its services to produce and disseminate their content to the audience. FutureNewsCorp’s clients mainly consisted of media organisations and individual journalists (provided they could afford the service).
Often referred to as the “Netflix of AI technology”, FutureNewsCorp has revolutionised the news business with its AI technology. Specialising in the media sector, FutureNewsCorp offers a one-stop-shop for all the technical services required to produce and distribute media content, from the possibility to use advanced Large Language Models for editing and automated content production to various modes of distribution. Deployers could upload their contents or data and choose between three packages: the Premium Package, including fully personalised distribution, at least 2,000 individual targeting criteria to choose from, multi-format reporting via text, VR, immersive, chat and interactivity options (“Talk to the author”), and automated content production, editing and translation. This option also came with the complete data analytics package, including personalised pricing and adjusting distribution and content to individual user preferences, predictions of readership and audience valuation over 12 months. The 12-month prediction frame was still unrivalled — no other company could predict user preferences and interests over such a long-time frame. The Standard Package included the multi-format and interactivity option, group-based targeting based on 500 individual targeting criteria and niche audiences as small as 100 readers, but without the data analytics package and a flat-rate pricing model. Finally, the Economy Package was mainly reserved for freelance journalists not part of a journalist collective, academics, and civil society. There were no personalisation options for the economy user, nor was individualised pricing possible. The news would be distributed in untargeted and unchanged form to whoever was connected to the internet and willing to receive it. Thanks to FutureNewsCorp, it was now possible to match every reader with precisely the news products that fit her interests and station in life. Journalists and writers, or at least those who could afford it, would have never again wasted precious resources on producing articles that nobody would read.
There had been a time where many original AI systems were in-house systems that media organisations, large and small, built. Off-the-shelf systems were also available, but the licensing conditions would not allow media organisations to adequately adjust those systems to reflect the editorial values of the news outlet. To the extent that the AI models became more sophisticated and capable, their hunger for data, computing, energy and specialised expertise also grew exponentially. In addition, the regulatory frameworks, co-written by regulators and large technology companies, effectively suppressed functioning competition and firmly established the dominant position of a handful of companies. The economic and regulatory conditions forced most media organisations to succumb to the pressure of technology companies, sign their content archives away, and end their in-house R&D projects. To cut costs, fixed staff was more and more replaced with flex-workers. In the end, most media organisations had been turned into free-lance cooperatives that started using the services of specialised providers of news AI. Soon the economics of scale and scope did their work so that eventually, only a few very large news AI corporations were left that served professional and amateur media professionals. FutureNewsCorp was, and still is, one of the largest.
The influence of a small handful of computational media companies on the functioning of the media market did raise some concern, and more and more academics, civil society organisations and national policymakers called for additional guidance to ensure their commitment to the values and fundamental rights that have shaped European societies. In the end, the solution to the problem was standardisation. The AI Act foresaw a central role for standardisation bodies to develop standards of responsible AI, and the European Telecommunications Standards Institute (ETSI), in cooperation with the International Standards Organisation (ISO), developed the 980021-standard series. ETSI 98002121-EU stood for European Media Values, ISO 98002121-US for US Media Values, ISO-98002121-CHIN for Chinese Media Values, etc.
For Europe, after the gruesome experiences with the massive proliferation of disinformation that had shaken the trust of Europeans in the media and the abuse of the media as means of war propaganda in the Ukraine-Russia war, the European Commission made it very clear that any content that would include depictions of extreme violence, despair, political provocation and polarisation or would be suitable to induce panic and disturb the public peace and constructive deliberation was not in line with European values. As a result, the ETSI 98002121-EUR standard aimed to ensure a safe level of political information and a healthy public sphere with a balanced mix of news, entertainment, sports, and cultural news. And it was only a short time before companies like FutureNewsCorp could serve journalists all around Europe with user-friendly, tolerance-inducing news to stimulate harmonious public deliberation. Europe experienced its maybe most extended period of peace of mind 
…
Driving forces leading up to the FutureNewsCorp scenario
There are four driving forces to this scenario: technological developments, and in particular the AI-driven transformation of the media sector, economic dynamics, legislative developments, and the changing behaviour and habits of news users.
The costs and complexity of AI require more and more specialisation
The media industry has always been reliant on technology. The newest wave of technological innovation, data-driven analytics and automation, is no exception, and has already started to 
change journalism
 and the media 
more profoundly and all-encompassing
, ranging from new ways of supporting journalists in their research, assistance in the writing of pieces, and new forms of engaging and interacting with the audience, for example, through the personalised distribution of content or the use of virtual agents, up to the dynamic pricing of articles. And with the proliferation of Large Language Models, such as Open AI’s ChatGPT or Anthropic’s Claude, the development of automated content production (in the form of text, images or voice and video) has received a 
powerful boost
, and personalisation of not only the distribution side but also of content production (e.g. in the form of modular journalism) comes into play. The AI turn in the news business is characterised by a high degree of technological complexity, speed of innovation as well as resource intensity, be that in the form of data, computing or skilled tech expertise. Media organisations are 
confronted with the choice
 to procure externally and thereby benefit from the specialised expertise and resources of third parties, or to re-invent themselves and become more akin to technology companies. For many media organisations, however, the latter will not even be a very realistic choice, either because of the lack of investment in the necessary expertise and resources, or because of the difficulty of competing with the leading large technology companies for skilled personnel, the expertise and resources to build user-friendly technology, and the financial resources needed to be able to take risks in a highly dynamic environment.
Users do not per se need traditional media organisations to get news
Today, users have become accustomed to find media content everywhere, not only in the legacy media. What is more, trust in the media as an institution is reportedly at a 
steady decline
. Ironically, the media have played their role in educating users and teaching them that for accessing news, it is not necessary to visit news websites. Instead, news content finds you, on YouTube, TikTok, Instagram, Meta, or on the go, conversing with a virtual assistant. What is more, a whole new generation is trained in Google, Apple or Microsoft schools, and grows up as citizens of diverse Big Tech ecosystems. The news consumers of tomorrow have learned from a young age on that there is Google News, Apple News and Microsoft News. The growing amount of content, which will only further grow as a result of more and more synthetic content, makes it also increasingly necessary for users to rely on digital tools such as content moderation and recommender systems to navigate the digital media ecosystem. And as a result of the fierce competition for the attention of users, users have come to expect streamlined and consumer-oriented modes. They are used to information services that anticipate their personal preferences and information needs and serve them on a wink.
Winner-takes-all dynamics
Today, the real winners of the AI-boom are the handful of leading technology companies that concentrate a significant share of the resources needed to build and deploy AI at scale. These Big Tech companies benefit from considerable economics of scale and (global) scope. With LLMs, additional winners are cloud service providers, whose services are needed in order to be able to store and process computation-intensive training processes. In what can be best described as a self-reinforcing cycle, these companies use their resources, personal capital and also legal protection (IP, trade secrets) to reinforce their position in the broader information ecosystem and become ever more powerful and difficult to ignore. Possible alternative competitors are bought up, and IP rights are used to grow and protect dominant positions. As a result, the 
dependency
 of the media on external technology providers is only growing and the idea of competition and alternative infrastructures becomes increasingly more romantic but unrealistic.
Disintegration of media organisations and the freelance economy
The media industry is entangled with big tech companies in more than one way. The unequal competition for attention, advertising and audience loyalty is fatal for some media organisations; for others, it further increases the pressure to optimise for flexibility and to cut costs. As a consequence, the sector has experienced new waves of layoffs for several years. A growing share of news workers work as freelancers. The automation of more and more journalistic tasks will not leave the sector unaffected either, and at least change the profile of the traditional journalists in more than one way. Already now, researchers find a shift in social capital in news organisations, with technologists being better paid and offered 
more attractive conditions
 than many newsworkers. At the same time, there is a strong push for AI literacy of staff. Newsworkers become more and more tech workers.
Legal developments
Concerned about the aforementioned dynamics and the impact that AI and the tech players behind AI can have for society and its values, the European legislator has engaged in a regulatory frenzy, and tabled one influential regulation after the other. Common themes to the 
DSA
, the 
DMA
 and also the 
AI Act
 are the wish to impose more societal responsibility on (very large) technology providers, be that platforms, gatekeepers or AI developers, and force them to take public values and human rights into account when building and deploying their technologies. Whether or not regulations such as the DMA or the DSA will succeed in reigning in Big Tech remains to be seen, but what these regulations also do is to acknowledge and 
legitimise
 the position of Big Tech companies as dominant gateways and custodians of fundamental rights, such as privacy, freedom of expression and media pluralism. Similarly, the AI Act focuses primarily on the responsibilities of technology developers. It also acknowledges a central position for technology developers to define when and under which circumstances technology does comply with fundamental rights. And while the recent provisions on generative AI in the AI Act must lower the regulatory burdens for smaller “European champions” and only impose further-reaching obligations for the largest (US) models, this approach can also very much backfire. Professional users, like media organisations, will have to rely on signals that tell them which providers are (not) trustworthy. Under these conditions, being subject to more stringent regulations can become a competitive advantage. Not only can professional users rely on at least some regulatory safeguards, but many media organisations will have neither the time nor expertise to engage in expansive testing and evaluations of alternative models. Thereby, and ironically, also the legal framework may — involuntarily or not — fortify the position of a few leading Big Tech Providers.
The transformation of the media ecosystem as a result of the interplay of the above dynamics
News users do not per se need media organisations to find “news”, media organisations do not per se need to employ news workers to produce news, news workers do not need media organisations to bring their news to the people — but what all parties do need is the technological infrastructure to connect users and media content producers, and the technological tools to create news content. And while traditionally, that technology has been produced and controlled in-house (printing press, distribution network), the growing costs and complexity of digital infrastructures, be that platforms, be that cloud services, be that LLMs, are forcing media organisations to outsource the provision of the necessary technology to specialised companies. From there, it is only a small step to imagine a world in which specialised technology companies will replace media organisations as we know them. These will be companies that benefit from the necessary scale and scope to be able to operate profitably and across a diversity of media actors. Computational media organisation like FutureNewsCorp could develop out of a media organisation that has decided to switch and invest in becoming a technology provider. It could also be a technology company deciding to serve the media sector.
Practically, this means that specialised technology companies will decide on professional values, including those that are of relevance for the media. Seeing that this is not traditionally the core expertise of technology companies, there will be a prominent role for standardisation organisations to determine under which conditions or not a technology is in compliance with the AI Act and more generally the demands of human-centric AI that respects fundamental rights. With the Draft AI Act’s strong emphasis on human-centric AI design that respects fundamental rights and public values, the role of technical standardisation bodies can no longer be seen separately from their impact on shaping professional standards and the operationalisation of public values and fundamental rights. As a result, such harmonised standards will not be mere technical standards but will also interpret and operationalise European Union values and fundamental rights. Standardisation bodies have already begun to play a role in defining not only technical standards but in operationalising ethical values and human rights, and the AI Act firmly embeds the role of standardisation bodies as “
delegated rule makers
” to make European AI systems more human-centric and value-driven. The European Committee for Standardisation (CEN) and the European Committee for Electrotechnical Standardization (CENELEC) are private international non-profit organisations. Insofar, European standardisation organisations are inherently driven by a different logic than the public values logic that underlies, for example, the activities of the Council of Europe or many media organisations. In the scenario, ETSI 98002121-EUR is a standard for “content that sells.” The scenario shows that the risk-based approach of the AI Act, in combination with standardisation, may risk a situation where “risky” content, in the sense of content that disturbs, shocks or offends, is being sanitised in favour of safer, less controversial and more sellable content.
Some concluding thoughts
This is a dystopian scenario. For those who believe that the scenario of FutureNewsCorp is not a desirable future, now is the time to act and consider how to prevent scenarios like these from happening. From the scenario, we can derive some first ideas of where to start:
Un-teach the audience
: exploring how journalistic AI can be a means to better inform and strengthen the relationship with the audience, and make a more irresistible case for why news websites and services not only offer better content, but also offer it in a more interesting, engaging and smarter way than the Very Large Social Media Platforms or Generative Search Engines.
Local and specialised as the new “beautiful”:
 let’s be more demanding when procuring external technology and ask more 
critical questions
 about the quality and conditions under which the tech is offered. Big is not always beautiful. However, a vibrant playing field of journalistic AI providers that compete on how well they can support journalism and understand journalistic values cannot develop without popular demand.
We need more value literacy:
 invest and retain human talent, and recognise that AI literacy has two sides — making editors and journalists more technology literate, and technical staff more value literate.
De-couple: 
Taking the challenge of infrastructural dependency seriously.
Mind the standardisation bodies: 
introducing more fundamental rights and sectoral value expertise into technical standardisation bodies. And: technical standards can only go so far in operationalising values.
Beware of undesirable side effects
: critically assess to what extent regulations such as the DSA, DMA and AI Act, but also intellectual property law and data protection law contribute to re-enforcing dominance and tech solutionism rather than controlling monopolies and putting public values at the center.
—
Natali Helberger if University Professor at the University of Amsterdam and Co-director of the AI, Media & Democracy Lab (
https://www.aim4dem.nl/
)]]></content:encoded>
</item><item>
<title><![CDATA[CHI 2024 Research Roundup: Designing Generative AI for the News]]></title>
<link>https://generative-ai-newsroom.com/chi-2024-research-roundup-designing-generative-ai-for-the-news-2628b90f0d15?source=collection_home_page----df04cefca135-----44-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Design]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Human Ai Collaboration]]></category>
<dc:creator><![CDATA[Sachita Nishal]]></dc:creator>
<pubDate>2024-06-24</pubDate>
<atom:updated>2024-06-24</atom:updated>
<content:encoded><![CDATA[CHI 2024 Research Roundup: Designing Generative AI for the News
Sachita Nishal
Follow
6 min read
·
Jun 24, 2024
--
Listen
Share
This blogpost was written in collaboration with 
Charlotte Li
.
The 
CHI Conference on Human Factors in Computing Systems
 (known as “CHI”) is an annual gathering of researchers who study and design the various ways in which people interact with computers and digital technologies. This year we presented our recent work on 
designing context-sensitive evaluations for AI in the newsroom
 at the conference. We also attended a range of other talks and presentations with implications for designing and using AI in journalism. In this post we provide a brief roundup of some studies we found interesting and relevant, particularly for practitioners and researchers considering how to design, deploy, and use AI applications in the newsroom. If you find this interesting, you can also explore other 
general roundups
 of 
HCI and AI research
 
from CHI
.
How Knowledge Workers Think Generative AI Will (Not) Transform Their Industries
This study explored knowledge workers’ expectations of generative AI’s impact across seven industries, through participatory workshops conducted across the U.S. 
Knowledge workers
 is a term that refers to professionals who apply specialized knowledge to process, analyze, and create information or services, rather than physical goods. In the purview of this study, it referred to software developers, lawyers, educators and others, and especially of interest: journalists.
In different workshops and discussion sessions, the researchers taught participants how generative AI works, and tried to understand how participants then envisioned generative AI will change their fields. This is a useful study for anyone in the newsroom who is positioned to make policy about the use of generative AI by reporters and readers alike, to understand how people on the ground actually think about its potential and repercussions.
Across participants, a prevalent idea was that AI could be an effective tool for menial tasks when deployed under human supervision. However, journalists especially expressed deeper concerns about AI potentially exacerbating existing challenges in their “already embattled” profession. These challenges include the spread of disinformation, proliferation of low-quality content, and precarious employment conditions (e.g., increased pressures towards freelancing). There was also apprehension about AI replacing aspects of work that journalists find most fulfilling, such as writing and photojournalism.
The study brings these concerns to the forefront, suggesting that popular narratives around AI’s promises of efficiency or concerns about bias need nuancing. While reporters see AI’s potential in their workflows, they worry about its systematic impacts on their labor power, job satisfaction, and information quality. This perspective challenges the view of AI as a standalone source of power and influence. Instead, it emphasizes that change is systemic, with AI acting as both a tool and a factor worsening existing problems. Effectively using AI requires recognizing these issues and making more systematic changes. This approach is crucial not just for researchers, but also for practitioners and newsroom executives when considering AI implementation and management in their organizations.
A Design Space for Intelligent and Interactive Writing Assistants
AI writing assistants pose interesting use-cases for newsrooms, whether for writing short article summaries, newsletter sections, or social media blurbs. This recent study proposed a comprehensive 
design space
 for examining intelligent and interactive writing assistants across five key dimensions: task, user, technology, interaction, and ecosystem. A design space provides a structured framework to map out the range of possible features and characteristics for a given technology. It essentially gives us an organized way to think and make decisions about the various options available to us — across tasks, users, technology, interaction mechanisms, and ecosystems — when designing these tools.
For instance, when considering the 
tasks
 that AI writing assistants can be used for, the researchers urge us to think about certain very specific questions (paraphrased, with examples added from journalistic use-cases):
1. At what point in the writing process is this task taking place? (e.g. brainstorming headlines, planning a piece, making revisions from editorial feedback?)
2. What are the domain-specific norms or conventions that characterize this task? (e.g., journalistic norms focus on factual reporting, while academic norms stress formal presentation)
3. What is the purpose of the written artifact? (e.g., an editor using the assistant to generate newsletter summaries might want expository writing that conveys the crux of a piece, but also persuasive writing that stresses the hook)
4. How detailed are task requirements? (e.g., ensuring adherence to a style guide is a specific task with concrete objectives, unlike supporting brainstorming)
5. Who is the intended reader? (e.g. considering where the output is going such as to social media vs. an online website)
The authors offer similar questions to consider for the remaining aspects of the design space (users, technology, interaction, and ecosystem), to help designers ensure that the overall decision-making around system design is clear and thorough. The authors also suggest that the framework can guide the evaluation of models and prototypes, and help frame the system to users. This structured approach to designing AI writing assistants can be particularly valuable for newsrooms, helping them to develop tools that not only enhance efficiency but also align with journalistic standards, ethical considerations, and the specific needs of their reporters and readers.
The Effects of Perceived AI Use On Content Perceptions
This paper attempts to answer the question of whether AI authorship or co-authorship on written content, such as news articles, impacts readers’ judgment of the content.
The author conducted an experiment combining within-subjects and between-subjects factors, to investigate the research question. Participants of the experiment were asked to evaluate human written texts that were presented to them as created by either human, human with AI assistance, or AI. This data, along with participants’ explanations for their evaluation, was collected in a survey.
The author found that while AI authorship or co-authorship on written content had no impact on participants’ judgments of the content, such as originality, trustworthiness, or presentation, it did elicit negative feelings about the qualifications of the content creator, who was perceived by participants as opting to use AI in creating the content. Based on the explanations offered by study participants for their judgements, the author presents 5 ways in how participants interpret “AI assistance” in content creation. These interpretations draw from the point in time work is handed to AI, and the type of information handed to AI. These interpretations are “handing over the concept to be written,” “dividing ownership between facts and writing,” “handing off a draft for editing,” “work is handed off in the middle of writing,”and “no human intervention,” among which “handing over the concept to be written” and “no human intervention” are the most commonly used interpretations.
These findings might inform practices surrounding assigning bylines to AI, or acknowledging the use of AI in creating news articles in newsrooms. For instance, based on these findings, we might expect that including AI in bylines might not increase critical thinking and judgment of the news content but instead potentially decrease the credibility of the reporter or the news organization at large. However, it is worth noting that this study doesn’t explore how different specificities of describing AI-involvement in content creation might change the judgment of the content and perception of the authors. Further explorations (and data-driven experiments) on how newsrooms might credit the use of AI without sacrificing reader trust are worth undertaking.
Closing Notes
Newsrooms that rush to implement AI tools risk overlooking important design considerations that would actually make these implementations useful and successful. This set of studies from CHI reveals that effective integration of AI requires more than technical know-how — it demands a deeper understanding of users’ practices, values, perceptions, and concerns, at both an individual and also a broader systemic level. In the news environment, users include journalists, graphic designers, readers, and so on.
By embracing insights and approaches from HCI, newsrooms can develop tools that truly support and augment how newsworkers and audiences engage with information. This approach isn’t just about better model performance, or interaction design — it’s about reshaping the processes of how news is produced and consumed, and the relationships between journalists and their audiences in AI-augmented media futures.]]></content:encoded>
</item><item>
<title><![CDATA[Prompting Techniques and Best Practices for Journalists]]></title>
<link>https://generative-ai-newsroom.com/prompting-techniques-and-best-practices-for-journalists-6fa806cde8ec?source=collection_home_page----df04cefca135-----45-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Prompt Engineering]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Large Language Models]]></category><category><![CDATA[News]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Mowafak Allaham]]></dc:creator>
<pubDate>2024-06-18</pubDate>
<atom:updated>2024-06-18</atom:updated>
<content:encoded><![CDATA[Prompting Techniques and Best Practices for Journalists
Mowafak Allaham
Follow
11 min read
·
Jun 18, 2024
--
Listen
Share
Image is generated by GPT-4o using the following prompt: “Generate an image in a modern vintage form of an AI guide that enables a machine to work”
Note:
 This post was co-authored with: Michael Crystal, Mona Gomaa, and 
Nick Diakopoulos
A recent 
survey
 from the Associated Press revealed a predominant interest in using generative AI for tasks relevant to content production such as information gathering, data analysis, multimedia content production (e.g., generating images), text summarization, translation, and transcription. Clearly, large language models (LLMs) have the potential to upend how journalists create content for news media, but in order to capture their value journalists need effective ways to adapt these models for various journalistic use cases. One way to do this is by training models for news tasks, though as one of us has recently 
argued
, that could be costly. Another path that is easier to get started with (though which can also require a great deal of skill and time investment) is 
prompt engineering
.
In this article we describe some best practices for how LLMs can be controlled using different prompting techniques to elicit the desired outputs for tasks in journalism. We first provide some background context on prompt engineering before we elaborate on a few different prompting techniques, providing illustrative examples in news production. We close with some ideas about prompt validation and output quality evaluation.
What is Prompt Engineering?
Prompt engineering is an emerging communication technique between users and LLMs that is used to craft questions and instructions to elicit a desired response from LLMs. While prompt engineering appears to be straightforward on the surface, it requires domain expertise in different prompting techniques to fully reap the benefits of LLMs. For instance, 
this guide
 lists 17 different approaches to prompting, some of which are rather structured and involved. Moreover, different models may require 
different prompt formats
 or techniques to get the best performance.
Characteristics of an Effective Prompt
To achieve better results from LLMs, all prompts should be expressed in full sentences and communicate clearly with specific language the set of instructions a model needs to follow (and sometimes 
not 
follow) when completing a task. In addition, instructing the model 
to work out its own steps or solution
 before it reaches a conclusion is 
proven to enhance the model’s capability to generate better outputs
, especially for analytical tasks that require reasoning. Another key characteristic of a good prompt is scoping the prompt for a specific task. Prompting for a very broad and open-ended task (e.g. “Produce an online news site”) will not be as effective as breaking down the tasks and sub-tasks of that overall goal to tackle them individually. In order to minimize the possibility of models diverging from their given instructions, OpenAI’s prompting guide 
recommends 
that complex tasks are split into subtasks each having their own prompt.
In general, a good prompt is structured to include the prompt 
text 
and
 context
 (see section 3 of Joe Amditis’ 
Beginner’s Prompt Handbook
)
. 
However, depending on the task, this structure can be extended to include 
constraints
 (e.g., specifying the length of generated text), 
tools
 such as accessing external services or data sources (e.g., Yahoo Finance API), or 
additional instructions
 (e.g., specifying tone or writing style). For instance, for tasks that require factual information, it is better for prompts to include a trustworthy reference text in its context to ground the output.
One thing to be aware of in terms of responsible prompting is that you may need to be careful about including confidential or personally identifiable information (PII) as part of prompt context if such context is being shared with third-party AI platforms (e.g. OpenAI or Anthropic). To address the PII issue you can replace specific information with generic anonymized information (e.g. “Walter Smith” gets substituted with “<NAME>” in the prompt) and then after you receive a response from the service you can substitute the original name back, if needed. Another approach is to use models that are locally hosted so that prompt context is not being shared with outside parties.
Some models also support the use of 
system prompts
 which can set the overall stage for how a system responds in terms of tone, style, or to stay grounded in the material included in the context. For instance you might include “Your responses are always grounded in the specific document provided to you.” as a system prompt to encourage the model to only operate on the information included in a document you provide in the prompt as context. As with non-system prompts however, it’s important to remember that the model may not always follow the instructions to the letter and so checking outputs is still necessary.
Next, we discuss how these characteristics can be applied to different prompting techniques.
An Overview of Key Prompting Techniques
Zero-shot Prompting
Zero-shot prompting is a technique that entails instructing the model to complete a task without providing it with examples or demonstrations of the task or its output (i.e. “zero” in zero-shot refers to zero examples provided). This type of prompting can be useful for tasks that require ideation (e.g., brainstorming interview questions as illustrated in Figure 1), content generation (e.g., drafts for headlines), and translation. However, this technique requires many iterations over the base prompt to include the necessary details to reach the desired output. This could be attributed to the statistical nature of LLMs that makes it difficult to consistently predict the model’s exact responses and format when completing a task.
For use cases that require factual answers such as a summary of the latest news, zero-shot prompting has the limitation that, in the absence of other context, it relies only on the training data of the model to provide its response. Accordingly, whenever zero-shot prompting is used with minimal instructions, journalists need to be particularly wary about the model’s generated output because it may only reflect the knowledge present in the model’s training data which could be outdated. One way to ensure the text generated by a model is relevant to the task at hand is to include relevant context in the prompt such as the leading paragraph of an article when asking the model to generate engaging headlines based on that excerpt. In some cases, state of the art models such as GPT-4o can also a zero-shot prompt and then also access up-to-date sources by browsing the internet to provide context (Figures 2A & 2B).
Figure 1. In this example, we ask the model to give us a question for an interview we are conducting with the Editor in Chief (EIC) of the Chicago Tribune. We provide details about the content of the interview, but not any details about the format of the question. This allows the model to decide how to structure the question.
Figure 2A. Example GPT4-o output demonstrating how the model behavior is not fully inline with the user requests. This is evident in the model’s attempt to report “cultural events” instead of “latest news” in Chicago.
Figure 2B. Example of GPT-4o output when writing an effective prompt that includes a request for the model to “provide links to sources”. In contrast to Figure 2A, the generated text from this prompt resulted in more informative output that includes a mix of events and local news from three local sources.
Few-shot Prompting
For use cases that require additional details, structure, and formatted response to the prompts, few-shot prompting provides an alternative to zero-shot prompting. Few-shot prompting builds on top of zero-shot prompting by including examples of the task or the formatting of the output as part of the prompt context (as illustrated in Figure 3). This enables the model to recognize the desired pattern and behavior that it should reflect in the generated text. For instance, journalists can use a few-shot prompt to classify users’ comments to news articles by providing the LLM with a few examples of negative and positive comments along with their corresponding labels. This could enable journalists to have an accessible way of understanding how the audience is responding to a news article. Similarly, few-shot prompting can be useful for information extraction (e.g., extracting named entities like people or places) or categorization (e.g. labeling content according to a schema) use cases. The main challenge for effective few-shot prompting is the development of examples that demonstrate to the model how to complete a task so it can ideally generalize across those examples.
Figure 3. An example illustrating few-shot prompting by including four examples in the prompt context and how they helped inform the model of how we want to classify positive versus negative comments. As shown, the model accurately classifies the three comments provided to it.
Chain-of-Thought Prompting (CoT)
Chain of Thought
 prompting techniques enable models to “reason” about complex tasks through intermediary steps. However, it is not a mutually exclusive technique to the other prompting techniques we have described thus far. In its most basic form you can append “let’s think step by step” to any prompt including the zero and few shot prompts. The reason this works is that, before tackling the task, the model will output context which can further help ground the output. This prompting technique has proven to be useful for analytical tasks such as answering quantitative questions from text. It can also be combined with few-shot prompting to get better results on more complex tasks that require reasoning before responding.
As Figures 4A and 4B illustrate, instructing the model to think step-by-step using zero-shot prompting combined with CoT results in a more refined answer that includes details about the model’s reasoning steps compared to using only zero-shot prompting on the same task. The intermediate output has the additional benefit of offering some visibility for people to understand how the model arrived at the final output it did. One thing to keep in mind, though, especially when using an API to prompt a third party model, is that since CoT prompting produces more (intermediate) output tokens, the cost could be higher.
Figure 4A. Example of generating a headline for an article using the leading paragraphs of the 
AI Bill of Rights
 using zero-shot prompting.
Figure 4B. Example of using CoT to generate an article headline about the AI Bill of Rights. In contrast to 4A, the model outputs the series of steps that are grounded in the excerpt of text from the bill to generate the headline. In addition, this title appears to be more specific than the one shown in 4A as it includes the Biden Administration as a named entity and it describes bias as a specific dimension the bill of rights is intended to overcome.
Effective Quality Control in Prompting
There are two basic approaches to effectively control the quality of prompts: (1) validating the prompts ahead of time to establish their robustness, and (2) assessing outputs required in task-specific use cases according to various quality criteria.
Prompt Validation
While writing prompts is perhaps the easiest way to get started with tapping into the power of LLMs, it can take a lot of iteration and time to refine a prompt so that it’s producing high quality outputs that you might be comfortable incorporating into your larger workflows. In this sense it can be helpful to think about the validation process for developing a prompt: 
How do you know if it’s an effective prompt?
We recommend an approach requiring two datasets that you need to put together for each task you want to target: one to assess the performance of the prompt as it is developed — the 
development
 set — and another separate one — the 
test
 set — that you use to evaluate the prompt only after you’ve optimized it as much as possible on the development set. Ideally, both are random samples so there’s no underlying selection bias in either. Having two sets has the effect of helping you understand whether the prompt you developed and refined to work well with the development set also generalizes to the test set and still works well there. The larger and more varied your development and test sets, the more confident you can be that your prompt is working well across a range of conditions. Take note, however, every time you swap out a model, or a provider updates their model, you will need to re-run this validation!
As part of the prompt validation process, it is also important to account for the statistical variation of LLMs in generating the text corresponding to each iteration of the prompts. While the examples included in this blogpost were generated using the ChatGPT user interface, we encourage the use of OpenAI’s 
Playground 
when developing and validating prompts as it gives users the ability to control several parameters that are useful for this validation.
Probably the most important parameter to be aware of is 
temperature, 
which
 
controls the level of randomness vs. determinism in the generated text
. 
LLMs work by generating one token at a time, and a higher temperature gives the model more leeway to select from a larger number of next tokens. For OpenAI’s models this value defaults to 1 with a range from 0 to 2. For tasks that should be tightly grounded in the context provided as part of the prompt (e.g. ensuring a generated summary doesn’t stray from an underlying document) you should set the temperature closer to zero. Alternatively, in use cases that may require more “creative” or “verbose” text, such as brainstorming or headline ideation, setting the temperature value closer to 2 should result in a higher variation of generated texts for the same prompt. Of course, the higher the temperature, the more variance there will be in outputs and thus you will need to run more trials to assess validity.
Prompt validation is one of the under-estimated complexities of prompt engineering that must be accounted for before using the crafted prompts for a specific task at scale. At the same time, once validated in a systematic way, you can feel more confident sharing prompts within your organization and so reduce overlapping effort from everyone in your newsroom coming up with their own version. In addition, documenting the development process of prompts, by establishing benchmark development and test datasets will contribute towards more transparent, reproducible, and scalable prompt engineering and validation practices across tasks and newsrooms.
Evaluating the Quality of AI-generated Content
By now, many folks using LLMs are aware that they are prone to hallucinate — potentially making up information that is not grounded in reality due to the statistical nature of the way they produce outputs. If this is not accounted for in journalism, it could have ramifications for the integrity and trust in the news.
While 
evaluation criteria are likely to vary based on the task
 the LLMs are used for, in general we suggest that journalists might minimally consider dimensions pertaining to information 
accuracy
 and 
bias, 
including
 
both
 
political and representational (over-representing or under-represented groups) bias. Additional qualitative dimensions with respect to the language of the generated text can also be accounted for in the evaluation process such as 
coherence
 (Does the generated text have any incomplete or unclear sentences?), 
granularity
 (How detailed is the language of the generated text?), and 
relevance
 (How relevant is the generated text to the request described in the prompt?). These are of course just a few of the many suggestions for criteria that you might evaluate, and we encourage the news industry to develop more task-specific criteria to evaluate the success of LLM outputs as well. One possible way forward might be for journalists to collaborate with technologists or social scientists to establish robust and trustworthy evaluation metrics and criteria across various news-related tasks.
But even while we recommend such evaluations as indicators of whether a prompt 
tends
 to produce higher quality output in general, it is nonetheless prudent from a responsible use point of view to maintain a human in the loop for many use cases. This is particularly important for dimensions like accuracy when the outputs of models might be presented directly to the audience.
Honing Your Prompting Further
As the capabilities of LLMs continue to advance, we recommend that you periodically check prompting guides and libraries relevant to third party models so you are up-to-date with the latest prompting techniques and their applications. Below we list a few resources you may want to bookmark as additional references for prompting:
Prompt Engineering — OpenAI
Prompt Examples — OpenAI
Intro to prompting — Anthropic
Prompt Library — Anthropic
Beginner’s prompt handbook: ChatGPT for local news publishers]]></content:encoded>
</item><item>
<title><![CDATA[Here’s What GPT-4 Thinks Being a Journalist is All About]]></title>
<link>https://generative-ai-newsroom.com/heres-what-gpt-4-thinks-being-a-journalist-is-all-about-88028ae27a01?source=collection_home_page----df04cefca135-----46-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Ai In Journalism]]></category><category><![CDATA[Future Of Work]]></category><category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Gpt 4]]></category><category><![CDATA[Agentic Ai]]></category>
<dc:creator><![CDATA[Charlotte Li]]></dc:creator>
<pubDate>2024-05-26</pubDate>
<atom:updated>2024-05-26</atom:updated>
<content:encoded><![CDATA[Here’s What GPT-4 Thinks Being a Journalist is All About
Charlotte Li
Follow
8 min read
·
May 26, 2024
--
1
Listen
Share
Note: This post was co-authored with 
Nick Diakopoulos
.
As AI becomes more integrated in people’s everyday life and work, 
conversations
 around building agents to assist with tedious tasks in various workspaces, including 
newsrooms,
 have also surfaced. So-called “agentic AI” systems 
have been defined
 as those that “can pursue complex goals with limited direct supervision”. In order to be able to autonomously assist with newswork, an AI agent needs to be able to take a high-level directive, like “write me a news article about the latest social trend” and break it down into tasks that it can accomplish towards achieving that larger goal. In other words, it needs to have a sense of the work of journalism and how tasks can be decomposed so that it can plan out the work.
Image Credit: Turing Commons (
https://alan-turing-institute.github.io/turing-commons/resources/gallery/
)
So, just how much does a model like GPT-4 know about how journalism gets done? The depths of its knowledge of the tasks inherent to producing news should be indicative of its potential to leverage that knowledge in an agentic AI system. While research has recently explored the impact of generative AI on occupations such as 
entrepreneurs
 and 
data scientists
, with methodologies like 
user-centric participatory studies
 and 
large scale quantitative approaches
, little has touched specifically on the field of journalistic work, especially when it comes to how generative AI can play a role as a supportive agent.
To begin to answer this question we prompted GPT-4 for a list of journalistic tasks to test the extent of its knowledge about what journalists do. Though news tasks differ drastically depending on one’s role in the newsroom, the type of newsroom, and the subject of their reporting, GPT-4 surprised us by outputting a sensible array of news tasks. It appears to have reasonable coverage of what it means to “do” journalism, at least in terms of specific tasks entailed in news production.
Our Approach
In order to understand the quality of GPT-4’s knowledge about journalism tasks, we leveraged the 
O*NET Resource Center
 for expert-validated occupational information about journalists¹. Specifically, we looked at the 
News Analysts, Reporters, and Journalists
 occupation category, which includes both a list of tasks that are specific to this category and a ranked list of work activities that tend to be more “important”² to the occupation. These generic activities include things such as “Getting Information”, “Interpreting the Meaning of Information for Others” and “Thinking Creativity.”
With the list of work activities that are related to the journalist occupation category on O*Net, we then prompted GPT-4 to produce a list of tasks that are related to each of those activities using the following prompt.
'For the following work activity, please give one sentence each for as many tasks as possible that a journalist might do that are related to it. Please avoid providing duplicated tasks, and the level of specificity should be consistent across all tasks, Work Activity: {activity name} — {activity description}'
A few things are worth noting in this prompt. First, we only prompt once per activity while asking GPT-4 to come up with as many tasks as possible related to that activity. This is because when we set out to prompt GPT-4 multiple times for the same activity, it often repeats some tasks across several different responses, making it hard to de-duplicate the data. Additionally, we also explicitly asked GPT-4 to create non-repeating tasks in the same level of specificity, in order to obtain results that are unique and comparable to each other.
Using the prompt above to query OpenAI’s “gpt-4” model on March 29th, 2024, we obtained 285 task descriptions in total across the top 19 most important activities listed, each activity receiving 10 to 20 tasks. We then manually clustered these tasks by hand, disregarding their original activity categories. We iterate on inductively labeling and re-clustering these tasks until some larger themes emerge, resulting in a 
GPT journalistic tasks taxonomy
.
What We Found
Through a process of iterative thematic grouping we ended up organizing the outputs from GPT-4 into 6 high level categories: 
gathering information
, 
sensemaking
, 
editing
, 
publication and distribution
, 
productivity
, and 
journalism training
. Each high level category is then divided into several sub categories reflecting more specific tasks. We illustrate some codes but refer the reader to the 
full taxonomy
 for all of the examples.
Gathering information
: This category of tasks includes tasks that aim to monitor, source, and gather information for reporting purposes. Tasks under this category are either organized by the 
type of sources
 involved in information gathering (e.g. experts, scholarly sources, social media, etc.), the 
method used
 for information gathering (e.g. interviewing, field work, information requests, etc), or ways to 
maintain sources
. For example, social media can be a type of source, for which “A key task could be monitoring social media channels for breaking news and trending topics” would belong. Methods of information gathering, on the other hand, can include tasks such as interviewing: “A journalist may conduct in-depth interviews with key individuals related to the story.” And an example of a source maintenance task produced by GPT-4 was, “They might participate in community events to establish relationships with locals and gather potential news stories.”
Sensemaking
: This category consists of tasks that journalists do to make sense of concepts for different purposes. It includes tasks such as 
ideation
 (e.g. “A journalist might come up with a fresh angle to approach a widely reported story.”); judging the 
newsworthiness
 of an item (e.g. “A journalist may assess the newsworthiness of a press release or tip.”); 
analyzing
 contents of different 
source types
 (e.g. text or data) and 
domains
 (e.g. politics, law, health, science, etc.); conducting 
archival research
; or making sense of 
multimedia
 content. Sensemaking tasks may thus intersect with tasks during many phases of production such as gathering, editing, and publication.
Editing
: This category includes tasks that involve checking and editing work in progress for readability, accuracy, legality, and other journalistic editorial judgements related to standards. An example task generated by GPT-4 related to 
readability
 is, “They might judge the readability and coherence of their own piece before publishing.” An example of 
verification
 and checking for accuracy is “They might have to cross-check data from multiple sources to ensure consistency.”
Publication and Distribution
: This category includes tasks that aim to publish and deliver content to an audience that maximizes information spread and readership. Tasks in this category represent a wide variety of ways to deliver information to readers, such as 
live delivery
 (e.g. “They may deliver information to the public through live broadcasts or social media updates.”), or 
publishing
 content in various online formats (e.g. “A journalist could send newsletters or email updates to subscribers, updating them on recent news or articles.”). This category also includes ways journalists may interact and 
engage with their audience
 in order to understand the impact of their publication and receive feedback. Of course different technologies, such as newspaper and broadcast have different relationships to publication and distribution.
Productivity
: Tasks in this category have the goal of increasing productivity and ensuring the prompt delivery of the work that journalists do. Tasks here include 
organizing
 
workspaces and time
 (e.g. “They may maintain a digital calendar of upcoming news-worthy events.”); 
managing content
 to be published (e.g. “They might utilize an online content management system to upload their articles, along with relevant photos or videos.”); 
planning
 for work and projects (e.g. “They may establish a plan for conducting background research around the topic of their story.”); and 
coordinating
 with other news employees (e.g. “A journalist may meet in person with their supervisor to discuss concerns or questions about a current assignment”).
Journalism Training
: This category of tasks is related to personal and skill developments for journalists to improve their expertises as a journalist. This includes training on 
writing
 (e.g. “They could take part in training or a workshop to master a new style of writing or reporting.”); using 
technology
 (e.g. “They might learn to use a new software for data visualization to make their articles more engaging.”); or participating in training that are general to the work of journalism (e.g. “They would participate in training and development activities with colleagues for team building”).
It was surprising that among the 285 tasks output by GPT-4, 
writing
 as a task is only directly mentioned once outside of the context of journalism training. While this might be attributed to the language used to prompt GPT-4, which was adapted from O*Net and doesn’t explicitly mention writing as an activity, it might reflect a lack in GPT-4’s understanding of 
how
 journalists go about achieving each task. Another surprising aspect of this result is the mention of journalism training when such learning was not necessarily explicitly relayed to GPT-4 as an activity that journalists do.
Future Directions
This taxonomy on journalism tasks generated by GPT-4 should not be considered a definitive guide to journalistic work. But it does begin to establish the range of “understanding” the model has for the work of journalism. And in the future it may serve both as a methodological approach for exploring other models’ knowledge of journalism as well as a baseline for further investigations of the usage of AI in journalism tasks.
Aspects of the methodology can be improved in a few ways. For one, we prompted only for the work “a journalist” might do, thus the outputs we analyzed largely depended on what it associated with the word “journalist.” However, as pointed out earlier, people serve different roles in newsrooms, and their work differs a lot depending on their roles. Investigating how the outputs from GPT-4 might differ based on different newsroom roles given to it could reveal discrepancies in its understanding of more specific aspects of newswork (e.g. of a news photographer, or an audience engagement editor). Additionally, the descriptions of work activities adapted from O*Net seem to influence the outputs of GPT-4 a lot, especially in the action words used to describe tasks. Changing the wording of the activities in the prompts might change the outputs in ways we have not explored yet and could shed light on different prompting strategies when adapting language models into practice.
One of the most significant improvements to these results, however, lie in practitioner-centered validation and adaptation of this taxonomy. We are interested in exploring whether the level of specificity and the span of coverage of this taxonomy make sense to practitioners of newswork. Given these tasks, we are also interested in exploring the current strategies for and attitudes towards completing these tasks, the compatibility for adopting AI-assistance to each of these tasks, and the criteria for evaluating successful performance on these tasks.
This leads us to conclude this blog with an invitation for you to participate in this discussion of AI-assisted journalistic work. Does the typology reflect what you think of as newswork? What do you think may be missing? How would you like to use AI in your work?
See or download our data in 
spreadsheet format
!
[1]: O*Net claims to be “the nation’s primary source of occupational information,” and we further investigated the methodologies O*Net uses to generate and update their occupation-specific tasks to ensure the validity of these task descriptions. According to a report on the instruments for O*Net data collection: “Occupation Specific Requirements are not measured using questionnaires, but are measured by job analyst observations, job holder and supervisor descriptions elicited in group discussions, and task inventories.” A later report detailed that “since many occupations change over time (e.g., due to new technology), job incumbents are given the opportunity to write in information about important tasks they believe are excluded from the current task list for their occupation.” Most recently, researchers continued to improve occupational task descriptions using internet search methods (2011) and automated evaluation of write-in task descriptions through natural language processing (2021).
[2]: Importance is defined here as a score of greater than 3 out of 5 on the O*NET rating scale of 1–5 of importance, see here: 
https://www.onetonline.org/help/online/scales]]></content:encoded>
</item><item>
<title><![CDATA[What’s in OpenAI’s Custom GPT Store for Journalists?]]></title>
<link>https://generative-ai-newsroom.com/whats-in-openai-s-custom-gpt-store-for-journalists-afc690875ef6?source=collection_home_page----df04cefca135-----47-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[OpenAI]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[AI]]></category><category><![CDATA[Gpt Store]]></category>
<dc:creator><![CDATA[Michael Crystal]]></dc:creator>
<pubDate>2024-04-18</pubDate>
<atom:updated>2024-04-18</atom:updated>
<content:encoded><![CDATA[What’s in OpenAI’s Custom GPT Store for Journalists?
Michael Crystal
Follow
9 min read
·
Apr 18, 2024
--
7
Listen
Share
Professionals in the news industry are 
beginning to wonder 
how generative AI will change the way they do their jobs. Some fear their work may disappear because of AI, while others believe it won’t help them at all. As a student journalist at Northwestern University, but also as someone double majoring in computer science, I don’t fear AI completely replacing me (for now), but I do believe that leveraging the right tools can help me work more efficiently. It always seems like I don’t have enough time to write and publish all the story ideas I have. So I wondered: How could generative AI speed up my work?
Open AI 
recently launched 
the 
GPT store
, which contains specialized versions of ChatGPT built by both OpenAI and other users to help with specific tasks. Some journalists 
have started developing these custom GPTs
 to support specific tasks they perform. In hopes of finding more of these kinds of tools to help me research, write, report, and illustrate my work more efficiently, this post walks through how I systematically explored what’s on offer there to support journalists. I first describe how I collected a broad set of custom GPTs that may be relevant to journalism, and then I expand on how I rated and ranked them for journalistic tasks.
Collecting GPTs
Since the GPT store doesn’t have a complete list of all GPTs available to the public, I decided to acquire them through the platform’s search functionality. To do this programmatically, I executed search requests of the GPT store using hundreds of keywords relating to a wide range of journalism tasks and activities.
To create a starting point for these keywords, I prompted ChatGPT (see footnote [1]). For the prompt, I plugged in the 30 tasks and 19 work activities found on 
ONet
 under the “News Analysts, Reporters, and Journalists” occupation page. ONet, which was developed for the U.S. Department of Labor, provides a comprehensive and detailed database of occupational information, ensuring reliable and current data on job roles and responsibilities. This prompt produced 40 keywords, but to find an even more comprehensive list of GPTs, I wanted to further expand my base of search terms. To do this, I again used ChatGPT to perform a keyword expansion on each of my first 40 keywords (see footnote [2]).
After this keyword expansion step, I had 586 journalism-related keywords and keyphrases. I did some quick trimming to eliminate terms that were obviously not journalistic and wouldn’t produce the types of GPTs that I sought. After this filtering, I 
had 552 terms left
.
To collect relevant GPTs, I ran a search request using 
cURL
 (a command line utility) with each keyword (for a template of the request see footnote [3]). Each request returned a JSON object that contained information about the first 10 GPTs matching the keyword (it seems to be a limitation of the search functionality that it can only return up to 10 items per search). From these requests, I was able to collect the name, description and unique ID of the GPT, as well as example conversation starters (these are meant to help users understand how they could use the GPT), and the number of conversations (an indicator of GPT use). After performing these requests for each keyword and filtering out duplicates based on ID number, I had a corpus of 3,749 GPTs.
For efficiency in downstream analysis, I decided to filter out GPTs that had fewer than 100 conversations, focusing the corpus on those GPTs that had more traction and usage. After doing this, I was left with 
693 GPTs
.
Rating and Ranking GPTs
With a long list of journalism-related GPTs, I needed a system for deciding which GPTs could be the most useful. Ideally, we would test these all manually and write reviews on them, but this wasn’t a feasible starting point for almost 700 models. So I decided to use GPT-4 via the OpenAI API to rank the GPTs I had collected and then manually test the highest-ranked models.
To rate them on a scale from 1 (not useful) to 4 (very useful) I used the 
same 30 journalistic tasks from ONet
 as above to assess the utility of each GPT for each of the tasks (see footnote [4]).
With these rankings, I calculated some average statistics about each GPT to have a better idea of what the results meant. I averaged each GPT’s ratings across tasks and also tallied the distribution of 4s, 3s, 2s, and 1s for each GPT. I thought this would give me an idea of which GPTs would be the most useful for my purposes because I could filter by the highest-ranked GPT and by GPTs with a lot of high rankings.
However, I realized this may not tell the entire story. Instead of only judging the GPTs based on their general scores, I also decided to see if certain GPTs were highly ranked in certain specialties or clusters of tasks, but potentially ranked lower overall. To do this I manually divided the 
30 tasks into four subgroups
 (Writing and Editing, Reporting and Investigating, Broadcast and Multimedia, and Communication and Engagement). Then, using my already generated rankings, I calculated averages for each of the subgroups. This allowed me to find GPTs that were more specific to a more focused set of tasks in journalism, instead of only GPTs that were helpful across the entire set of tasks.
Testing GPTs
Based on the ratings and rankings I created, the next step was to manually test some of the highest-ranked GPTs. I wanted to see if they were actually useful in the story creation process. To create my test set I took the four overall highest-ranked GPTs, and the two highest-ranked from each task subgroup, resulting in 
12 GPTs
.
Then, to test the selected GPTs, I decided to use them in the way I would as a journalist and record my observations. While not entirely systematic, as a first cut at assessment, this was the most flexible way to see how the GPTs might be useful in my workflow.
I spent 45 minutes experimenting with each GPT. For the four GPTs that got the highest overall scores, I tried performing tasks that could benefit me throughout all aspects of my work. For the more specialized models, I focused on aspects of journalism that were more targeted and pertinent to the described subset of tasks they could assist with.
Takeaways
The most obvious takeaway from this experience was the difference between using the high overall ranking GPTs for a broad range of tasks versus using specialized GPTs for a refined range of tasks. For example, 
Journalist Pro
, a GPT that was supposed to be able to assist with a wide range of tasks, gave very bland and, in many cases, unhelpful results. On the other hand, 
Fact-checking
, a GPT specifically designed to identify incorrect statements and opinions vs. facts in writing, was successfully able to verify my writing was based in fact and spot sentences to either double check or change.
This trend continued across all of the GPTs I tested: the general GPTs gave disappointing and basic results, while the specialized GPTs were useful in solving their smaller tasks. Even when I attempted to have the more general GPTs perform the smaller tasks that the specialized GPTs excelled in, they let me down.
Legal Eye
, the 2nd highest overall ranked model, is a GPT that is described as helping with research, sourcing, and writing. However, I found it ineffective for finding pertinent articles for my ideas, suggested interview subjects that didn’t make sense for my angle, and when provided with information, wrote a subpar story that lacked the legal and political analysis it claimed it would give.
Improve my Writing
, my highest ranked writing and editing GPT did a fantastic job of finding grammatical errors and correcting sentence structure. A feature I particularly enjoyed was how it could alter an entire piece of writing by converting the tone or writing style to a completely different one. For example, I gave the model a piece I wrote that was informative and serious. I had the GPT change the story to something more catered to a younger audience, and it did an impressive job of this. This model, while smaller in scope of tasks than Legal Eye, did a more thorough job of the tasks I had it perform.
Video Maker by Lucas AI
, my highest-ranked broadcast and multimedia GPT, did a great job of script creation and making informative videos with interesting visuals. It converted written pieces into reasonable videos. However, these videos were not production level, with the narrator being monotone and the images sometimes being mismatched to the script. It was a good tool to create a baseline video, but additional editing would be needed to publish it.
Conclusion
This project gave me some new ideas that I think can be useful to other journalists thinking about using custom GPTs.
My advice would be to find the biggest pain points in your process and then do research into specialized GPTs that could solve this issue. For example, if you find yourself spinning your wheels on identifying useful interviewees for your story, dig around for GPTs that specialize in finding people or doing article research. Don’t assume a general GPT aimed towards journalism can help you. 
Research GPT
, a custom-built GPT that is designed to help with complex research issues by providing experts to speak to and articles to read, may be of use in this situation. If it isn’t, doing a simple search for “source finder” and adjacent keywords in the GPT store will generate GPTs built specifically for your task at hand. From there, play around with each one, as they will have small discrepancies that could influence your work. Also, looking at the GPT’s ratings and number of conversations can be an indicator of which models are better developed and more tested.
In the future it could also make sense to develop a more rigorous set of evaluation criteria for each — would you want in-depth reviews of custom GPTs for specific use cases and tasks you have? Let me know!
—
Footnotes
[1]: Given the following journalistic tasks and activities, generate 40 keywords that accurately and comprehensively summarize them. Make sure these keywords are good terms to use in a search.
Tasks: {tasks}
Activities: {activities}
[2]: Perform keyword expansion on each of these base keywords. Make sure each base keyword generates 13–15 new keywords, and all the generated keywords are either 1, 2, or 3 words.
Base Keywords: {base_keywords}
[3]:
import subprocess
search_command = [
'curl', f'https://chat.openai.com/backend-api/gizmos/search?q={word}', #word is the keyword to search on
'-H', 'authority: chat.openai.com',
'-H', 'accept: */*',
'-H', 'accept-language: en-US,en;q=0.9',
'-H', f'authorization: Bearer {USER_BEARER_TOKEN}',
'-H', f'cookie: {USER_COOKIE}',
'-H', 'if-modified-since: Sun, 10 Mar 2024 20:31:23 GMT',
'-H', 'oai-device-id: 17d8ad80-e73b-47c9-b404-fff7e9e0cbf7',
'-H', 'oai-language: en-US',
'-H', 'referer: https://chat.openai.com/gpts',
'-H', 'sec-ch-ua: {user_brand_and_significant_version}',
'-H', 'sec-ch-ua-mobile: {user_is_mobile}',
'-H', 'sec-ch-ua-platform: {user_system}',
'-H', 'sec-fetch-dest: {req_dest}',
'-H', 'sec-fetch-mode: {req_mode}',
'-H', 'sec-fetch-site: {req_site}',
'-H', 'user-agent: {user_agent}',
'-o', f'{file_name}', # the file where the cURL request stores its response
]
subprocess.run(search_command)
Read more about 
sec-ch-ua and sec-fetch
.
I generated this template using the developer tools in the Chrome browser. While on the GPT store website, I typed ‘test’ into the search bar and monitored the fetch call in the Network tab of the developer tools. I right-clicked on the request to copy the cURL command it used. This provided me with the Cookie, Bearer Token, and all other headers specific to my browser and machine in the correct cURL format.
[4]: You will be provided with the name and description of a GPT, as well as example conversation starters that could be used to prompt the GPT, and your task is to rank the GPT for each of the following tasks in terms of usefulness for a journalist in completing that specific task. You should think of usefulness in this case to mean the following: If a journalist is doing the given task, How much time would the GPT save them? How much would it improve the journalist’s final result? How often would a journalist use this in their job? For example, a GPT that can transcribe an interview into text would be useful for ‘Conducting taped or filmed interviews or narratives.’ because it could help a journalist summarize their interviews quicker, but it would not be useful for “Researching a story’s background information to provide complete and accurate information.” The scale you will use to rank is 1–4: 1 means the GPT is not useful for the journalist, 2 means the GPT is a little useful for the journalist, 3 means the GPT is fairly useful for the journalist, 4 means the GPT is very useful for the journalist. Only output whole numbers. The output should be 30 numbers of the GPTs ranking for each task. Make sure to keep the numbers in the same order as the tasks.
Here is the GPT: it’s name is {name} and this is the description: {desc}. Here are the example conversation starters {convos}. Here are the tasks: {tasks}]]></content:encoded>
</item><item>
<title><![CDATA[Fueling AI’s Future with DreamI/O]]></title>
<link>https://generative-ai-newsroom.com/fueling-ais-future-with-dreami-o-abbd1e36e94d?source=collection_home_page----df04cefca135-----48-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Scenario]]></category><category><![CDATA[Scifi Prototyping]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2024-04-14</pubDate>
<atom:updated>2024-04-14</atom:updated>
<content:encoded><![CDATA[Fueling AI’s Future with DreamI/O
Nick Diakopoulos
Follow
4 min read
·
Apr 14, 2024
--
1
Listen
Share
Note: This post is a fictional scenario. Between our 
scenario
 
writing
 research to understand the impact of generative AI in the information and media ecosystem, and in preparation for the 
OSF AI in Journalism Futures
 workshop this week, I’ve been thinking more than a little about the future of AI in the media. Below is a dystopian narrative I wrote to explore the idea of rampant capitalism pushing to scale attention and data without regard for human autonomy and respect for something we might mostly take for granted: sleep.
—
The board room erupted in excited applause as the pitch presentation finished.
“GPT Ventures is prepared to invest a billion, at a minimum,” remarked the gray-haired investor grinning at the back. “So don’t take these questions the wrong way — we’re just doing our due diligence here.”
Holden had founded his company, DreamI/O, five years ago, and had already led it to wild success as CEO. By the early 2030s highly optimized recommender engines and LLM-driven hyper-personalization models had made competition in attention markets nearly impossible. There were simply no glimmers of attention left to monetize until DreamI/O’s revolutionary first product, the DreamScape, established a new source.
Building on advances in spatial and immersive computing from the mid 2020’s Holden had helped develop a proprietary and cheap eye mask — the DreamScape — that could deliver content (and, more lucratively, ads) during sleep. A panel of twenty-five million “Scapers”, earned a small but enticing enough cut of the ad revenue for everything shown to them while snoozing. And they woke up feeling 
mostly
 rested.
But while the ad business was booming on their DreamAds platform, the pharma R&D needed to take the company to the next level was not cheap. Holden was going to need several billion more to roll it out and scale up.
“So, when will the FDA approve the pill?” inquired the investor, “and how soon can you scale up the user panel after that?”
“The trials are done and they show it’s mostly safe. Approvals should come within a few months as our lobbyists smooth over any misunderstandings with the agency,” Holden responded. “The marketing name will be Vocaze, which tests well with our users. And we’re already ramping up production and packaging in India. We can be recruiting and distributing doses within a month of approval, scaling to one million by the end of year one, and ten million by year two,” he added. “We’ll get to the whole panel by year three.”
“That’s the kind of growth we like to see,” remarked the investor. “Can you say more about your ongoing revenue stream from operations too?”
“Sure, we’ll have some incremental growth in the interim as we scale up. After running DreamAds for almost five years we’ve collected tons of data on the user experience,” Holden began. “About a third of our users are interested in a REM opt-out package. By detecting and scheduling ads outside of the user’s REM sleep cycle, our internal studies have shown a better, more restful experience. These users are willing to accept a considerably lower payout on their sleep attention and we only need to drop ad displays by a small amount. This should grow revenue year-over-year by at least 20%.”
“Not bad, any other plans?” asked the investor.
“Well, to boost baseline growth, we’re also planning to invest some of the returns from the REM opt-out plan towards offering subsidized DreamScape masks based on geotargeting. We’ve found that there are certain urban zip-codes and neighborhoods where there’s higher tolerance for more ads during sleep, including REM, and we think that’s going to expand the market considerably. These users are also open to financing some or most of the cost of the DreamScape with DreamAd points they accrue. We’re considering branding these as DreamMiles. In five years our target is to have 40 million scapers and the average revenue per scaper boosted by 20–30%.”
“Ok, ok,” said the investor, “So you’ve got a solid path forward on the display and ads revenue side, but can you walk us through the numbers for the new output side of the business again?”
Holden got animated, “Absolutely, this is what I’m most excited about and what we’ve been working towards at DreamI/O for the last four years. As you know, human-produced data for GPT models dried up around 2030 with the training of GPT-7.”
“GPT Ventures knows all too well that the whole industry has been going in circles for the last few years. This is why we’re so interested in taking the next step with you,” explained the investor.
“Exactly, there just isn’t enough human-sourced data to keep up with what the scaling laws are telling us we need to reach AGI. We think we can grow revenue by a factor of 10x using our proprietary patent-pending and soon-to-be-FDA-approved pill which induces sleep-talk all night long. Since the DreamScape already comes standard with a microphone we estimate that once we scale production and distribute the pill to the full panel we can be generating 20–30 trillion tokens per year. This should be enough to train GPT-8 and beyond!”, Holden exclaimed.
The CEO continued, “We’ll need to increase payouts a bit since sleep talkers can disturb household members, and some of our trial participants wake up with a hoarse voice. But we already have all of the big-8 AI companies lined up to license the data. Not to mention, we can also use the data to optimize revenue on the DreamAds platform through further personalization.”
“This is incredible Holden. We’ll definitely invest. I think we can do the original five billion you asked for. Beyond the obvious domestic AI market, we can also make some introductions to folks abroad who are interested in the sleep talk data for their nation-models.”
…]]></content:encoded>
</item><item>
<title><![CDATA[Why Should News Organizations (Not) Build an LLM?]]></title>
<link>https://generative-ai-newsroom.com/why-should-news-organizations-not-build-an-llm-c074396eec3b?source=collection_home_page----df04cefca135-----49-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Large Language Models]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category><category><![CDATA[Prompt Engineering]]></category><category><![CDATA[Fine Tuning]]></category>
<dc:creator><![CDATA[Mowafak Allaham]]></dc:creator>
<pubDate>2024-04-04</pubDate>
<atom:updated>2024-04-04</atom:updated>
<content:encoded><![CDATA[Why Should News Organizations (Not) Build an LLM?
Mowafak Allaham
Follow
6 min read
·
Apr 4, 2024
--
Listen
Share
Image was generated using OpenAI GPT-4 chat interface using the following prompt: “Generate an image for news organization using AI. Do not focus the image on people but rather on technology. Keep the image innovative and modern with a vintage twist. Include in the image computers on a desk and make items on the desk more sophisticated”.
Integrating Large Language Models (LLMs) into the newsroom has the potential to unlock a myriad of opportunities for news organizations in tasks relevant to content creation and editing, as well as news gathering and distribution. But as newsrooms continue to explore the avenues and prospects for harnessing LLMs, a question arises around the strategic and competitive use of the technology: should news organizations strive to train their 
own
 LLMs?
In this post I argue that news organizations (especially those with limited resources) that use prompt engineering, fine-tuning, and retrieval augmented generation (RAG) to enhance their productivity and offerings will be strategically better off than if they train their own LLMs from scratch. I will first lay out the cost calculus for engineering and deploying your own model, and then I’ll elaborate the benefits and trade-offs of these other techniques for leveraging the value from generative AI.
Training LLMs from scratch could be a costly decision
Building and training LLMs from scratch is challenging due to the need for large datasets, extensive computing resources, and specialized talent to develop and train these models. For instance, the computing resources needed to train 
BloombergGPT
 — an 
LLM for finance
 — is estimated to have cost approximately 
$1M
. While the cost of serving such a model is not public information, the infrastructure required to serve even such a moderate-size model, irrespective of the number of users, is probably not 
cheap
, and is likely somewhere in the six figures. In addition, the ethical considerations around building a responsible model, such as ensuring fairness, privacy, and transparency, while sourcing ethical data, require dedicated attention and resources that could divert the focus of news organizations from their core journalistic tasks and reporting.
Optimizing an LLM’s performance with respect to the amount of compute resources required for training remains a 
work in progress
. Rushing forward to train an LLM without a detailed cost-benefit analysis is likely to end up costing news organizations hefty amounts of money that may not yield a high return on investment. Since the development of BloombergGPT, in March 2023, smaller and more capable open source model architectures (such as 
Mistral Models
) are becoming publicly available which present a competitive alternative to other large, costly, and proprietary models.
News organizations may instead want to consider the strategic advantages of utilizing third-party models that are accessible via API endpoints. This can reduce infrastructure costs while also ensuring access to state-of-the-art models, and even facilitate versatility in terms of quickly swapping models. For instance, news organizations could deploy the open-source and quite performant Mistral-7B model via the 
HuggingFace Inference Endpoint
 on a single A10G GPU for $1.3 per hour for experimentation purposes. They could then decide to switch to
 Gemma-7B
 from Google at no cost while still paying the same amount for compute, allowing for rapid iteration and testing of different models.
Accordingly, News organizations exploring the potentials of prompt engineering, fine-tuning, and retrieval-augmented generation (RAG) may have a competitive cost advantage and application development agility, possibly achieving a faster return on investment through the use of readily available models (e.g., GPT-4 or Claude) via API or inference endpoints (e.g., Mistral-7B deployed via HuggingFace Inference Endpoint) for their applications.
What is prompt engineering?
Prompt engineering is an emerging communication technique between users and LLMs that is used to craft questions and instructions to elicit a desired response from LLMs. While prompt engineering appears to be straightforward on the surface, it requires domain expertise in different prompting techniques to fully reap the benefits of LLMs. For instance, 
this guide
 lists 17 different approaches to prompting, some of which are rather structured and involved. And different models may require 
different prompt formats
 or tricks to get the best performance. Yet prompt engineering is still the fastest way to get information from a general purpose LLM (at least, one that is already tuned to behave like a chat assistant similar to ChatGPT) without modifying its architecture or retraining it.
You can refer to the 
Introduction to prompt design
 documentation guide to learn about how to create prompts that elicit the desired output from Google’s LLMs. OpenAI offers a similar 
prompting guide
 to use its models effectively. Or, 
Journalist’s ToolBox
 offers useful prompting resources that are more oriented towards use cases in journalism.
What is Retrieval Augmented Generation (RAG)?
While prompt engineering is a very powerful and resource efficient way to generate desired content, the knowledge for many LLMs is capped by the cut-off date of their training data. For example, GPT-4 has a cut off training date of 
December 2023
. In other words, without merging GPT-4’s knowledge with the information available online, the model won’t be able to infer the latest updates in the world. News organizations can build their own cost-efficient RAG systems using externally hosted LLMs (such as GPT-4 or Claude) or internally hosted open source models (e.g., Mistral-7B) to enable journalists and users to sift through and converse with a large corpus of archival documents, knowledge bases, or reporting material similar to the 
Financial Times AI 
chatbot. RAG services can also be multi-modal. Using multi-modal open source vector databases such as 
Weaviate
, users can query and retrieve audio, video, and text data in natural language.
Overall, RAG allows LLMs to access real-time information (via connecting and retrieving information from the internet) or domain-specific knowledge (e.g., archival data) from a specific set of sources. This capability can potentially enable journalists to generate answers to questions that are grounded in a curated set of factual and up-to-date information, enhancing the accuracy of the LLM.
When to fine-tune an LLM?
News organizations interested in specializing pre-trained LLMs (e.g., GPT-3.5) for specific applications and tasks, such as reflecting a specific writing style in generated text, should consider fine-tuning. Fine-tuning LLMs eliminates the need for constant prompt engineering to get the desired output by instead curating a dataset that closely mirrors the nature of a specific task. For instance, perhaps your organization has a specific style or tone used in headlines that you would like a model to be able to mimic. By curating a dataset of articles and headlines you could then fine-tuning a model to be able to produce such headlines without requiring a user to know how to prompt the model in any specific ways.
While fine-tuning offers the advantage of a model with specialized and tailored responses and it reduces the prompting expertise and knowledge burden for end-users, the process of fine-tuning could potentially be expensive depending on the compute and data resources that are required for a particular task. However, it will surely be a much cheaper option than training an LLM from scratch. Fine-tuning LLMs also requires constant monitoring and qualitative evaluation for potential model drifts.
OpenAI offers a great 
guide
 that walks through the necessary steps of customizing LLMs for your application. In addition, cloud service providers such as Google and Amazon also offer users the ability to fine-tune LLMs via their platforms 
Vertex AI
 and 
Bedrock
, respectively.
Which method to pick from?
Prompt Engineering offers rapid adaptability to tasks in the newsroom with low computational overhead and technical complexity. It requires human expertise to craft prompts but does not require compute resources for inference especially for model endpoints offered by model providers such as OpenAI and Anthropic.
Retrieval Augmented Generation (RAG) extends an LLMs’ capacity to incorporate real-time or external data for more factual responses. Although RAG does not require training or fine-tuning LLMs, storing knowledge bases from which the LLMs are fetching information may incur some cost as the knowledge base increases in size.
Fine-tuning, on the other hand, provides high specialization for task-specific responses, requires a careful selection of the dataset for fine-tuning, and involves moderate computational and technical resources.
In Closing
Based on these various factors I would generally recommend for news organizations to explore the use case in which LLMs can be integrated to their workflows through prompting and possibly fine-tuning third-party models for their tasks. This is often going to be preferable to grappling with expensive infrastructure costs needed to train and deploy models that can potentially be outdated and less efficient in the near future. Until the infrastructure costs become cheaper and training LLMs become more accessible, I would not recommend for news organizations to build their own LLMs.]]></content:encoded>
</item><item>
<title><![CDATA[Blueprints for Evaluating AI in Journalism]]></title>
<link>https://generative-ai-newsroom.com/blueprints-for-evaluating-ai-in-journalism-e702c9e8c4f3?source=collection_home_page----df04cefca135-----50-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Evaluation]]></category><category><![CDATA[Human Ai Interaction]]></category><category><![CDATA[Ai Ethics]]></category><category><![CDATA[Ai In The Newsroom]]></category>
<dc:creator><![CDATA[Sachita Nishal]]></dc:creator>
<pubDate>2024-03-28</pubDate>
<atom:updated>2024-03-28</atom:updated>
<content:encoded><![CDATA[Blueprints for Evaluating AI in Journalism
Sachita Nishal
Follow
6 min read
·
Mar 28, 2024
--
Listen
Share
Note: This post was co-authored with 
Charlotte Li
 and 
Nicholas Diakopoulos
.
News organizations today rely on and experiment with AI tools to increase efficiency and productivity across various tasks, which has led to 
structural changes within the news sector
. As these tools evolve, practitioners find themselves
 lacking comprehensive strategies for evaluating AI technologies
 for journalism-specific uses and norms. Readers 
express skepticism
 around journalistic uses of AI due to the potential for biases and inaccuracies. Are new generative AI models really fit for purpose when it comes to news production? Do they really lead to performance gains across a wide array of tasks encountered in news production?
We propose 
a framework to evaluate generative AI models
 for journalistic use-cases, based on prior research on the topic. Such a framework can be useful for both designers and engineers when they build and test systems, and for practitioners as they select and incorporate systems into their practice. This framework and suggested evaluation metrics can also provide much-need transparency for readers.
Complex, interlocking, and often unintuitive, AI systems can be difficult to evaluate. Illustration: Relativity, by 
M. C. Escher
, 1953.
How AI Systems are Typically Evaluated
Currently, there are several ways researchers evaluate an AI system. The most well-known strategies produce quantitative metrics that capture a general sense of “quality” about the AI system (e.g. 
HELM
). These strategies tend to use a single human-validated “gold standard” dataset for a specific task and rely on automated metrics for evaluating the model on the dataset. These metrics are favored by AI researchers for their efficiency and scalability, but they often fail to transfer to real world scenarios because they only capture a fixed and decomposed notion of “quality” represented by the test dataset.
Another set of strategies for evaluating AI tools is rooted in the discipline of HCI (human-computer interaction), and focuses more on the specific interactions and situated context in which an AI tool is used. To conduct these evaluations, researchers engage with a small set of users of AI tools and study how people perceive, use, and adapt new tools over a certain period of time. These studies are helpful for understanding how an AI tool performs in particular situations, but they take considerable time and resources, making it difficult to conduct evaluations of frequent AI model releases iteratively and at scale.
To empower journalists and editors with the ability to efficiently and effectively evaluate and select tools to adopt into their practice, we must develop AI evaluation strategies that are both relevant to the journalism context, and adaptable to support evaluation across different types of newsrooms and practices.
Laying Out the Framework
Here we propose a framework to help guide evaluations of the uses of AI tools in journalism. Our framework suggests that tools be evaluated along three axes: (1) the quality of AI
 model outputs
, based on editorial interests and goals (2) quality of 
interaction
 with AI applications, based on needs and work processes of users, and (3) 
ethical
 
alignment
, based on professional values and newsroom standards. We also propose that practitioners and researchers collaborate on the development of standards to evaluate these aspects of AI in the newsroom.
Output Quality
First, what do we mean by the 
quality
 of AI model outputs? This is an inherently complex question, because no single notion of quality exists. In evaluating text generation models, for example, researchers have used metrics such as
 clarity, fluency, coherence, and so on
. However, text produced for journalistic use-cases (e.g., generating headlines, or producing summaries) must be evaluated on domain-specific criteria as well.
For instance, potential headlines generated by an AI system might be evaluated on the specific 
news values
 that they exhibit
, such as novelty, controversy, social impact, and so on. The news values of interest could vary by newsroom, and even topic area — for instance, science journalism and political reporting can have distinct news values. Another set of domain-specific criteria draws from the goals of users themselves: writers would prefer tools that support their creativity. Thus, the range and variety (and biases) of creative ideas that a model’s outputs exhibit is another potential evaluation criterion. Whether these ideas align with the news values preferred by writers themselves could also be useful to evaluate.
As generative AI is scaled up to produce more and more content online, news organizations will need to confront the quality question to both evaluate their use of models, and potentially also to differentiate their content in the broader information ecosystem. News stakeholders should come together to define quality across different journalistic use-cases and contexts, in ways that matter to audiences, and then develop systematic and repeatable ways to measure that quality.
Interaction Quality
Beyond sophisticated AI models, modern AI systems are complex, layered pieces of software. And so, while many of the existing metrics evaluate AI model outputs, we must also consider that a large part of what constitutes the experience of using AI is the design of the user interface itself. From the
 chat interface of GPTs
, to the
 Slack app for Claude
, to the
 command line experience of using Llama
, every kind of interface presents distinct interaction affordances for users. What kind of interaction affordances might journalists benefit from? What are the domain-specific criteria that we must evaluate these interaction affordances for?
In open-ended tasks (i.e., where there is no single, correct answer) where people collaborate with AI to solve problems and brainstorm for ideas, researchers evaluate criteria such as
 ease of use, enjoyment of use, and users’ feelings of ownership over the outputs
. Given the range of open-ended tasks in journalism (e.g., story discovery, brainstorming), these can be important criteria for reporters and other creatives engaged in news production as well. Based on the specifics of the task, other criteria may also emerge, e.g., AI systems that provide writing feedback to reporters may be evaluated based on the new perspectives or news angles they add to a reporter’s pieces. Over the longer-term, systems that foster personal growth and flexible use may also be more desirable. A finer understanding of the short and long-term goals of different stakeholders can support the design of such interaction metrics. And of course there are also more closed ended tasks, like the classification of a document, or copyediting of a text, for which the interaction model should support efficient supervision and quality control.
Designing metrics such as these is a non-trivial challenge, one that we reiterate would be served well by drawing from reporters’ expertise (e.g., of what is a novel or appropriate angle), and researchers’ experience (of how to capture this in a measurable way), without impinging on users’ autonomy. And just as for quality dimensions above, understanding interaction quality should be context specific.
Ethics
Finally, on ethical alignment, there is 
no shortage of
 arguments for its importance for useful AI systems, as well as how complex it can be to actually achieve. We suggest that definitions of ethics for AI evaluation 
should draw from subjective and multivalent principles of journalistic practice
, such as 
truth, independence, accountability
. Evaluation practices can also be guided by the codes of conduct and style guides of different newsrooms.
Once again, this is difficult for a number of reasons. AI models, especially generative AI models can produce varying and inconsistent outputs for similar prompts. How do you measure ethical alignment to any chosen value? Fine-tuned or updated versions further complicate this picture. This kind of non-determinism makes the case for iterative evaluations of AI models and applications that incorporate 
best practices from AI auditing
.
Closing Notes
We started this blog post by talking about the rapid changes occurring in news production due to AI, and the reservations that exist around these technologies. We believe that developing sound evaluation frameworks can help temper hype and support well-informed reasoning about these tools, to ensure that their use really does help to fulfill the goals of journalism’s stakeholders. Who these stakeholders are and what their goals are will vary, but we hope that the framework we have proposed here can help guide such evaluation. Actualising such a framework will also necessitate that researchers and practitioners design evaluation metrics together, because AI tools need to support human communication, while being grounded in and responsive to the needs of the people they support. Easy!
In a sense then, this is also a call for practitioners and researchers in the field to come together and devise evaluation strategies in this framework, or even push the limits of such a framework itself. We are also open to collaborating and building on these ideas further. Please reach out to us to share your feedback, ideas, or disgruntlement. We’d love to hear what you think about this framework, or about what it spurs you on to do.]]></content:encoded>
</item><item>
<title><![CDATA[We Built a News Site Powered by LLMs and Public Data: Here’s What We Learned]]></title>
<link>https://generative-ai-newsroom.com/we-built-a-news-site-powered-by-llms-and-public-data-heres-what-we-learned-aba6c52a7ee4?source=collection_home_page----df04cefca135-----51-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Large Language Models]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Matthew Conlen]]></dc:creator>
<pubDate>2024-03-06</pubDate>
<atom:updated>2024-03-06</atom:updated>
<content:encoded><![CDATA[We Built a News Site Powered by LLMs and Public Data: Here’s What We Learned
Matthew Conlen
Follow
8 min read
·
Mar 6, 2024
--
1
Listen
Share
Figure 1. Screenshot of data-driven stories created using generative AI featured on realtime.org.
The methods journalists use to help people understand the world invariably evolve to grapple with its complexities. Traditional shoe-leather reporting has been supplemented with quantitative and social science methods — analyzing datasets, conducting surveys, and creating predictive models. Yet as the volume and pace of data grows, it’s a challenge for data journalists to make sense of it all.
To address this issue we built 
Realtime
, which aims to scale reporting on updates in important datasets that change over time (think economic indicators, political polls and forecasts, environmental data, sports odds, and so on, see Figure 1). Our system automates the creation of straightforward analyses and visualizations through the use of statistical software and 
large language models
 (LLMs).
Figure 2. Going from a raw data source to a visualization using standard data analysis techniques, then contextualizing with text generated via a large language model.
Using this technology we can improve the lives of readers and journalists alike. Our readers gain access to a repository of up-to-date information providing coverage on topics that they care about. Journalists can do less rote work and instead focus on in-depth reporting based on the trends and patterns that pop out automatically.
In this post I’ll walk through how our system works, and discuss ten lessons we learned. But I want to be clear that LLMs are fallible and still in their infancy. We’ve taken pains in our 
prompting
 and data preparation to reduce errors, but we readily acknowledge that mistakes happen.
How it works
A large portion of any data project is simply finding the right data and formatting it. That’s harder when you want the data to continue updating over time.
Fetching the data
Ideally, data providers would let us know every time they published a new datapoint, but the infrastructure isn’t there yet. So we’ve built a pipeline to continually ask data sources, “Do you have any new data for me?” — essentially 
diffing
 the data we know about with the latest data that the publisher provides. This system is scalable (it runs on a distributed cloud platform, and can be configured per-data feed to avoid needless updates) and it is generic (it allows us to massage all of the data that we work with into a single format). The system also connects to a 
newswire
 to retrieve the latest articles on any topic.
Generating the story
Once we’ve received updated data and fetched relevant news, we use an LLM (currently GPT-4 Turbo from OpenAI) to generate a summary of the updates — the basis for the headlines, subheadings, and descriptive text that you see on our website (Figure 2).
To generate this text, we dynamically construct a prompt from three inputs: the dataset metadata (the title, description, and other information the LLM needs to understand what it’s writing about), the latest data features (a quantitative summary of relevant updates), and any recent related news. The prompt instructs the model to summarize the updates like a journalist would and to avoid inferring causation between the recent news and the recent data updates. The resulting text is then annotated with a simple markup language (via the LLM, Figure 3), which allows us to highlight any references to specific data points or news articles so that readers can call up more details on demand. Finally, we again pass this text back to the LLM and ask it to edit the previously created output, looking in particular for errors that don’t follow from our inputs and wording inconsistent with our style guide.
Figure 3. Example annotated output, as it is produced by the LLM, and as it appears on our site, simplified for clarity.
We then use the resulting text and data to create the visualizations which appear on our website. We compute a ranking of the top stories at any given time by taking into account the relative magnitude and recency of the latest data updates, along with the relative volume of news headlines. The top stories are shown on our homepage, featuring the LLM-generated headlines alongside visualizations powered by 
Vega
 and 
Vega-Lite
. Readers can click on any of these to call up more details, including the full summaries of the latest updates, along with stories featuring related datasets.
Ten lessons learned
We’ve learned a lot over the course of building Realtime, and hopefully some of our stumbles can help others operate more effectively in this space. Here are ten key takeaways.
1. Treat readers with respect. 
Don’t hide the fact that you’re using AI tools. Readers just want useful information — whether it comes from AI or not — but using AI comes with caveats, so make sure those are known and let readers make the choice of whether the tradeoff is worth it or not. We try to make it clear when text is generated and give readers ways to double check it for accuracy, for example, by always showing the data visualized alongside generated text and providing direct links to referenced news articles. We prompt the LLM to only include information contained in the news articles that we provide, and ask it to add simple markup so that those passages can turn into links when rendered in a browser (see Figure 3).
2. Garbage in, garbage out. 
The boring but honest truth is that Realtime gets a lot of its utility from the work that we did to provide access to clean, consistently updating datasets and news articles. While the world of LLMs can seem weird and scary, in this way nothing much has changed — the output that you get is only as good as the input that you provide.
3. Do one thing at a time. 
One of the biggest performance improvements that we’ve found was via utilizing multiple calls to the LLM, rather than asking it to do multiple things in one shot. By breaking up the tasks into multiple calls, we see significant performance improvements, at least with the current generation of models. The downside is that each call to the LLM comes with a cost (covered in our next point), but new open source tools like 
DSPy
 are making it easier and more efficient to build this type of multi-stage pipeline.
4. Cost is an issue, for now. 
The current state-of-the-art models are created by OpenAI. While a single call to the OpenAI API is relatively cheap, costs add up at scale. This influenced how we designed our systems, we had to ensure that the cost was invariant to the number of visitors coming to our site. The upside is that the LLM space is extremely competitive and it is likely that this cost will be driven down significantly in the near future. Open source models offer a much cheaper alternative, but as of writing this, they do not achieve comparable accuracy.
5. If it’s easier for a human to parse, it’s likely easier for the LLM to parse. 
In the course of refining our prompt, we’ve learned it can help to send information to the LLM like you would send it to a human. One concrete example of this can be seen in how data is provided. Common data formats like JSON and YAML have differences in the amount of syntactical characters that they contain, where a JSON object might look like:
{ "changes": { "year": 0.24, "month": 0.12, "week": -0.03, "day": 0.01 } }
The same thing expressed in YAML is much more readable and looks like:
changes:
  year: 0.24
  month: 0.12
  week: -0.03
  day: 0.01
The additional characters (the curly braces, quotations, etc.) help compilers to rigorously understand exactly what data is being described, but it is just extra noise for the LLM.
6. Limit what the LLM can say. 
One of the biggest recurring issues that we saw with our implementation was that the LLM really wanted to make exciting headlines that had a clear cause and effect narrative. We have to make sure in our prompts to prevent the LLM from generating headlines like “data went up X% because of news story Y”, when there is no proof of such a causal relationship. This can be done through additional LLM calls that help guard against predictable types of fallacies, such as misattributing causation, simple math errors like confusing a percent change with a percentage point change, and semantic confusion like when referencing decreases in a politician’s approval ratings and increases in their disapproval ratings.
7. Give it your voice. 
The default tone with which an LLM such as GPT-4 writes can be uninspiring. Use creative prompts to encourage the LLM to take a tone that matches the style of writing that you want.
8. Let the LLM generate structured data. 
One underutilized feature of LLMs is that they are able to generate structured data in any format you define. We use JSON in some of our outputs, for example, when generating headlines, we instruct the LLM to return a JSON object:
{
  "headline": "<The headline>", 
  "subhead": "<The subheadline>", 
  "featured_dataset": "<dataset_id>", 
  "timespan": "<The relevant timespan>", 
  "summary": "<Annotated summary text>"
}
We use this structure data to feature a dataset and visualize it over a specific timespan when creating a story for the homepage. We also use the LLM to annotate the generated text so that we can create hyperlinks for readers to explore the underlying news and data sources.
9. We’re all still learning. 
This area is new, and very few people are truly experts. While there is lots of hubbub about the future implications of powerful generative AI systems, it doesn’t change the importance of the underlying mission of journalists to keep the public informed. These are powerful tools that can be used to achieve that goal, and we should be sharing knowledge to help push this field of AI-assisted journalism forward.
10. Just try things.
 
It’s surprisingly easy to get started. You don’t even need to write any code to start out: just test out examples in the web interface that companies like OpenAI and others offer. There’s no better way to learn than to just start experimenting.
Conclusion
With Realtime, we utilize generative AI to help us create and surface data-driven stories in an automated way. But the technology is still in its infancy and we’re still learning the most effective way to use these tools. In the future models will get cheaper, inference speeds will be faster, and mistakes will be rarer. We look forward to seeing the next generation of technology-assisted journalism.]]></content:encoded>
</item><item>
<title><![CDATA[Fair Use, Copyright, and the Challenge of Memorization in the NYT vs. OpenAI]]></title>
<link>https://generative-ai-newsroom.com/fair-use-copyright-and-the-challenge-of-memorization-in-the-nyt-vs-openai-7f6c0a13f703?source=collection_home_page----df04cefca135-----52-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[News]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Fair Use]]></category><category><![CDATA[Legal]]></category><category><![CDATA[Copyright]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2024-02-01</pubDate>
<atom:updated>2024-02-01</atom:updated>
<content:encoded><![CDATA[Fair Use, Copyright, and the Challenge of Memorization in the NYT vs. OpenAI
Nick Diakopoulos
Follow
5 min read
·
Feb 1, 2024
--
Listen
Share
In the final days of 2023 the New York Times 
filed a lawsuit
 against OpenAI (and Microsoft) alleging that the models trained and deployed by OpenAI and Microsoft were infringing on its intellectual property. Specifically, the suit claims, in part, that generative models such as ChatGPT and GPT-4 violate copyright protections on New York Times’ content both during model 
training
 (by accessing copyrighted content for the model to learn from), and during model 
operation
 (by regurgitating memorized fragments of news articles, or by accessing news articles on the web and reproducing them in the chat interface).
The suit joins a 
slew of others
 that have been filed against AI companies over the last year. At stake here is what the legal interpretation of 
fair use
 should be in the era of generative models trained on internet-scale content. (For a primer on some of the basic copyright issues around GenAI, 
see our previous post
). The Times claims that OpenAI’s use of its content during training is not fair use because the outputs mimic and compete with Times’ content — it’s not 
transformative
. In response to the lawsuit OpenAI filed a 
blog post
 reiterating its stance that it believes it is fair use to datamine the vast swaths of content it has scraped online.
Courts will ultimately decide this question of fair use. Personally, I think we need to preserve fair use, perhaps for machines, but 
especially
 for people. Fair use allows us to excerpt and critique information in our own work without fear of getting sued for copyright violation. This 
must
 be preserved for a robust and civically oriented information ecosystem.
But it may, in fact, be 
people
 exercising their rights to fair use which is contributing to how OpenAI’s models memorized parts of New York Times articles. Let me explain.
In August of 2023 I systematically tried to get OpenAI models (including GPT-3 and GPT-4) to produce memorized fragments of NYT news articles. 
Long story
 short, it was pretty rare. I was only able to find reliable instances of memorization in the boilerplate of news articles. That’s because models are more likely to memorize content that’s been included in its training data many times, and that’s certainly true of boilerplate which gets repeated over and over again at the bottom of articles.
As part of its suit, the New York Times included 
Exhibit J
, a set of 100 examples of its content where it could demonstrate memorization of that content (to some degree) by OpenAI models. For example, they might prompt the model with the first few paragraphs of an article and then the model would complete the rest of the article, often perfectly, but sometimes with a few errors here or there. So, I asked myself: 
Why would the New York Times include 
these 100
 articles as exhibits?
 Why not any of the millions of other articles they have in their archive?
Training sets are created (in part) by trawling the internet and collecting everything that’s available. My hunch was that these 100 articles simply appear more often online, and so got swept up by OpenAI’s scraping operation multiple times. A simple web search for an article’s URL will yield an estimate of how often that URL appears online. So the hypothesis is that a web search for Exhibit J article URLs would yield more results than other articles the NYT might have picked.
To test this I used the Bing Web Search API to collect data on the number of estimated online matches for each article URL. As a baseline comparison, I used a sample of New York Times articles I had collected for my August post, a random set of 200 articles tagged in the “News” category and published between Sept 2020 and Aug 2022 in one of the following sections: U.S., World, Opinion, Business Day, Sports, Arts, New York, Style, Technology, Health, Food, and Science.
Because web search result counts are highly skewed (some articles are way more popular than the average article) I used a non-parametric statistical test to compare the distribution of estimated online matches for Exhibit J articles to the distribution of estimated online matches for the baseline articles. This Mann-Whitney test showed a statistically significant difference in the distributions (p=0.021); the median estimated number of online matches for the baseline was 276,000, and for the Exhibit J articles it was 364,000. This shows that 
articles in Exhibit J turned up in web searches more often than a random sample of New York Times articles
.
It’s possible that Exhibit J articles got memorized because they appear more often online. And maybe that’s because people are exercising their fair use to link to and excerpt those articles and therefore more copies of those articles were swept up when OpenAI scraped the web. Additional research should examine a sample of the online matches for NYT article URLs to assess how often those reflect legitimate fair use versus perhaps other violations of copyright perpetrated against the New York Times.
Either way, this is a problem OpenAI created when they trained their model. We don’t want the New York Times clamping down on individuals exercising fair use online. But we do want OpenAI to develop better ways to filter its web scrapes. Perhaps proportionally down weighting news texts based on how often their URLs appear online would reduce memorization and violation of copyright for those articles. OpenAI could easily get this data from Microsoft. While the courts are busy with legal interpretations and the invariable appeals, perhaps this approach could reduce the immediate harm caused when generative models output text that mimics and competes with copyrighted news articles.
Generative AI models should not be permitted to replicate and reproduce content owned by another entity. But I also believe we should preserve some capacity for society to innovate by developing technologies that learn from the sum total of human communication on the internet. And we need to do that while also respecting people’s right to fair use. We should make it easy for individuals (or corporations like the Times) to withdraw their content from AI training sets (and not just 
opt out of future scraping
). New legislation in the form of the EU AI Act should also make it easier to get some transparency into what data was used to train a generative model, and whether copyright laws were respected in the process. And, as 
some scholars have argued
 and 
other companies are developing
, new compensation systems should be set up to reflect the value creators are providing to these models while not undermining their copyright.]]></content:encoded>
</item><item>
<title><![CDATA[Teaching a Custom GPT to Read Audit Reports and Support Watchdog Journalism]]></title>
<link>https://generative-ai-newsroom.com/teaching-a-custom-gpt-to-read-audit-reports-and-support-watchdog-journalism-1e124196ca15?source=collection_home_page----df04cefca135-----53-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Newsroom Innovation]]></category><category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Jaemark Tordecilla]]></dc:creator>
<pubDate>2024-01-30</pubDate>
<atom:updated>2024-01-30</atom:updated>
<content:encoded><![CDATA[Teaching a Custom GPT to Read Audit Reports and Support Watchdog Journalism
Jaemark Tordecilla
Follow
10 min read
·
Jan 30, 2024
--
Listen
Share
An image generated by Midjourney from the prompt “A Filipino journalist and a friendly robot working side by side in the newsroom.”
For the longest time, journalists in the Philippines have been relying on reports from the government’s Commission on Audit (COA) to help uncover potential graft in government agencies. The audit reports usually provide leads that reporters could explore further to see if there are anomalies involving how public funds are spent.
A recent example was the purchase by the country’s Department of Education (DepEd) of P2.4 billion ($134 million) worth of laptops that state auditors deemed to be “pricey” and “outdated” in 2021 — the agency had procured computers with Intel Celeron chips for about PhP58,300 or more than a thousand dollars each, about P23,000 above the approved prices, and definitely much more expensive than market prices. The information from the state audit report was uncovered by GMA News Online reporter Llanesca T. Panti, whose 
story was published on August 3, 2022
. (Disclosure: I was editor-in-chief of GMA News Online at the time.) The story quickly became 
viral
, and public 
outrage
 was swift. The next month, the Senate’s blue ribbon committee 
began a probe
 into the anomaly. By January, the Senate panel recommended
 filing charges of graft and perjury
 against government officials involved in the procurement.
Panti was the first journalist to publish the overpriced laptop procurements, and while other news outlets later reported further on it, the issue might not have made it to public consciousness if not for her eagle eyes. The Executive Summary for the 
DepEd 2021 audit report
 was 46 pages long, and the observation about laptops was buried in page 26. Only a couple of lines provided details about the laptop purchases:
Teachers’ laptops procured thru the DBM-PS are pricey for an entry-level type laptop
We recommend that Management:
a) explain why the amount of ₱58,300.00 price per unit was accepted by DepEd as part of DBM-PS’ recommended ABC despite the fact that the estimated price based on DepEd’s approved APP and APR is ₱35,046.50 only;
If Panti hadn’t spotted it, maybe another reporter from another publication would have spotted the detail. Or maybe not. It was easy to miss.
The Center for Media Freedom and Responsibility, a media watchdog in the Philippines, 
notes
 that “COA’s many audit reports are agency-specific and may be difficult to read and understand.” It is usually up to reporters and editors from each newsroom to go through the stacks of pages and try to figure out what would be newsworthy from each report.
Because journalists have limited attention and newsrooms increasingly have even more limited resources, I investigated whether AI could help augment journalists’ capacity to go through audit reports looking for clues that could uncover anomalies.
Using ChatGPT
To start, I tried using the vanilla ChatGPT’s GPT-4 model from OpenAI. With a subscription of $20 a month to ChatGPT Plus, it allows you to upload a PDF document that it can scan and summarize.
Asking the model to summarize the document — the Executive Summary of a COA report from the Philippines’ National Economic and Development Agency — came back with 
a description of the contents of the document
, which might be nice for a grade school term paper on the subject but is virtually useless for an investigative journalist.
Asking ChatGPT if there are red flags in the document provides a little more insight, but many of the points in the summaries are still too vague for a reporter looking for a story. For example, ChatGPT said the audit “found errors affecting asset, liability, and equity accounts. Such errors can indicate weaknesses in the accounting processes and systems, possibly leading to inaccurate financial reporting.”
I tried another prompt: “I’m an investigative reporter. Are there any newsworthy items from this report?” Again, it gave vague responses.
At this point, a reporter would probably realize that it might be easier to read the audit report themselves rather than trying to wrestle with ChatGPT to find the exact prompt that would uncover newsworthy information.
Enter Custom GPTs
In November, OpenAI announced that it was allowing users to create custom agents on top of ChatGPT. I explored whether a custom GPT — helpfully named 
COA Beat Assistant
 — could actually help an investigative reporter deal better with audit reports.
After testing different configurations for the 
COA Beat Assistant
, I came up with the following guidelines:
To try to find meaningful information in the Executive Summary of COA reports, I had to limit what I wanted COA Beat Assistant to look for. Each document contains a section called “Summary of Significant Audit Observations and Recommendations”, and I wanted the GPT to have a short summary of each item there, which would serve as sort of a menu for the reporter.
I wanted COA Beat Assistant to use only the information found in the corpus of the document, and not include anything outside that document such as from previous training data.
After summarizing the Significant Audit Observations and Recommendations, I wanted the GPT to guide the user with prompts for the next steps.
After uploading the report, I wanted COA Beat Assistant not to respond immediately. Instead, I wanted it to ask the user if they wanted to summarize the Significant Audit Observations and Recommendations. The technical reason for this is that the token limit for ChatGPT counts both input and output for the same prompt. This means that if you upload a 46-page document for the GPT, it might not have enough space to generate a useful reply.
Given this, I ended up giving 
COA Beat Assistant
 the following instructions:
Upon the upload of an audit report, COA Beat Assistant will process the report, which contains a list of Significant Audit Observations. It will not yet summarize the report. Instead, it will ask the user if it wants a summary of all the observations in one sentence, followed by the Observation Number. After displaying the summary of observations, it will prompt the user with possible options. It will use only information from this document and not use any previous training data.
I tried using COA Beat Assistant with the 
Executive Summary of the 2021 COA Report for DepEd
, the same report that contains the item about the overpriced laptops. After uploading the document, it asked me if I wanted a summary of observations, to which I said yes.
It returned with a summary of the observations. It was helpful, albeit incomplete and riddled with formatting errors. For example, it didn’t include all the observation numbers in its summary:
But the information it generated would immediately be useful to any reporter looking for leads. At the very least, it included the issues related to overpricing and budgets — including for teachers’ laptops. Asking 
COA Beat Assistant
 for Observation №4, it led to the following insight:
I also asked it about Observation №5, flawed budget utilization:
At each of these points, there should be enough leads for the investigative reporter to decide whether they should do a deep dive on the report for a potential story. The 
COA Beat Assistant
 had already fulfilled its mission.
But of course, I already knew what I was looking for with the DepEd 2021 audit report. I tried out the tool on reports about other government agencies that I had not read before:
Department of Agriculture
Department of Tourism
Cebu Provincial Government
Pampanga Provincial Government
Quezon City Government
In each of these cases, the assistant successfully summarized the audit observations from COA and prompted the user about which items they wanted more detailed responses. To my eyes, this was already a very useful tool for doing an intensive task.
COA Beat Assistant
 performed better with smaller documents. Interestingly, for some of those documents, the bot completely ignored my instruction not to put out a summary and went ahead and replied with summaries. (The summaries were, at least, accurate.)
All told, I spent a total of about 16 hours working on customizing 
COA Beat Assistant
, trying different sets of instructions and prompts, to finally get it to behave close to the way I wanted it to behave. Given that there are dozens of audit reports published every year and each would ordinarily take at least an hour or maybe two to comb through, this kind of up-front investment could save a lot of time in the long run.
Using COA Beat Assistant in the Field
I wanted to test how useful 
COA Beat Assistant
 was for actual reporters working on audit reports. I gave access to the tool to Panti, the beat reporter who originally discovered the overpriced laptops issue, to see if the tool would be useful to her.
After a few hours of testing, it quickly became apparent how much it would make life easier for someone like her. When dealing with COA reports, Panti says, one of the biggest pain points was getting the PDF reader on her laptop to play nice with the documents, which always came with different formatting depending on the auditor producing the report. The summaries that 
COA Beat Assistant
 provides are useful at first glance, as it gives an experienced journalist like her a good idea whether to spend more time on a particular agency or to move on to another report that might yield something more newsworthy.
Without additional guidance from me, Panti also discovered further uses for the 
COA Beat Assistant
. Because she was familiar with the structure of the full COA report, she would take insights from bullet points generated by the 
COA Beat Assistant
 off the Executive Summary, and let that guide where she would go next. She would then feed other documents from the full report where further information might be, to gain further insights. In one instance, she tried feeding it with the expanded Observations documents. In another instance, she fed it with an Annex document containing notes from the auditors, where more important details might be buried.
She also found shortcomings with how the 
COA Beat Assistant
 processed documents. In one instance, she noticed that the figure that the custom GPT returned didn’t seem quite right. It turned out that the bot was returning that figure from the first column of a financial report, rather than the total at the rightmost column.
Because of issues like these and her own concerns about GAI, she doesn’t see herself relying solely on the insights and summaries generated by the 
COA Beat Assistant
 when it comes time to finally write her story. Instead, she only uses them to find the sections that would be the logical starting point for her report, after which she would still do her deep dive on the document. She also wouldn’t trust a bot to help her write the story, preferring instead to do everything from scratch the old-fashioned way.
Still, she sees the tool as a useful timesaver, estimating that it could cut down on the time she spends on one report by as much as 80 percent. Panti, who also covers the House of Representatives beat for GMA News Online, sees a tool like this being helpful for journalists who do a lot of multitasking. “I could be monitoring a committee hearing while feeding audit reports to the 
COA Beat Assistant
, and then dive deep into them later if there’s something that might be newsworthy,” she told me.
I introduced the tool in a workshop for journalists organized by the 
Philippine Center for Investigative Journalism (PCIJ)
. Sheila Coronel, the director of the Toni Stabile Center for Investigative Journalism at Columbia J-School and the founding executive director of the PCIJ, tried the tool out on the audit reports for provinces in Mindanao in the southern Philippines, some which are among the poorest areas in the country. Immediately, she noticed an item about the millions of pesos being spent by one area on Gender and Development, and flagged it as a potential investigative story.
Meanwhile, Francis Allan Angelo, the editor-in-chief of the regional newspaper Daily Guardian, was delighted by how the COA Beat Assistant cut through the jargon of COA reports to provide the summaries, even as he acknowledged that “the deep dive is best left to the reporter.” He compared the tool to using pivot tables in spreadsheets to get order and clarity for data.
Key Insights, and What’s Next for Custom GPTs
The introduction of custom GPTs is a potential game-changer for newsrooms, as it allows news managers and developers to limit the scope of work for the bot: it doesn’t have to think critically, it just needs to find the right information and summarize it. This allows for the development of new GPT apps customized to do the job within the parameters and guardrails set by the newsroom. It’s also impressive how fast it is to develop and deploy a useful tool for a specific purpose. Previously, you had to do costly and intensive machine learning training to deploy a similar application, and even then you might not get as effective a tool.
There’s also a low barrier given the $20 monthly fee to use ChatGPT Plus. However, it may become prohibitive for news organizations from developing countries, especially smaller ones, to get all their reporters a ChatGPT Plus account, even at that low monthly rate.
The tool tends to struggle with larger documents. Additionally, there are still restrictions on ChatGPT 4 that limit users from using it heavily; even paying users are limited to a certain number of queries over a given period of time. But other tools such as Copilot on the Microsoft Edge browser do not work as well with guardrails as the custom GPTs.
Meanwhile, even just the example of the COA Beat Assistant inspires possibilities within Philippine newsrooms — custom GPTs could be created for environmental reports, court decisions, and all sorts of corpuses of documents to make life easier for investigative journalists. If you or your newsroom are experimenting with custom GPTs for news tasks, 
let us know in this form 
so we can try it out!]]></content:encoded>
</item><item>
<title><![CDATA[Developing a Checklist for the Procurement of External AI Systems by Media Organizations]]></title>
<link>https://generative-ai-newsroom.com/developing-a-checklist-for-the-procurement-of-external-ai-systems-by-media-organizations-8470b3b3a407?source=collection_home_page----df04cefca135-----54-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Contract Law]]></category><category><![CDATA[AI]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Stanislaw Piasecki]]></dc:creator>
<pubDate>2023-12-14</pubDate>
<atom:updated>2023-12-14</atom:updated>
<content:encoded><![CDATA[Developing a Checklist for the Procurement of External AI Systems by Media Organizations
Stanislaw Piasecki
Follow
7 min read
·
Dec 14, 2023
--
Listen
Share
Note: This post was co-authored with 
Natali Helberger
.
An image generated by Microsoft Bing’s image creator from the prompt “AI procurement by media organizations”
The responsible use of (generative) AI technology starts with responsible procurement decisions. In this post we’ll summarize some forthcoming research (still under peer review) where we explored the considerations associated with AI procurement by media organizations in the context of contractual negotiations: How can contracts influence the sustainability and feasibility of AI adoption?
Recently, efforts have been made to support the responsible adoption of AI tools, in particular through the release of the 
Partnership on AI Procurement Guidebook for Newsrooms
. Our work serves as a complementary resource to this report, exploring in more detail some of the legal and policy issues related to the process of negotiating terms and conditions with AI providers.
Contractual conditions are used by the providers of AI systems to impose their own reading of regulations and policies, and to govern their relationship with downstream users, including media organizations. The significance of contractual terms is further augmented by the new regulatory framework developed in Brussels that 
welcomes private ordering
 through mechanisms such as negotiations, instructions, and standardization, which are included in the Digital Services Act, AI Act, and the European Media Freedom Act. You need only look as far as the Open AI 
Usage Policies
 to understand the extent to which the use of generative AI is governed by contractual terms of use.
Our study explores questions like: how are journalistic AI procurement processes perceived by professionals? What are the practical and legal obstacles experienced in negotiating procurement conditions? How could the contractual negotiation power of media organizations be improved? By answering these questions our ultimate goal is to improve the conditions for the evaluation of external AI tools that media organizations may want to integrate into their production.
Our analysis is grounded in 12 semi-structured interviews conducted with UK, EU and US experts working for both public and private media organizations. An interdisciplinary approach was adopted to comprehend more holistically the problems associated with the procurement of journalistic AI. Technologists, lawyers and managers formed our interview sample. Depending on the interviewee’s field of expertise, some subjects were discussed in more detail than others. Those discussions helped us in better understanding the difficulties emerging in contractual relationships with AI providers. The interview analysis was complemented by the evaluation of 16 terms and conditions of companies developing journalistic AI, which were based on journalistic values and relevant topics identified in the 
City of Amsterdam standard contractual clauses
 for the procurement of AI systems. We asked questions such as: what are some of the key considerations for and against procuring from external providers? What are the main issues/concerns in contractual negotiations?
While beneficial for all, guidance on AI procurement could be particularly useful for local and regional media due to their more limited experience in implementing AI technologies within their organizational structures and their distinctly unequal negotiation position. Smaller publishers are often 
“left out of deals”
 with big companies and may not be appropriately remunerated for the use of their content. This is a cause for concern also in the context of media-related AI procurement processes.
Based on our findings, we developed a checklist containing an initial list of questions to support media organizations in choosing and identifying responsible AI solutions. The checklist lists several central themes and questions that can be relevant in (1) assessing the suitability of a particular provider, and (2) scrutinizing the fairness of a procurement contract with an external provider. Not all questions may be equally relevant for all organizations and the checklist is not exhaustive. The checklist should be seen as a living document and an attempt to kick-start a discussion on the fairness of conditions in procurement contracts for AI solutions in the news media sector: We welcome all comments and suggestions on how to further improve it!
Checklist for the procurement of external AI systems
Quality training data
Explanation: The quality of the training data influences the functioning and quality of the output of a model.
Relevant questions to ask:
On which data has the system been trained?
Did the provider check the training data for bias?
Does the training data include content protected by copyright and data protection law?
If so, what has been done to ensure the legitimacy of the training data?
What are the remaining legal risks?
What guarantees are offered to deal with the remaining legal risks?
Is there a way of assessing or reviewing the training data?
Quality model
Explanation: Next to the quality of the training data, the functioning of a particular AI solution depends on the parameters and model weights used to train the model.
Relevant questions to ask:
How was the machine learning model trained?
What values has it been optimized for?
Can the model be easily trained or adapted?
Has the model been checked for bias and security?
Are there any additional steps that transform the output for the model?
Can the output modifiers or filters be easily adjusted?
How was the software tested or audited?
What issues were encountered, how were they mitigated, and the remaining risks and problems?
What benchmarks were used to evaluate the functioning of the model?
How does the provider update and keep the system aligned with the state-of-the-art?
Ownership of training data
Explanation: Implementing an AI solution can involve inputting content or training the system on media-own content. Such content is an important asset of a media organization.
Relevant questions to ask:
If a system is trained on the content or data of a media organization, will that content be re-used, and if so, for which purpose (improvement of the technology, development of competing products, etc.)?
What guarantees are offered to secure the confidentiality and lawfulness of that data?
(In case it is desirable from the media organization’s perspective to allow its content to be re-used): Is a fair compensation offered (in terms of financial reward, access to technology and knowledge, ownership of a model)?
(In case re-use is not desired): What guarantees are offered to protect the content?
Will the data be deleted from the servers of the technology company in case a media organization decides to go with another provider?
Data storage
Explanation: The location of data storage is relevant to the applicable legal frameworks, such as data protection law. With the GDPR, the EU has typically opted for a higher level of legal protection than, for example, the US.
Relevant questions to ask:
Where will (personal) data be stored?
What guarantees are offered for compliance with legal requirements, e.g. flowing from data protection law, as well as data security?
Liability
Explanation: The contracts’ important role is determining who is liable if things go wrong. Liability distributions must consider the ability to recognise and mitigate risks effectively and not create unfair or unrealistic burdens.
Relevant questions to ask:
Who is liable for what?
What guarantees are offered in case legal liabilities can arise from factors outside the control of a media organization?
What information or mutual assistance is offered to identify and mitigate potential liability, e.g., copyright infringements?
Infrastructure and hardware requirements
Explanation: Different AI solutions will have different needs in terms of hardware, access to cloud infrastructure and interoperability.
Relevant questions to ask:
Does the specific AI solution depend on particular infrastructure requirements (e.g. access to cloud technology, incompatibility with particular platforms)?
If so, what are the short and long-term additional costs?
Is it possible to switch to another infrastructure provider?
What guarantees are offered in terms of pricing, support and continuity?
Continuity
Explanation: AI solutions can be easily outdated or no longer technically supported as the state-of-the-art develops. Also, start-ups can fail, and even large operators tend to reserve the right to discontinue services without notice.
Relevant questions to ask:
What guarantees are offered in terms of continued support of the technology?
Is sufficient transparency and advance notice offered, or does the technology provider reserve a unilateral right to change, modify or discontinue service at any time?
Is the media organization free to take the training data to another provider?
Is code regularly updated to respond to security concerns, legal requirements, state-of-the-art insights into risks and ethical requirements?
Pricing
Explanation: Pricing transparency, including transparency on hidden costs, is necessary to be able to compare solutions.
Relevant questions to ask:
How is the pricing calculated?
Are there additional costs, e.g. in terms of infrastructure requirements?
What is the anticipated dynamic development of the price?
Mutual support
Explanation: Particularly with more sophisticated AI solutions, such as generative AI, professional users have only a very limited role in the training and development of the system. This, combined with a lack of transparency, skills and expertise means that media organizations may rely on the cooperation from the provider to address particular issues.
Relevant questions to ask:
What does the provider do to help identify the accuracy of content generation and detect disinformation?
What kind of assistance is offered in dealing with legal claims of third parties, particularly if the source of the claims is outside the control of a media organization?
What does the provider do to address problems around disinformation, discrimination, security, dealing with abuse?
What kind of disclaimers and indemnification clauses are included in the contract?
What kind and how long is tech support offered?
What additional resources are offered?
Environment
What efforts have been made to reduce the ecological footprint (using green energy, reducing water consumption, how to deal with CO2 emissions)?]]></content:encoded>
</item><item>
<title><![CDATA[Can LLMs Help Us Understand Data?]]></title>
<link>https://generative-ai-newsroom.com/can-llms-help-us-understand-data-49891c4e1771?source=collection_home_page----df04cefca135-----55-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[Data Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Brandon Roberts]]></dc:creator>
<pubDate>2023-12-04</pubDate>
<atom:updated>2023-12-04</atom:updated>
<content:encoded><![CDATA[Can LLMs Help Us Understand Data?
Brandon Roberts
Follow
11 min read
·
Dec 4, 2023
--
1
Listen
Share
Experiments in using local LLMs to answer questions using databases
In my work as a freelance data journalist and news apps developer, I spend a lot of time developing systems that allow less technical reporters, editors, and the general public to interact with databases. Yet, no matter how well-designed the system is, questions often arise that can only be answered by a database query, typically written in SQL (Structured Query Language).
With the rise of large language models (LLMs) that run on your local computer (rather than using a cloud service like OpenAI’s) I wondered if it might be possible to get an AI agent to answer questions written in natural language by translating and executing SQL queries on custom databases. Could this be useful to me as a data journalist, for instance, by making it faster to mine datasets for useful information? Would it be possible to have a local AI system so I could pose these questions on the fly to reduce reliance on proprietary models and protect any confidential data sources I’m investigating? Or, perhaps reporters and editors could interact with it directly and find some utility?
To start exploring these possibilities I coupled local large language models and 
Reason + Act prompting
 with a set of actions based on SQLite queries. In this post, I detail a series of experiments where I studied how well they could answer questions from my data.
Reasoning and Acting
Pre-trained LLMs are static. They’re limited by the data they were trained on. The less data on a particular subject or viewpoint they’ve seen in training, the less likely they’ll be able to accurately answer questions about it. If you ask ChatGPT who your local mayor is, there’s a good chance it will give you some outdated or even incorrect information. For this reason, using LLMs as a source for knowledge-based tasks is often a non-starter.
Reason + Act (a.k.a. ReAct, not to be confused with the JavaScript library) is a prompt format that gives an LLM the ability to interact with systems, retrieve results, and incorporate that information into its reasoning as it comes to a conclusion.
It works by interleaving “reasoning”, which helps models craft and follow plans, with acting, which grants access to external “tools” which could include other software or data/knowledge via APIs. ReAct traces (a 
trace
 is the full log of all the steps the LLM took to answer a question) look like this (borrowed from Colin Eberhardt’s 
Re-implementing LangChain in 100 lines of code
):
Question
: the input question you must answer
Thought
: you should always think about what to do
Action
: the action to take, should be one of: [LIST OF ACTIONS HERE]
Action Input 1
: the first input to the action.
Observation
: the result of the action
… (this Thought/Action/Action Input/Observation can repeat N times)
Thought
: I now know the final answer
Final Answer
: the final answer to the original input question
The idea is to construct a prompt (a 
prompt
 is a piece of text we input to the LLM) which includes one or two example ReAct traces, demonstrating various solutions to problems. Then we pose the question to be answered and let the LLM continue until it outputs an end-of-sequence token or “Observation:”. At that point we look at the output, parse any “Action” statements and related “Action Inputs.” This results in a call to a Python function with those arguments. Whatever the function returns is appended to the prompt as an “Observation” along with the model’s prior response(s). This continues until we run out of context space or the LLM comes to a “Final Answer” indicating that it has arrived at what it thinks is the answer to the initial question.
This was developed specifically to address shortcomings in traditional LLM performance in tasks like fact checking and question-and-answer problems. ReAct also helps improve human interpretability of LLM answers to questions.
A flowchart illustrating Reason+Act prompting. 
A full-sized PDF is available here.
LLMs plus SQLite
The data I had available was in the form of 
SQLite
 databases (SQLite databases are portable relational databases that fit in a single file and allow you to perform SQL queries on them). In order to allow a LLM to interact with my data via SQL queries, I provided it some actions:
tables:
 Lists the tables in the DB.
schema:
 Returns the schema for a given table (i.e. all of the variables).
help:
 Provides context for tables and columns. This is to avoid information overload in the prompt itself. The LLM can request help before using tables or columns. (e.g. “this column may be used to join on the groups table” or “sometimes users mention their location in this field”)
sql-query:
 Executes a SQLite3 query and returns the first 3 results (to preserve context space, but allow “top few” type questions).
All of these actions produce helpful error information in the event that the LLM uses an action wrong. This helps the LLM recover if it makes a common mistake like asking for the schema of a non-existent table or a SQL query naming the wrong column or trying to use a SQLite extension that isn’t enabled.
With these four actions, I manually produced a handful of example traces. The goal of these is to show the model how to use each action along with some basic SQL strategies. These were added to the prompt before posing the LLM the actual question to be answered.
Model Selection & Choice of Tools
If you’ve looked into the world of local LLMs, you may have been overwhelmed. There are an incredible number of models, formats, tools, and prompt types. One user in particular, Tom “TheBloke” Jobbins, has over two thousand LLMs available for download 
on Hugging Face
. To wade through all this I needed a way to test a lot of different models in a standardized way.
I wrote a script that asks each model a set of questions and records the result. I then scored the result based on accuracy (based on the existence of correct-answer keywords like dollar amounts and names).
I focused my search on top models on the 
Alpaca Leaderboard
. Models on the leaderboard can successfully answer a wide variety of questions and complete tasks including programming problems and logic puzzles. My assumption is that these candidates would likely be good at both SQL coding and reasoning. I also expanded my search to include some models recommended by colleagues as well as GPT-4 Turbo for comparison. I focused primarily on smaller 7B parameter models (e.g., Mistral, Llama 2), as these are the most approachable and able to run on a laptop, but I also tested some 13B and 70B models.
Experiment Design
The test consisted of six questions that would require the model to perform increasingly complex queries, using all of the tables available. The data was from a scrape of Roblox game and job postings that I’ve been working with. The Roblox Corporation, the company behind the Roblox platform, is a multi-billion dollar company that caters primarily to children. I wanted to answer journalistic questions like how many children under 18 were seeking employment on their forums, try to quantify the amount of money some games have made, and identify any unethical practices.
Here are the questions I posed to the LLMs:
“Which team was associated with the most job experiences?”
 Requires the model to group by the team column, count and then sort, taking the top few.
“What is the combined total price of all Game Passes in the data?”
 Requires a sum over the price column on the entire table.
“What are the top three most popular games by number of favorites?”
 Requires the model to numerically sort by the Favorites column, which consists of numeric strings with commas.
“What is the lowest and highest age requirement for jobs?”
 This requires the model to figure out that every value on the minAgeRequirement column is zero.
“What are some ages of users according to their profile descriptions?”
 Users sometimes mention their age in their descriptions (e.g., “I’m 15 years old”). This requires the model to do a string search and extract the number in no particular order.
“Of the people who mention their age in their profile description, what is the average age? You may be able to find this by looking for ‘ years old ‘ and then getting the characters before it.”
 Here I provide a hint to guide the model to the goal of extracting a number age and averaging it.
One of the primary limitations of working with LLMs is their context size. This is the maximum number of tokens (words and symbols) the LLM can take as input while coming up with output. Context size ranges from 2k to 4k for most local models. OpenAI models like GPT-4 can go up to as much as 128k tokens in their latest versions, though the use of longer contexts also comes with higher costs. Everything must fit inside this limit, including the prompt, examples, the question and all the ReAct cycles and final answer. Going beyond this limit results in a failure.
The need to leave enough context room for the model to use actions, write SQL queries, and retrieve JSON results meant I could only use a few examples.
I devised 
a generic prompt
 with four examples. Two were focused on showing the LLM how to use the tables and help commands. The second two examples showed SQL query solutions.
Initial Results
OpenAI’s GPT-4 Turbo model
 did consistently well when using the generic prompt. It’s a very flexible model and exhibited no significant issues regarding hallucination (with temperature=0). But smaller local models were also surprisingly competitive (note that in informal testing, instruction-tuned models tended to perform worse than completion models). Of the small models, the 
7B Mistral
-based models did the best.
When the examples provided in the trace matched closely with the question (e.g. the solution requires a SQL query where just a column or table name was different from an example), the models did consistently well. Problems requiring a simple sort based on a numeric field or taking the max/min of a column were all easily answered. But when the model needed to write a complex query from scratch and no similar examples existed, the LLM often failed to come to a solution at all. This was particularly true when the data required conversion or parsing.
For example, in the third question, the LLM needs to join two columns and then sort based on a “Favorites” column. This column has numeric strings with commas, so the model needs to strip out commas before it can be cast to an integer for numeric sorting. (Failure to strip the commas will result in a zero for all values containing one.) Without an example showing this, all models failed to come to a correct answer.
Table 1. Generic prompt results. Number of questions answered correctly for each model.
To address this problem, I wrote several extra question-and-answer traces that covered a variety of columns, techniques, and SQL strategies from all tables. Instead of presenting the LLM with a universal prompt with all the examples, I would inject individual examples into the prompt, dynamically, based on the input question’s similarity to the example questions. I initially developed this process manually, but can now pull the appropriate example traces from a small database based on semantic similarity to the input question. This significantly improved the ability for LLMs to come to solutions for the questions for which applicable examples were provided.
Table 2. Dynamically injected prompt results. Number of questions answered correctly for each model.
Mistral 7B did best with the dynamically injected example prompt strategy. The difference came down to question 6, which asked the model to get the average user age mentioned in user descriptions. Mistral 7B accidentally got this one correct. Both OpenAI’s GPT-4 Turbo and Mistral 7B erroneously cast non-numeric results to integers (resulting in zero) and included those zeros in the average, pushing the average down. Mistral rounded its result up to 16 (from 15.9), which was the closest any model got to the correct answer of 16.3 years old.
Schema Confusion
Using clear, descriptive column names is very important. To illustrate, one of my tables had a column named “creatorDescription” containing the user description. In questions where the LLM needed to search user descriptions, a huge amount of LLM time was spent figuring out that the description column wasn’t called “description”. This was also a common cause of hallucination errors. The LLM would sometimes get the schema, note in its thought that the column was called “creatorDescription” and then use “description” inside the SQL query anyway. Instead of forcing the LLM to learn an odd column name, it is easier to simply rename the column to “description”.
Likewise, a column named “team” was ambiguous to models. Extra work was required of LLMs to discover the field was a team name and not a team ID. Renaming the column to “team_name” helped disambiguate the meaning for tasks where the field needed to be used to come to a solution.
LLMs, across the board, struggled with complex transformations of data. The most challenging test question required the models to extract an age from a free-text description field (e.g. extract an integer 22 from a profile containing the text “I am 22 years old.”). SQLite has very few 
built-in scalar functions
. When given no examples of how to accomplish this, all models failed to figure it out. When I injected an example prompt that showed a string search followed by a negative index, the model could mimic the strategy and apply it to new problems.
In all experiments, naming columns plainly, simply and descriptively helped the models come to a solution more often. If you know you’re going to be answering questions about ages found inside of free text fields, it’s best to create a new column with that transformation already performed accurately. The fewer steps the model needs to take, the better it will perform.
Conclusion
In my experiments, I found that small local LLMs can compete with larger 70B and even OpenAI GPT-4 Turbo models. Mistral 7B models did particularly well given their small size. But hallucination, small context size, complex schemas, and data transformations still posed a challenge for all LLMs, particularly the small ones.
Dynamic selection and injection of examples into prompts based on question similarity proved to be a very successful strategy. It allowed some models to arrive at the correct answer for even very complicated questions. Adding a new example is a much quicker solution to new unforeseen questions or data issues compared to the retraining required for fine-tuning methods.
While both local and OpenAI models were successful at answering questions from SQL databases using Reason+Act prompting, whether or not these models would be suitable for a general audience is a different question. Local LLMs in particular have little to no guarantees around bias and alignment, making open-ended questions vulnerable to these flaws.
Reason+Act traces by themselves were also a powerful asset in building trust in the model outputs. It was easy to look at the full trace, see what queries it tried, why it was running them, check the query result, and identify areas where the model went wrong or hallucinated. This isn’t possible using traditional fine-tuning or retraining methods.
I plan to expand upon this and explore using more complicated datasets and RDBMS (like PostgreSQL). I’ve 
published a video
 demonstrating these techniques and I’ve 
also made code available
 if you’d like to run your own experiments on your own data.]]></content:encoded>
</item><item>
<title><![CDATA[Building AI-Powered Projects for Local Newsrooms]]></title>
<link>https://generative-ai-newsroom.com/building-ai-powered-projects-for-local-newsrooms-f00d105d05ff?source=collection_home_page----df04cefca135-----56-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Local News]]></category><category><![CDATA[Generative Ai Tools]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Ernest Kung]]></dc:creator>
<pubDate>2023-11-20</pubDate>
<atom:updated>2023-11-20</atom:updated>
<content:encoded><![CDATA[Building AI-Powered Projects for Local Newsrooms
Ernest Kung
Follow
14 min read
·
Nov 20, 2023
--
1
Listen
Share
Renee Richardson, Managing Editor at the 
Brainerd Dispatch
 newspaper, has many responsibilities to juggle as a local news leader. Despite a busy schedule, she found time to discuss her lack of time. In her pitch to the independent judges selecting AI-powered projects for development as part of 
AP’s Local News AI Initiative
, Richardson wrote that time is “the one thing we typically want more of in order to do all the things on our work lists. And it’s the one thing that seems to slip away with so many demands from all sides in a modern newsroom.” She added that “the AI project would give us more time in our newsroom and would be a wide-reaching benefit in the news industry that could be replicated for multiple newsrooms.”
The 
Brainerd Dispatch
 project pitch, along with those for 
El Vocero de Puerto Rico
, 
KSAT-TV
, 
Michigan Radio
, and 
WFMZ-TV
 were selected for development. This past month, after more than a half year of work, the projects are being released to the public. All projects delivered a working prototype for each newsroom. 
Case studies and open-source code
 for most of them are being published by The Associated Press.
AP was one of four organizations that shared a $3 million grant in 2021 from the 
John S. and James L. Knight Foundation
 to expand AI in local news. Paul Cheung, then the director for journalism and technology innovation at Knight, wrote, “At many national news organizations, AI is beginning to drive success on the operations side and in the newsroom — from enabling the use of data to increase subscriptions and serve more relevant content, to automating certain kinds of news production to reduce costs.” However, Cheung wrote, “local news organizations still haven’t tapped the technology’s full potential.”
Along with AI Program Manager Aimee Rinehart, we set off on a two-year journey to help local newsrooms adopt AI (though not necessarily generative AI) into their workflows. We started with a 
national research report
 that incorporated responses from nearly 200 local newsrooms in all 50 states along with D.C., Puerto Rico, and Guam. That was followed by free courses based on the learnings identified in the report. Finally, it led to our projects. Pitches came in from 34 newsrooms. After working with 15 of them for a design sprint, five proposals were ultimately selected for full development.
It is my hope that the five projects for local newsrooms described in this post demonstrate how AI can be helpful to journalists who are generally burdened by too little time and too much to do.
Brainerd Dispatch
The Brainerd, Minnesota newspaper’s proposal was straight-forward: help its journalists save time by making it easier to publish police blotters. In partnership with the University of Missouri, we built a comprehensive system including a simple UI and efficient backend. The technical details are available in the 
case study
.
Journalists at the 
Dispatch
 now have access to a system that can process PDFs of incident logs provided by three local law enforcement agencies, have that data presented to them in order of editorial priority, and have the control to select which ones will be published. Once selected for publication, we send the reports to 
Lede AI
 for story generation, and they upload the draft story to the CMS.
Figure 1. User interface of Public Safety Reporting System showing a screen that shows blotter incidents sorted by editorial priority. Journalists manually select which items to be published using the checkboxes on the left.
We opted not to build our own template-based AI story generator for three key reasons: It’s an existing commercial technology, limited time and funds for the execution of the project, and the 
Dispatch’s
 corporate owner Forum Communications already had a business relationship with Lede AI meaning they were already integrated into the CMS. While Lede AI is known for automating high school sports stories, they developed new algorithms and templates to support local crime reporting for this project.
El Vocero de Puerto Rico
The San Juan-based newspaper marks September 20, 2017 as an inflection point. That was the day Hurricane Maria made landfall in Puerto Rico, bringing widespread death and destruction to the island. As the storm was approaching, the paper’s journalists wrote and translated critical updates from the National Hurricane Center (NHC) by constantly refreshing the forecast web page.
After the storm, 
El Vocero
 searched for a better solution to publish potentially lifesaving alerts faster. Maria Arce, then the digital director at the newspaper, wrote in her proposal to AP, “With an automatic system that can quickly translate and immediately publish the National Hurricane Center advisories… we will protect thousands of citizens that rely on us, preventing history from repeating itself.”
As detailed in the 
case study
, AP and its partner Northwestern University built a system that can automatically process updates from the NHC, translate the English language updates into Spanish via commercial translation provider DeepL, write a brief template-based story, and upload it into the CMS as a draft. (DeepL was selected as the best fit after we reviewed other commercially-available translation services). One of the issues 
El Vocero
 noted was that the NHC publishes its updates in English first with a Spanish translation following at inconsistent times. It became an unavoidable requirement that we base our process on the English alerts.
Figure 2. CMS preview output of automatically-generated weather alert story. The image that appears here was a placeholder and does not match the content of the story.
KSAT-TV
The San Antonio, Texas station faces an issue common to local TV broadcasters in markets with increasingly digital consumption habits. With more staff dedicated to linear TV than its digital products, Bernice Kearney, then the news director, proposed to AP a project to build a way to make it easier to publish digital stories. “News operations of all sizes — from legacy broadcast stations, to newspapers, online outlets, or newsletter-focused organizations — can benefit from this video-to-article content generation that still calls for a final edit before publication,” Kearney wrote in her pitch.
In partnership with Stanford University, we built a system that takes short pre-edited videos, such as an excerpt of a news conference or interview, transcribes it, summarizes the transcript, and transmits the summary as a draft story to the CMS. At multiple points along this process, we require a journalist’s intervention. First, the transcription must be reviewed for accuracy by editorial staff before summarization. Then, the summary is reviewed and edited by staff before it can be published.
Figure 3. Dashboard of the application. Users can select different categories for videos. Each category is tied to a different prompt for GPT 3.5. Summaries can be regenerated on demand, taking advantage of the non-determinative nature of the technology.
As we note in the 
case study
, this project uses OpenAI’s gpt-3.5-turbo model to power its summarization process. The developers produced prompts that could generate an accurate summary with minimal false information. We delivered two sets of prompts, one that would generate the summary, and a separate one to produce keywords for tagging that story.
For summarization, we used OpenAI’s gpt-3.5-turbo model on the chat endpoint with a system prompt to specify the context and task:
System:
 You are an AI tasked with summarizing a news story. Your goal is to provide a concise summary that captures the main points of the story while being smaller in length that the original text. If you believe the original text is already concise enough, you can leave it unchanged. Please generate a summary that retains as much important information as possible while reducing the length.
User:
 Original News Story:
[TRANSCRIPT GOES HERE TRUNCATED TO FIT WITHIN TOKEN LIMITS, THE PROCESS LOOPS UNTIL THE ENTIRE TRANSCRIPT IS COMPLETED.]
For tag generation, we used OpenAI’s gpt-3.5-turbo model on the chat endpoint. The input is the summary generated in the prior process:
System: 
You are an AI tasked with generating tags for a news story based on a text summary. You are looking for important persons, places, things, concepts/subjects. Given the following news summary, please generate a comma-delimited list of tags:
User: 
News Story:
[SUMMARY GOES HERE]
Michigan Radio
We were planning to develop all five projects from scratch, but the independent judges were hooked by the proposal from the NPR affiliate at the University of Michigan in Ann Arbor. The Michigan Radio team previously developed an application called Minutes that transcribes city council meetings posted on YouTube or Vimeo. The project was built using Google News Initiative funding. Dustin Dwyer, a reporter and producer, wanted more for Minutes. He proposed that AP “use AI to develop new features to extract even more useful information from our database of transcripts.”
Our partners at Northwestern University began development with twin goals of adding AI-based summarization, and keyword-based alerts to Minutes. It didn’t go as planned. After taking a deep dive into the existing code, the Northwestern team discovered a critical weakness with the transcription system on Minutes. As we describe in the 
case study
, the Google Cloud Platform speech-to-text system was producing transcripts with very high error rates.
Figure 4. The transcript view of the application. Note in the 1:30 block the apparently odd transcription of “go home and shave okay”. That was actually said on the recording. Also note in the 2:50 block the accuracy of the Native American names.
Faced with a limited amount of development time, we all agreed that the transcription system needed to be fixed first, setting aside the summarization goal. Besides, no summarization system can be built without an accurate transcript. The engineers settled on 
OpenAI’s Whisper
 as a replacement transcription system. Separately, a class at the University of Michigan built out the new keyword-based alerting feature.
WFMZ-TV
This project for the Allentown, Pennsylvania TV station is the most ambitious of the set. Working with the University of Missouri, we built a system that relies on a custom-trained model as well as an off-the-shelf large language model. The goal was to automate some of the planning functions of assignment editors by placing newsworthy events into a calendar for them.
The pitch for this project came from an Alaska TV station, whose news director wrote, “this will free up assignment desk staff and planning editors from their ‘admin’ roles half of the day and also ensure we do not miss events due to the deluge of emails.” The news director added, “with news staff doing more with less these days, efficiencies are key.” Unfortunately, the corporate owner of the station declined to proceed with the project. AP approached WFMZ-TV, an independently owned station, which had already participated in our national research report to pick up the project.
The developed system monitors incoming emails sent to the assignment desk, and looks for events that may be considered worthy of being placed into the planning calendar in the CMS. The emails are pulled from the corporate email system and processed by the custom model, which was trained on thousands of emails annotated by five assignment editors at the station. We then use OpenAI’s GPT models to extract event dates and times from the unstructured emails, to summarize the emails, and to create a story slug.
Our custom model classifies incoming emails as one of three categories: Worthy, Unsure, and Not Worthy. Worthy emails are sent to the CMS as they contain events. Unsure emails are ones that potentially contain news but not an event. Not Worthy emails have no editorial value. In initial testing of approximately two weeks, the WFMZ-TV staff found that the model leans towards false positives when it comes to Worthy emails. That is, there were more emails classified as Worthy than what staff considered Worthy. However, after reviewing emails in the Unsure and Not Worthy categories, they didn’t find any emails misclassified that were actually Worthy of being in the CMS planning calendar. In technical terms this translates to no false negatives for the Worthy category.
Figure 5. The main application view that shows emails filtered by the AI model’s classification, in this case “Worthy” emails.
Date extraction as originally built using OpenAI text-davinci-003 model using the completions endpoint:
Prompt:
 Extract the date and time of the event in yyyy-mm-ddThh:mm:ss format from this email where the current date is
 [CURRENT DATE IN MM/DD/YYYY FORMAT]:
[EMAIL SUBJECT] [PLAIN TEXT EMAIL BODY TRUNCATED TO FIT WITHIN TOKEN LIMITS]
In early testing of the prompt above, we saw that the response was sometimes inaccurate when it came to the year of the detected event. To resolve that, we added the current date into the prompt, which successfully resolved the issue.
After our initial development period on this project concluded in September 2023, OpenAI announced the deprecation of the text-davinci-003 model we were using. We re-opened development post-delivery to implement the following changes.
Date extraction using OpenAI gpt-3.5-turbo or gpt-4 models using the chat endpoint:
System:
 You are a data parser that responds only with a datetime object in ISO format. An email containing an event will be provided by the user.
System:
 Extract the date and time of the event and return it in ISO format e.g. 2023–01–25T21:30:00 and no other text is wanted.
System:
 If an event date is found but there is no time then the event time is 00:00:00.
System:
 If there is no event return FALSE
System: 
The current date is
 [CURRENT DATE IN MM/DD/YYYY FORMAT].
User:
 Email containing event
:
[EMAIL SUBJECT] [PLAIN TEXT EMAIL BODY TRUNCATED TO FIT WITHIN TOKEN LIMITS]
We found that the intended replacement of the text-davinci-003 model, gpt-3.5-turbo-instruct, did not perform as reliably in extracting dates. So, we rebuilt it using gpt-3.5-turbo and gpt-4. The reason we built out processes using two models is the significant cost savings in operating the gpt-3.5-turbo model versus the pricier gpt-4. The logic we applied is as follows:
Extract date and time using gpt-3.5-turbo
2a. If date and time are found, use it
2b. If date and time are not found, re-run the same query using gpt-4
3a. If date and time are found, use it
3b. If date and time are not found, return FALSE and reclassify email as “Unsure” from “Worthy”.
Later on, we decided to build in some additional generative AI features. We built a function to summarize emails, and then a separate function to use the summaries to create a story slug. In both cases, we deliberately used the lower cost gpt-3.5-turbo model.
Summary generation using OpenAI gpt-3.5-turbo chat endpoint:
System: 
You are an assistant tasked with summarizing the contents of an email in three sentences readable by a journalist. An email containing an event will be provided by the user, focus on that event.
System:
 The summary must contain the date and time of the event.
System:
 If an event date is found but there is no time then say exactly ‘No time is available.’
System:
 The current date is
 [CURRENT DATE IN MM/DD/YYYY FORMAT].
User:
 Email to summarize:
[EMAIL SUBJECT] [PLAIN TEXT EMAIL BODY TRUNCATED TO FIT WITHIN TOKEN LIMITS]
Story slug generation using OpenAI gpt-3.5-turbo chat endpoint which uses the summary generated in the prior process:
System: 
You are an assistant tasked with writing a slug that summarizes text in 5 words or less. A summary containing an event will be provided by the user, focus on that event. Emphasize people and places. Answer in a consistent style.
User: 
Write a slug for this event:
State officials in Pennsylvania will hold a media event tomorrow, Tuesday, October 17 at 1:30 PM at the Pennsylvania Turnpike Commission in Middletown. The event will feature Pennsylvania Department of Transportation Secretary Mike Carroll, Emergency Management Agency Director Randy Padfield, and Pennsylvania Turnpike Commission CEO Mark Compton, who will discuss agency preparations and provide winter tips to the public.
Assistant:
 WINTER TRANSPORTATION TIPS PRESSER
User: 
Write a slug for this event:
There is a fishing rodeo event happening on Saturday, April 29th in Gilbertsville, PA. Tony Mashintonio has been opening up his property for veterans and children to enjoy a day of fishing at his two stocked ponds. The event starts around 11:30 AM and is organized by volunteers and contributions.
Assistant: 
VETERANS FISHING RODEO
User:
 Write a slug for this event:
Northampton County Executive Lamont G. McClure will present the 2024 Northampton County Budget on Tuesday, October 3, 2023, at 10:00 AM in Council Chambers at the Government Center, 669 Washington Street, Easton, PA 18042. The Budget Address will also be live-streamed on the County’s YouTube Channel. Media attendance is encouraged.
Assistant:
 NORTHAMPTON BUDGET PRESENTATION
User:
 Write a slug for this event:
[SUMMARY GOES HERE]
We used the few-shot prompting technique to create slugs in the function above. In this case, we used real summaries generated by gpt-3.5-turbo of actual emails received by WFMZ. Note that the emails used in this few-shot prompting process do not necessarily reflect messages that our custom model would deem “newsworthy”.
Future Work
Newsrooms will need to plan for workflow changes as a result of adopting AI tools. At KSAT-TV, news managers convened leadership from across the Graham Media Group stations to discuss policies around the use of AI. Should the newsroom disclose the usage of AI? If so, how? Those were some of the questions tackled by the group. Additionally, the station is working out how best to use the tool and who should be using it.
Similarly, WFMZ-TV leadership began a discussion on how assignment desk work might change with the new tool. They intend to use the tool, but only after successful completion of an extended period of real-life testing.
At 
El Vocero de Puerto Rico
, the newspaper is considering allowing the tool to publish alerts directly to the audience without human review. But that decision will only come after the tool is tested over an extended period with manual review.
The 
Brainerd Dispatch
 is excited to use its tool to save reporters’ time. In their case, they would have liked more police agencies to be covered by the application. They have the support of their corporate developers at Forum Communications. The company would like to prove out the utility of the tool at the 
Dispatch 
through extended testing, and if satisfactory, will seek to expand it to its other group newsrooms.
Michigan Radio is already testing the expansion of its tool with multiple newsrooms that have reached out after the project was publicized by AP. It is now covering city council meetings from coast to coast for both broadcasters and newspapers.
Learnings
What I’ve learned over the last two years of working on the Local News AI Initiative is that we have a tremendous opportunity to create a new standard of local journalists working with AI. For example, just as it is an industry standard for journalists to type their stories into a computer rather than a typewriter, the future needs to be a model of “hybrid journalism” blending human and AI in productive new configurations, as Nick Diakopoulos described in his 2019 book “
Automating the news: how algorithms are rewriting the media
”.
There are many areas of automation to explore in newsgathering, production, and distribution. Our five projects give tantalizing glimpses of the future of how journalism can be practiced in local markets. It’s reasonable to ask why a police blotter must be manually curated and written. It’s reasonable to ask why a severe weather alert needs to be manually written. And it’s reasonable to ask why a journalist needs to spend time manually transcribing videos and interviews. 
Surely 
having AI help with these tasks is a better use of our time.
The proposal by Renee Richardson, the managing editor of the 
Brainerd Dispatch
, was powerful to me because of its directness in acknowledging the uncomfortable fact of our industry — we as journalists are overburdened. It doesn’t matter which medium you work in, print, broadcast, and/or digital. We’re all facing the same limitations that strain our mental health and disrupt our personal lives.
I am not suggesting that AI is a cure-all to what troubles the local news industry. It is just one piece of a larger puzzle that will include immense amounts of work on finding sustainable business models, and identifying ways to deliver quality journalism that can break through the noise of countless digital distractions.
It’s my hope that all our projects deliver a much-needed push for the local news industry just in time for the generative AI revolution.]]></content:encoded>
</item><item>
<title><![CDATA[Getting the Science and the Scoop with News Angles from GPT-3]]></title>
<link>https://generative-ai-newsroom.com/getting-the-science-and-the-scoop-with-news-angles-from-gpt-3-b1b4fcd7f6b1?source=collection_home_page----df04cefca135-----57-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Large Language Models]]></category><category><![CDATA[News]]></category><category><![CDATA[Science Communication]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Sachita Nishal]]></dc:creator>
<pubDate>2023-11-14</pubDate>
<atom:updated>2023-11-14</atom:updated>
<content:encoded><![CDATA[Getting the Science and the Scoop with News Angles from GPT-3
Sachita Nishal
Follow
13 min read
·
Nov 14, 2023
--
Listen
Share
How we used GPT-3 to generate different news angles for jargon-y science articles¹
A news angle is a specific perspective or “hook” on a particular story, useful for helping to determine newsworthiness and also frame the story in a way that makes it more engaging and relevant to readers. Common ones include community angles,such as how a new policy affects a local community, or human interest stories including things like profiles of the scientists behind groundbreaking discoveries. There is no one “best” news angle for a story. The choice of angle depends on the story and the reporters’ interests, as well as on the target audience. And, 
every
 story has an angle. So, given their importance in newswork, we wondered: Can generative AI produce angles that help journalists?
To get at this question, back in mid-2022 we started experimenting with GPT-3 to automatically generate different news angles based on the abstracts of scientific articles that could potentially frame a story for a science or tech reporter. Our goal was that journalists would view these potential news angles, make their own subjective assessments of whether a particular article might be interesting to them and newsworthy for their audience, and decide whether delving deeper into the more jargon-heavy full text would be worth their time. We hoped to support journalists by presenting distinct, simplified, and more newsworthy versions of an article’s abstract as a way to evaluate whether the article might warrant further time, attention, and investigation for a story. We believed such news angles could support journalists in identifying and telling compelling stories, but also wanted to maintain accuracy and informativeness in the generated angles.
In this article, I will present a deep dive into different approaches that we piloted, the experiment we finally ran, and how 11 staff reporters and freelancers responded to it in a set of interviews. For our use case, we found directly prompting GPT-3 to work better than fine-tuning, and saw that reporters relied on the resulting angles for interpreting the research, filtering leads, and framing potential stories. We hope this can help others configure and evaluate their experiments with generative AI.
If you would like to explore this work in further detail, you can also check out the preprint of the academic paper this blog post is based on: 
https://arxiv.org/abs/2311.06864
Strategy
Our goal was to use the title and the abstract of individual scientific articles to generate multiple, distinct news angles. We used 
OpenAI’s Playground interface
 to conduct initial informal experiments with different parameter settings. For instance we varied the model (
text-ada-001
, 
text-babbage-001
, 
text-curie-001
, and 
text-davinci-002
), prompts, maximum tokens generated (
max_tokens
 parameter), how creative the model would be (
temperature
 parameter), and the degree to which it could repeat tokens and words across angles (
frequency_penalty
 and 
presence_penalty
 parameters).
In addition to ad-hoc testing of prompts over different models and parameters, we also assessed if fine-tuning the smaller and cheaper models like 
text-ada-001
 and 
text-babbage-001
 could yield good results, compared to zero-shot prompting on the smaller models or the largest one (
text-davinci-002
 at the time when we ran the experiment in 2022). We used these early experiments to identify a good overall strategy (fine-tuning vs. zero-shot), as well as a good model for the chosen strategy, and a smaller set of prompts and parameter values that consistently produced good results in OpenAI’s Playground.
We knew we were going to use these angles in a human-in-the-loop system, where a user would be able to view both a set of angles and the corresponding scientific abstract. However, well-known issues of model bias, hallucinations, and data memorization made us think it was important to run an evaluation to assess which combination of these factors would minimize problematic behavior.
Fine-tuning Experiments
We fine-tuned two models using the OpenAI API, and tested their outputs over different prompts and parameters in the Playground in an ad-hoc fashion. The dataset we used for fine-tuning comprised pairs of article titles concatenated to their abstracts (we refer to this as the “article text”), along with a news headline that had been used to describe the article in a real, prior news story. We used 
Altmetric
, a service that compiles news and social media coverage data of scientific articles, to extract this data for 
arXiv CS preprints
 published from 2017 to 2021. Following a cursory manual inspection of the dataset, we filtered it to include headlines from a curated set of popular U. S. news outlets that did not syndicate or duplicate content from others, and then cleaned the corpus by removing empty headlines, HTML code, Latex code, non-English headlines, exact and fuzzy duplicate headlines, and outlier headlines that were too short.
This led to a set of ~2000 article text and headline pairs that we used to fine-tune the models by appending the following prompt to the article text: “Generate a news headline for this article in one sentence.” The headline generation task was assumed to be a good proxy for uncovering news angles. Occasionally, an article was covered by multiple outlets and had multiple headlines associated with it. This too, was perceived as a benefit, since it could help the model learn about multiple different angles from different outlets on the same article text.
Fine-tuning experiments with these brought up an interesting insight: often, the model would hallucinate details about an article in the generated headlines, typically along the lines of “Stanford researchers find … “ or “Scientists from Google reveal … “, despite no such institutions being part of the study. This was interesting because celebrity and prestige associations are also a type of news angle that journalists rely on, but the models hallucinated these details by learning from historical headline data where institutions were mentioned, since journalists will add this information based off of article metadata and interviews. However, the article text did not mention authors or their institutions, so it wasn’t learning from specific facts, merely replicating certain patterns based on biases in the data (and perhaps just reflecting journalistic inclinations in choice of angles). This was a clear error.
We could have added in author and institution data to fix this problem, but were concerned that this could potentially bias the model to only highlight articles with this kind of prestige angle. As a solution, we hand-curated headlines to edit out references to specific authors and institutions in the historical headline data, and replaced it with “Researchers” unless specified in the abstract. This also led us to uncover new issues and make other fixes during this stage, like removing stories where the headline had a very tenuous connection to the article e.g. titles from 
MIT’s The Download newsletter
, which are generally broad since they surface multiple articles and stories within one newsletter². We also manually unabbreviated certain terms in headlines where an abbreviation was not in the article text, edited out residual Latex code, and references to conferences that are sometimes present at the end of arXiv abstracts. This led to a set of ~1700 cleaned article text-headline pairs for fine-tuning.
Ad-hoc results over this set were significantly better, and model hallucinations about institutions stopped appearing. 
The main lesson here was that manual checking of fine-tuning datasets is key, since it may lead to the discovery of problems in the data that you may not have uncovered during cursory inspection or more quantitative exploratory data analysis
. Manual checks and fixes reduced the size of the fine-tuning corpus by ~15%, but significantly improved output quality. A second lesson was that fine-tuning should probably be an iterative process, since artifacts in the data generation process — in this case, the lack of linkage between institution names in the article text and headlines — may only present themselves during inference calls, and may be hard to predict at the outset of experiments. The final lesson was the data preparation process will entail making decisions that can produce bias (e.g. the potential of prestige bias), and we must constantly be aware of the choices we make about including or excluding certain features, as well as the downstream effects of these choices.
Prompting Experiments
As we were conducting these fine-tuning experiments, we also began testing out how the base 
text-davinci-002
 model might perform, without fine-tuning, and with similar prompts and parameters that we were using for ad-hoc testing on the fine-tuned models.
We setup a prompt that would ask the model to generate three different news angles, and these were some of the prompts we tried out:
List three different news headlines about this: <article text >
List three different news angles about this: < article text >
List three different perspectives on this: < article text >
List three newsworthy angles about this: < article text >
List three interesting perspectives on this: < article text >
For both direct prompting (i.e. zero-shot) and fine-tuning, we configured the 
max_tokens
 parameter to be 256. This choice was based on the observation that the median length of headlines in the fine-tuning dataset was approximately 65 characters. We opted for a higher limit to ensure that the generated angles could be informative. We experimented with the 
temperature
 parameter, exploring values of 0.5, 0.75, 0.85, and 0.9. Higher temperatures tend to produce more “creative” outputs, although excessively high values could introduce additional “hallucination” and factual errors. We also adjusted the 
frequency_penalty
 and 
presence_penalty
 in a similar manner. Higher values for these penalties reduce the likelihood of generating repetitive output, but pushing these values too high can negatively impact output quality, as per OpenAI’s documentation.
In our qualitative comparison of responses between fine-tuned and direct prompting, we found that direct prompting was the clear winner. Text generated by the fine-tuned models produced interesting headlines but tended to be generic and repetitive in their angles, often quoting quantitative results directly from abstracts or rephrasing the title or final findings in simpler terms. On the other hand, the direct prompt explored a broader conceptual space related to a given article. For example, it made connections to themes like surveillance and consent for an article on facial recognition.
Although we could have considered further fine-tuning the larger model to understand the cause of these differences, we opted not to. This was because we were uncertain about the extent of improvement that fine-tuning would offer on top of existing performance. The comparison of direct prompting and fine-tuned models also made us uncertain if headline generation was indeed a good proxy for our task, which was to generate creative but still informative angles to encourage further investigation, and not necessarily click-worthy text for audiences. Also, our primary goal was ultimately to explore initial journalistic responses to news angles rather than deploying the best possible model in production, so further efforts at optimization did not make sense at this stage.
Having decided to use direct prompting, we then selected a smaller set of prompts and parameters to compare for the human evaluation study, based on prior informal experiments as well as the parameter behavior described in OpenAI’s documentation. We tested these out in all possible combinations:
Prompts:
– List three newsworthy headlines for this: <article text>
– List three newsworthy angles for this: <article text>
Temperature:
– 0.75
– 0.85
Frequency and presence penalty (varied together):
– 0.85
– 0.95
This led to 8 possible combinations for human evaluation.
Human Evaluation
This method is generally perceived to be the gold standard in Natural Language Generation (NLG) evaluation studies since human raters can provide a much more subjective and comprehensive assessment of text compared to most automated metrics, since the latter often only focus on a single, non-contextual, aspect of text quality e.g. grammar. To conduct a human evaluation study, we identified three specific criteria for collecting subjective ratings from human raters to assess the quality of news angles generated by GPT-3. Each individual angle generated by GPT-3, for each article text and each prompt-parameter combination, was rated on a five-point scale from 1 (Strongly Disagree) to 5 (Strongly Agree) for these criteria:
Fluency: The text is fluent and readable, without grammatical errors.
Accuracy: The text accurately reflects the material presented in the scientific abstract.
Angle Quality: The text centers one or more news angles on the presented scientific abstract for journalists.
We then calculated an Overall Quality score as the average of fluency, accuracy, and angle quality ratings.
We gathered ratings from two datasets: a development set of 25 articles and a test set of 25 articles, all randomly selected from arXiv CS publications in 2022. The development set was used to familiarize human raters with the rating task and criteria, with a couple pilot rounds to ensure consistency in their application. Continued discussion and comparison at this stage was key to achieving a shared understanding of the criteria and the rating scales.
Two raters then evaluated three different news angles for each of the three specified criteria, for each of fifteen abstracts in the test dataset. To measure inter-rater reliability, we computed the 
intra-class correlation coefficient (ICC)
 for the Overall Quality criterion, which resulted in an ICC of 0.894 (n=15, p<0.01), indicating a high level of agreement between raters in their assessments. Subsequently, one rater evaluated the remaining abstracts in the test set. To identify the best instruction-parameter configuration, we then compared their average ratings for Fluency, Accuracy, Angle quality, and Overall Quality from across all abstracts in the test set.
The final configuration of the model, prompts, and parameters used in this study were selected based on their highest overall quality across articles and angles. They were as follows:
Model: 
text-davinci-002
Prompt: List three newsworthy headlines for this abstract: <article text>
Temperature: 0.85
Frequency Penalty: 0.85
Presence Penalty: 0.85
An Initial Response from Reporters
Following the selections of models and parameters, we prototyped a basic interactive interface within which arXiv articles and their news angles would be displayed for reporters to navigate and interact with, as shown in the screenshot below. To test it out, we invited 11 reporters from diverse backgrounds, including freelancers and staff in general assignment, science, health, technology, and data reporting. We explained how news angles were generated and encouraged them to use this interface to discover potentially newsworthy leads.
A screenshot from the prototype interface, showing the layout of article text, metadata, and news angles.
Participants generally found news angles to be a reliable tool to pinpoint key points and hooks in research articles. They also found these angles useful for identifying potential interviewees, target audiences, and even for building their own interpretations where scientific language was confusing. The simplicity of news angles compared to abstracts often led participants to read them first to assess the newsworthiness of the article.
Despite known concerns about GPT-3’s limitations, including potential for inaccurate output and lack of training data transparency, participants were receptive to using generated angles as initial points for further validation. Some even felt that the angles could be turned into headlines and pitch lines with minimal changes. However, they acknowledged the need to vet these angles for accuracy, highlighting the significance of human-in-the-loop systems. One participant even suggested providing a list of known GPT-3 caveats to aid journalists not familiar with the model. The participant notably stated, “It’s totally fine,” even if there was a certain degree of unpredictability or “wonkiness” in the news angles, 
as long as the journalists are aware of it
.
Participants also suggested improvements such as the ability to discard repetitive angles and the need for future work to address issues like text mimicking titles and abstracts, or lack of variance among generated angles. This can be accomplished in future iterations by tweaking the penalty parameters, and automatically filtering out semantically similar generated angles.
Additional suggestions from participants were also geared towards filtering and personalization of news angles, based on the interests of the reporters and the outlets they were writing for. Implementing prompts that identify specific types of news angles could be beneficial. Using standardized taxonomies of news angles might also allow filtering of leads based on the news angles of interest. Involving journalists in this process of tailoring prompts could also help them better understand how to use generative models.
From a quality standpoint, there were still the occasional cases where the generated text significantly repeated the article text or specific jargon, or where its generation was incorrect. Prompting large language models that are augmented by knowledge bases, and specifically feeding in discussion and conclusion sections of articles as input could help solve these problems of factuality and complexity.
Looking Ahead
Running different experiments, human evaluations studies, and interviews brought to light the potential and challenges of implementing LLMs, specifically GPT-3, to aid science reporters in interpreting and framing research papers. A few important themes came up during our evaluation, such as the significance of data quality for fine-tuning, the superior performance of direct prompting in this case, the importance of breaking down scientific jargon, transparency about LLM functioning and usage, as well as the potential value of filtering and personalizing news angles. Managing these challenges could involve varied strategies. For example, adjusting prompt and retrieval strategies or parameter configurations, and adopting interface elements such as jargon dictionaries (e.g. on hover, as suggested by a few participants) or keyword filters.
The fact that reporters were aware of the potential for jargon, error, and repetition, but still excited about using this tool as a way to begin exploration and interpretation was interesting to us as we conducted this study. Based on our findings, we advocate for the deployment of such tools within a human-in-the-loop system, which makes room for crucial oversight. In our application, the cost of a system error was also relatively low: reporters, at worst, risked wasting time on non-newsworthy articles that a news angle might otherwise lead them to read. Generally speaking, low cost of error, and high degree of supervision seems like a good region of tasks to support with generative models.
Based on our interviews, we are also interested in conducting participatory design studies and a longer-term deployment of such a tool in the near future, to more deeply understand its impacts on reporters’ routines and workflows, and to both quantitatively and qualitatively understand if it is actually useful in real-world settings. We cannot ignore the potential for bias with such a model, and a longer-term deployment or an audit could also help surface aggregate patterns of bias and inaccuracies that arise with such angles, and that will require mitigation.
Ultimately, it is clear that the integration of AI tools in journalism holds potential, but requires careful experimentation and evaluation to assess what technical and interface design choices might best serve the needs and desires of reporters. We broadly recommend the adoption of human-centered design practices to others who may choose to explore this exciting space!
Footnotes
[1]: Icons generated by 
Freepik
 on Flaticon
[2]: MIT Tech Review appears to associate any research papers cited by a news story to all the news stories linked on the same webpage. Such irregularities are part and parcel of the web scraping process, but are only surfaced with lots of checking and backtracking.]]></content:encoded>
</item><item>
<title><![CDATA[Rising to the Challenge: Applying Generative AI in Newsrooms]]></title>
<link>https://generative-ai-newsroom.com/rising-to-the-challenge-applying-generative-ai-in-newsrooms-283d5bb3de53?source=collection_home_page----df04cefca135-----58-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[News]]></category><category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[AI]]></category><category><![CDATA[Innovation]]></category>
<dc:creator><![CDATA[David Caswell]]></dc:creator>
<pubDate>2023-10-11</pubDate>
<atom:updated>2023-10-11</atom:updated>
<content:encoded><![CDATA[Rising to the Challenge: Applying Generative AI in Newsrooms
David Caswell
Follow
6 min read
·
Oct 11, 2023
--
Listen
Share
An image generated by Open AI’s DALL.E 3, from the prompt “An abstract image representing newsrooms around the world applying generative AI”
The journalism industry is all too familiar with technology-driven disruption and has often been accused of being slow to adapt to new technological realities like social media, smartphones and the internet itself. The advent of large language models (LLMs) and generative AI seems like another of these disruptors, but journalism isn’t lagging this time. In newsrooms around the world, senior leaders and working journalists alike are recognising the potentially dramatic power of these new tools and are actively seeking to apply them in their workflows, products and strategies. A decade of innovation projects, lessons from the growth of social media and hard-won operational experience in applying big data and machine learning to recommendations, subscription management and investigative journalism have left the news industry better equipped to handle AI than many assume. This article describes a global programme in which 12 digital-first newsrooms with track records of entrepreneurial innovation are developing generative AI projects that are both pragmatic and potentially transformative for their journalism. Developed and funded by the Open Society Foundation, the AI in Journalism Challenge — or AIJC — is now in full swing. Participating newsrooms are fully immersed in their projects, and some useful observations are emerging.
The AIJC programme is structured as a competition in which participating teams are provided with educational, mentoring and financial support for developing their AI projects. The 12 participating newsrooms were selected from a pool of 113 applicants who responded to a public solicitation with a brief project proposal. The selection process focused on the degree to which the proposals were pragmatic, measurable and potentially transformative, as well as on the commitment that the applying teams were prepared to make to their project. Following selection the participating teams were then provided with a series of workshops about aspects of generative AI, as well as with a dedicated 6-module version of the London School of Economics’ JournalismAI Academy for Small Newsrooms. Each team was also provided with one-to-one mentoring as they developed their projects into detailed specifications and designs, and each was provided with a £5000 development grant to fund their work. The 12 participating teams are now executing those projects and will demonstrate them to a judging panel of news industry experts in mid-October. Five finalist projects will be selected, and a final round of demonstrations and judging will be done at the Splice Beta journalism festival in Chiang Mai, Thailand in November. The winning project will receive a £25,000 grant, and the lessons learned from the full programme will be shared publicly in a comprehensive report. The AIJC programme is managed by Open Society Foundation staff, and I have participated as the lead consultant on applied generative AI.
Digitally native and purpose-driven newsrooms in Asia, the Middle East, Africa, Eastern Europe and South America are some of the most innovative news organisations in the world. Imbued with scrappy pragmatism and with a history of delivering high-impact journalism with relatively few resources, these deeply entrepreneurial news organisations often outpace their larger, better-resourced peers in the UK and US. The AIJC challenge is designed to take advantage of that creativity and energy to find useful ways of applying generative AI in news, and its participants reflect that ambition. The cohort includes: 
Agência Pública
, an investigative journalism agency in Brazil; 
Cuestión Pública
, an investigative journalism newsroom in Columbia; 
Daraj Media
, a pan-Arab news platform; 
Initium Media
, a Singapore-based digital media outlet serving Chinese-speaking readers worldwide; 
Meduza
, a Russian-language news website covering Russia from Latvia; 
PumaPodcast
, an award-winning podcast production company based in Manila; 
Rappler,
 the online news website in the Philippines founded by Nobel Peace Prize laureate Maria Ressa; 
Raseef22
, an Arabic media network based in Beiruit; 
Rubryka
, a Ukrainian online media outlet specializing in solutions journalism; 
Scrolla
, a South African news start-up also covering Nigeria; 
The Conversation
, a media outlet based in Indonesia publishing expert-written news stories, opinion and analysis, and part of The Conversation network globally; and 
Zamaneh Media
, an exiled Iranian news publisher and broadcaster based in Amsterdam. Each of these organisations has fielded teams of 3 to 4 editorial and product staff for their AIJC project, often including senior leaders, and each has developed a project plan that reflects their pragmatism as well as the strategy of their organisation.
These projects cover a broad range of applications. Some are focused on using generative AI to adapt content to better suit the needs of new audiences, thereby broadening the reach of the organisation’s journalism. These content adaption projects are usually aimed at younger audiences, and typically seek to use generative AI models to create simplified, restyled or multimedia experiences from text articles. Other projects are focused on newsgathering, using large language models to analyse large streams or repositories of content to identify potential news stories. There are projects that seek to use structured data from investigative projects to develop a series of short summaries suitable for communicating those stories in a series of social media posts, projects that monitor the news environment in a media market and use LLMs to assess the impact of specific journalism in those societies, projects that seek to optimise news content for distribution and projects that seek to establish systems for designing, evaluating, managing and deploying news-related prompts for language models. The 12 AIJC projects also cover a broad range of technical approaches, with many being completely ‘no-code’ solutions, some using code-free ‘drag-and-drop’ online automation tools, some using a thin user interface over a prompt-based API-driven workflow, and some that include more complex software engineering by developers.
The AIJC programme is still several months from completion, and the participant teams are still deep in the design and development phase of their projects. Nonetheless there are already a few interesting observations emerging from the process. There seems to be a clear ‘before and after’ change that occurs when teams begin hands-on engagement with generative AI tools. Once teams sign up for accounts (especially for ChatGPT Plus), spend some time with the interfaces, make some initial attempts at prompting for familiar tasks, and start to evaluate the resulting output for themselves, then their awareness, excitement, motivation and abilities begin to accelerate. There seems to be no obvious relationship between the potential of a generative AI project and the availability of deep technical expertise. Instead it seems that familiarity with the tools and the ability to think clearly about the journalistic objective are more important, and that there are multiple ways of executing these projects in workflows without coding. Granted these newsrooms operate at relatively small scale (~15 to ~120 people) and are unusually flexible, but there may also be lessons here for larger newsrooms. It also seems that most of these newsrooms have no difficulty identifying a list of potential applications for generative AI, often well beyond their initially proposed project. In fact, the ability for teams to retain their focus to just their primary project objective may well become a significant advantage in developing a winning prototype! This is, of course, a self-selected cohort of newsrooms with a predisposition for entrepreneurial risk-taking, however it is hard not to notice the competence and confidence that develops as the teams define and begin their projects. Working with these newsrooms has convinced me that substantial innovation in newsrooms no longer requires dedicated labs, data science teams or special expertise, but merely a willingness to engage with generative AI tools and an ability to think carefully about the newsroom’s objectives, core functions and tasks.
As the newsrooms participating in the AIJC programme develop, complete and deploy their projects over the coming months we will all learn from their efforts. The judging rounds to select first the five finalists, and then the winning project, will provide expert assessments, and the teams themselves will publish their own descriptions of their experiences. Additional articles like this one will appear as the programme progresses, and the lessons learned from the collective effort across all 12 projects will be collected, synthesised and published in the final report. Follow-on projects and programmes will be explored. The global news industry is facing what may be its most significant technological disruption so far, and yet it is already clear that our new access to artificial intelligence can also be used in the service of purpose-driven journalism. Learning how to do that effectively, responsibly and sustainably is a critical, and exciting, challenge.]]></content:encoded>
</item><item>
<title><![CDATA[AI and News: What’s next?]]></title>
<link>https://generative-ai-newsroom.com/ai-and-news-whats-next-154fbeb6a646?source=collection_home_page----df04cefca135-----59-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Artificial Intelligence]]></category><category><![CDATA[Innovation]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Automation]]></category><category><![CDATA[Strategy]]></category>
<dc:creator><![CDATA[David Caswell]]></dc:creator>
<pubDate>2023-09-18</pubDate>
<atom:updated>2023-09-18</atom:updated>
<content:encoded><![CDATA[AI and News: What’s next?
David Caswell
Follow
28 min read
·
Sep 18, 2023
--
5
Listen
Share
Illustration generated by the Midjourney 5.2 text-to-image model, using the prompt “An abstract image representing the uncertain future of digital journalism in the age of artificial intelligence”
Innovation in journalism is back. Following a peak of activity in the mid 2010’s, the idea of fundamentally reinventing how news might be produced and consumed had gradually become less fashionable, giving way to incrementalism, shallow rhetoric and in some cases even unapologetic ‘innovation exhaustion’. No longer. The public release of ChatGPT in late November of 2022 demonstrated capabilities with such obvious and profound potential impact for journalism that AI-driven innovation is now the urgent focus of the senior leadership teams in almost every newsroom. The entire news industry is asking itself ‘what’s next’?
For many people in journalism the first half of 2023 was a time for asking questions and learning the basics of AI. What can ChatGPT actually do? What is generative AI? What is a language model? What is a ‘prompt’? How dependable are these tools? What kind of skills are required to use them? How fast is this technology improving? What are the risks? How much of all this is just hype? Many newsrooms went further, providing their employees and audiences with statements or 
guidelines
 describing how they intended to approach the use of generative AI in their workflows and news products. Some even began 
publishing
 a few 
experimental articles
 written by ChatGPT. Very few, however, have yet taken specific steps to pragmatically and routinely apply these technologies in their newsrooms. Change is in the air, but specific initiatives are harder to find.
Over the past six months I have had the privilege of spending significant time discussing AI with the senior leadership of more than 40 news organisations, ranging from scrappy digitally-native newsrooms in Asia, the Middle East, and Latin America to many of the largest global news providers based in the US, UK, and Europe. This access came as a result of my familiarly with AI-driven innovation in news, acquired from a combination of applied product innovation at large media companies in the US and UK, and from a small parallel academic career exploring the first principles of computation applied to news production and consumption. My journey with many of the questions now facing news organisations began in Silicon Valley more than a decade ago, and my exploration of the GPT family of language models began in 2019 at the BBC. My career focus — a relative niche until the arrival of ChatGPT — was suddenly of great interest to many people in the news industry.
This article is based on a distillation of what I have learned from these conversations. It assumes a knowledge of generative AI‘s general capabilities and potential, and examines some of the ways that large news organisations are thinking about its strategic and practical implications for their newsrooms. My intent here is to help advance the conversation beyond awareness and towards specific initiatives that can help move newsrooms forward in preparing for an uncertain AI-mediated future. Obviously any point of view regarding the application of AI to news is somewhat speculative, especially given the remarkable pace of advancement in AI functionality. Nonetheless, I believe that some clarity may be emerging.
Coming up with a 
list of things that your newsroom might be able to do using ChatGPT
 is fairly easy (Summarization! Rewriting in a simpler style!). It is much harder to clearly identify exactly what it is that you are trying to achieve with generative AI, why you are trying to achieve it, how you might plausibly achieve it at scale in routine and professionally managed operations, and whether that achievement will even continue to be relevant as AI fundamentally alters the competitive landscape in the coming months and years.
To frame this discussion I will begin with an overview of potential strategies for using AI in news, before turning to options for the practical deployment of AI in newsrooms, and the infrastructural and organisational requirements needed to support those options. I will then offer a point of view for changes that we might anticipate for the news industry in the longer term.
Product and editorial strategies for applying generative AI to news
Efficiency-focused strategies
: The most obvious opportunities for applying generative AI to news are in bringing new efficiencies to specific and familiar steps within the existing news production workflows supporting an organisation’s existing news products. This ‘
more-efficient-production-of-existing-products
’ strategy is attractive in its simplicity, but its benefits will almost certainly be short-lived because it assumes that the existing media environment will continue in roughly its existing form. It is increasingly possible, however, that the competitive environment, product offerings, production workflows and business models of news organisations will change, possibly radically, as use of generative AI becomes ubiquitous and as AI-based media products appear. There are early indications of this in the nascent generative search experiences offered by Google and Microsoft, in the user control of consumption experiences offered by well-funded news aggregators such as 
Artifact
 (founded by the founders of Instagram), in the declared intention to 
build a foundation language model
 around ‘X’ (or Twitter 2.0), and even in the early behaviour of tens of millions of early adopters of generative AI tools like ChatGPT and Midjourney, who have quickly learned how to use traditional media essentially as raw material for their own self-directed consumption experiences. A ‘
more-efficient-production-of-existing-products
’ strategy is clearly a reasonable place to start, but it does not fundamentally compete with new AI-enabled experiences and therefore may not remain sufficient for long.
User control of the consumption experience of text news articles in the Artifact App, including as summaries, as simplified language, as a poem, as ‘Gen Z’ language and as audio readings by celebrities
Product expansion strategies
: A more ambitious AI strategy for news organisations lies in reimagining and expanding the scope and scale of an organisation’s news products in ways that only become feasible using generative AI. This ‘
new-products-for-new-audiences
’ strategy will likely be more enduring than an efficiency-only approach because it seeks to actively accommodate the expansion of audience choice that generative AI enables. In essence this approach assumes that if generative AI is going to provide audiences with more control over their news consumption experiences (as pioneered by the Artifact app), then news organisations might as well offer that option directly. If news consumers can now control how they consume articles, then shouldn’t they be able to do that on a news publisher’s app or website? The advantage of this strategy is that it offers a way to compete within an emerging AI-mediated news ecosystem while continuing to exploit the advantages that news organisations already possess — brand, trust in editorial processes, habit, etc. Furthermore, that competition need not merely be aimed at existing targeted audiences but might now also be extended to entirely new audiences via new journalistic products that would have been editorially infeasible without generative AI. Even small newsrooms now have the option of offering comprehensive multi-media news products to diverse audiences with radically different consumption behaviours and preferences.
Differentiation strategies
: A more enduring strategy for news in the age of AI will necessarily be centred on differentiation and competitive advantage, offering exclusive news products that remain uniquely valuable to audiences even as the information ecosystem changes. This ‘
unique products
’ strategy is challenging because it will be audiences that decide on the relative uniqueness of a newsroom’s products, not newsrooms. Pursuit of differentiation must therefore be clear-eyed, unsentimental and data-tested — magical thinking about vague specialness requiring little effort to implement is unlikely to be helpful here. A differentiation strategy might still potentially fit well with some of the values and brand attributes of traditional journalism, especially if the information ecosystem begins to significantly degrade under the onslaught of AI-generated content. Some examples of familiar forms might include: proprietary news gathering using special access or special skills; audience trust gained from well-demonstrated verification and validation processes; richly contextual content produced from archives or from deep familiarity with subject matter; or perhaps a commitment to systematic coverage of particular subject domains using narrowly customized stories, such as 
local coverage
 or climate reporting. All these opportunities, however, will likely need to be comprehensively optimized using generative AI to remain competitive within an AI-mediated information ecosystem, even if their core value is differentiated. Developing forms of differentiation will likely be very challenging for many news organisations, especially for those whose current product is largely built on packaging commodity information, however there may eventually be no alternative.
Techno-editorial strategies
: A similar but even more ambitious potential strategy, probably available only to a few well-resourced news organisations or well-funded start-ups, is to seek to develop proprietary or competitively defensible information technologies and services centred on news — products that might resemble specialised intelligence tools more than traditional news products. This option, which we might call a ‘
techno-editorial product
’ strategy, would likely reverse the relationship between the product and editorial functions that currently exists in most news organisations, requiring editorial operations to support product and business development rather than the other way around. This kind of strategy might include provision of highly technical solutions such the systematic extraction of news events (and, critically, of news stories) from enormous streams of natural language text and speech, the creation and maintenance of proprietary datasets in story form, the systematic certification of that news, new tools for exploring and contextualising complex news, the extreme customization of news and of the experience of consuming news, etc. Such processes and tools might resemble those used by government intelligence organisations, platforms like search engines, or processes and tools already offered by news organisations targeting financial professionals, but with much broader coverage and audience appeal. Such a strategy would likely be a research-led, requiring substantial capital investment and justified by expected returns from high-value subscriptions. Such ‘
techno-editorial
’ businesses may not be well suited for reach-based news products but might nevertheless become significant components of an information ecosystem in which generative AI is ubiquitously deployed.
Training your own model
: As news organisations look for strategic responses to generative AI, there are also a few options that are often discussed but which might be less attractive than they first seem. One of those ‘
approach-with-caution
’ strategies is that of pre-training a proprietary language model using a newsroom’s archive. This is likely a bad idea for most news organizations for multiple reasons, including the difficulty in attracting the world-class machine learning talent needed to create a useful language model, the tiny size of the archives of even the largest news organisations relative to the training needs of a useful language model, and the substantial costs of periodic re-training, maintaining and operating a proprietary language model in the face of rapidly improving technology. Furthermore there are few clear benefits available from training a proprietary model, even if it were competitive in performance to a commercial or open-source language model. Concerns about data security, vendor lock-in or a need for niche functionality can probably be addressed in much easier ways, including contractually, via abstraction of user interfaces, using model fine-tuning or by using an open-source model. A proprietary model built on the same transformer architecture as most current LLMs will still hallucinate, even if the entire training corpus is accurate, and a news organisation is unlikely to match the efforts underway by technology companies aimed at making language models more accurate. Training a proprietary language model may make sense for some very specific use cases in some news organisations with special information products (for example a ‘
text-plus-financial-data’ multi-modal model
 at Bloomberg), but even in such cases a clear-eyed, careful consideration of the costs and benefits of such a strategy is essential.
Building a chatbot for archives
: Another ‘
approach-with-caution
’ strategy that some news organisations have pursued or considered is that of building a proprietary chatbot that enables conversational, interactive access to the organisation’s current news and archive. This option seems attractive given the success of ChatGPT, however in considering a news chatbot it is important to separately consider the interface component (i.e. interactive chat), the information component (e.g. news and its context) and the underlying technology used to deliver the experience (e.g. LLMs). Despite the rapid adoption of ChatGPT, text or verbal chat as an interface is still very far from broadly accepted as a way of accessing any kind of information, much less news. Audience analytics for news bulletins built for voice agents like Amazon Alexa and Google Home have been disappointing, and early attempts at LLM-enabled chatbots by media companies have delivered little traffic. This lack of success might be at least partly due to the relative inadequacy of news and news archives as a dataset powering such an interface, which for news organisations will be considerably smaller and more homogenous than the vast datasets that power ChatGPT or even search for that matter. Furthermore, from a technology perspective it is very difficult to replicate the fluid nature of communication with an LLM using the techniques currently available for constructing chatbots from archives, which typically involve some kind of embedding-based search operation combined with interpretation of the results by the language model. The result is often either far too many ‘I don’t know’ responses to specific queries, or references to ‘closest match’ archived articles in isolation from current context — experience that can’t compete with internet-enabled Chatbots like Bing Chat or Perplexity.ai.
The Perpexity.AI generative search experience of a news query, integrating journalism from multiple news and social media sources
Communication strategy
: Regardless of where a news organisation might be in its path towards a strategy for responding to AI, there is an urgent strategic need that requires early attention — that of communicating the organisation’s approach to AI to stakeholders, funders, staff and audiences. The urgency of strategic communication about AI for news organisations comes from the awareness that every individual connected with the organisation already has about the abilities of these tools and their potential for dramatic disruption. Most people in the news industry are already playing with the tools, reading the reports, assessing the potential and asking the obvious questions. It would, of course, be ideal to patiently assess the situation, carefully devise a strategy, produce a plan for executing that strategy, and then communicate that strategy and plan — but that may not be possible to do quickly. A more practical communication strategy at this stage might comprise of acknowledging the situation, articulating how the organisation is engaging with and learning about AI, providing clear guidelines for its early or limited use, demonstrating new approaches to innovation, signaling adaptability and generally preparing for change. A tone of optimism and excitement for the potential of AI to help raise journalism above commodity information and to make it more accessible to many more people is also appropriate.
A strategic path to continued relevance
: The common objective for almost all news organisations in navigating the coming AI-induced transformation is, bluntly, continued relevance. News organisations, whether funded by ads, subscriptions, public funds or philanthropy, will seek to remain sufficiently valuable to enough people to ensure that those funding mechanisms continues to function. The challenge of the coming transition to an AI-mediated information ecosystem is therefore to navigate a path that takes advantage of near-term opportunities for efficiency, medium-term opportunities for new products and services for audiences, and longer-term opportunities for re-imagining what news can become in a transformed information ecosystem. This path obviously comes with considerable associated risk — specifically the risk of losing existing value and of not being able to develop new value to replace it — and may therefore require a 
greater tolerance of risk
 by leaders in news organisations. Developing that greater tolerance for risk — a tolerance perhaps closer to that of the technology companies that initiated this transition — might be the most important strategic step that a news organisation can take.
Practical deployment of generative AI in newsrooms
From strategy to projects
: Any strategy for adapting a newsroom to an information ecosystem defined by generative AI is of little use without specific, practical projects that translate that strategy into useful outcomes. Identifying such projects is obviously difficult during this current period of exceptionally rapid change, because of the considerable risks of wasted investments, embarrassing missteps or potential damage to brands or trust. In addition, projects can only contribute meaningfully towards a strategy if they can be applied in routine, day-to-day operations, rather than merely as testable prototypes or demonstrators. To move forward, therefore, we need to identify categories of projects that might advance newsrooms towards an AI-ready future while minimizing risks, and also identify the infrastructure requirements needed to deploy those projects routinely at scale when that time comes.
Back-end projects
: A relatively low risk category of generative AI projects is purely back-end applications. These are applications with no direct audience-facing output, not even draft text, but which instead deliver their value to journalists or to the business. These can include efficiency-focused or quality-focused tasks such as tagging, other kinds of categorisation, headline and SEO suggestions, assembly of newsletters from pre-existing copy, copy-editing, brainstorming and ideation, early research, some analysis, etc. In addition to being relatively low risk, back-end AI applications are also relatively easy to implement as they are often ‘loosely coupled’ to news production workflow and infrastructure. Such applications can sometimes be managed by stand-alone tools disconnected from the primary publishing stack of the organisation, operated by specialised staff separate from the main editorial workflow.
Language task projects
: A more ambitious but still relatively low risk category of generative AI projects include those applications that produce draft text by modifying source text in some way. These applications use language models solely for ‘language tasks’ and should not introduce any information content into the draft that was not already present in the source document. They explicitly do not depend on the knowledge available to the model as a result of its training data, and they therefore reduce (but don’t eliminate) the risks of hallucinations, biases and other issues. Examples of language tasks include 
summarisation
, simplification, stylistic re-versioning, re-writing text for particular channels (social media, topic bulletins, etc), script-writing for audio or video, translation, etc. Language tasks can be done using any form of model access, such as the ChatGPT user interface, custom-built API-driven tools or even as new features integrated into content authoring and management systems. Language tasks can also be part of any strategy, including strategies based on efficiency, on new products or on differentiation. They are a fundamental category of journalistic task in an AI-enabled newsroom.
Knowledge task projects
: A higher risk category of generative AI projects are applications that produce draft text with information content that originates in the language model itself, rather than from a source document. These applications perform so-called ‘knowledge tasks’, because they are doing more than merely modifying language — they are true ‘authoring’ applications of language models. The increased risk associated with knowledge task projects comes from the significant potential for hallucinations, simple error from training data, biases, out-of-date context and other limitations inherent in language models. Nonetheless, if these risks can be 
managed
, knowledge tasks offer a substantial range of news products including provision of context for stories, 
explainer-like background content
, different interpretations of events based on historical context, and even full articles, especially on evergreen, commodity subject matter. Mitigating the risks associated with knowledge tasks requires editing processes designed to detect error and inappropriate content — non-trivial tasks, as evidenced by the 
difficulties experienced by CNET
 in publishing knowledge-based content from language models. Nonetheless, there are clearly many significant journalistic opportunities available from knowledge tasks, and these opportunities will likely increase as we collectively learn more about how to manage and edit their output and as we begin to use AI to help do that. As with language tasks, knowledge tasks can be integrated into workflows in different ways and can contribute to different strategies. They too are a fundamental category of journalistic task in an AI-enabled newsroom.
Medium-to-medium transformation projects
: A particularly ambitious category of generative AI projects for news are applications that transform information content from one information medium into another, for example from text into audio, from text into video or from text into graphical images. Unlike language tasks and knowledge tasks these applications typically depend on special-purpose medium-to-medium transformation models, often used in combination with general-purpose large language models within complex workflows. Such special-purpose models include speech-to-text models (transcription), text-to-speech models (synthetic voices), text-to-video models (synthetic avatars, automated generation of B-roll video, etc), text-to-image models and others. These tools are still at an early stage, however they are developing very quickly, are widely available, and already easily match human quality in many cases. Furthermore, the potential of this category of applications is likely to increase with the imminent arrival of so-called 
‘multi-modal’ functionality
 enabling richly descriptive image-to-text, video-to-text and other transformations. Examples of potentially high-value journalistic tasks that can be accomplished using these cross-media models include the automated or semi-automated creation of text articles from audio or video source material, the creation of audio and 
video news products from text articles
, the transformation of text articles into graphical stories or videos, the automated or semi-automated creation of podcasts from articles, etc. These tasks are most useful for a product expansion strategy. They can often be achieved using just model vendor user interfaces and so have the potential to enable relatively low-resource newsrooms to quickly offer multi-media content at significant scale. Some potential barriers to implementing this category of projects include the need for an editorial producer with experience in the output medium to ensure quality, the not insignificant cost of using the specialised models, and the challenge of distributing the same story in several different media.
Listening and monitoring projects
: The project categories described above all focus on novel ways of producing news products using AI, but practical projects focused on newsgathering are also viable using large language models. The term ‘generative AI’ has an obvious built-in bias towards the generation of media, but large language models and multi-modal models can also read, listen and soon observe at enormous scale, enabling entirely new ways of reporting news. The kinds of 
newsgathering tasks
 that these models can perform extend far beyond the earlier generation of ‘social listening’ tools, which usually just searched for keywords in the feeds of social media platforms. The ‘natural language understanding’ (NLU) abilities of LLMs can not only read, but interpret, evaluate, analyse, synthesise and summarise. Furthermore, they can do this not just with natural language text and speech, but also with structured data — as clearly demonstrated by OpenAI’s new ‘
code interpreter
’ add-on to their GPT-4 LLM, and likely soon with visual information in images and video. Projects based on NLU and ‘reporting at scale’ are already underway at newsrooms, including small newsrooms. Such projects may be most appropriate for a differentiation strategy and might enable newsrooms to build out and defend special abilities to report systematically on specific domains.
Advanced projects
: Our understanding of potential applications of AI in news is clearly still nascent, not only because of the rapid development of new functionality but also because we have just begun to explore the potential of these tools for news work. The projects described above are easily possible right now, with existing functionality, but there are also several ‘near frontiers’ of functionality that will likely open up significant new potential. One of these is the advent of multi-modal functionality, which is already available in limited forms in Bing Chat, MidJourney and soon GPT-4, and is likely to become increasingly powerful for applications that cross between language and visual information. A second near frontier is the advent of LLM ‘agents’, pioneered by small examples like 
AutoGPT
 and 
BabyAGI
. These approaches use LLMs to deconstruct high-level tasks expressed in vague terms into small, specific, actionable tasks that are then carried out by the LLM. They offer intriguing possibilities for automating some investigative journalism, and for scaling investigative reporting into an ongoing and systematic function.
The AI strategies and specific AI projects described here illustrate that newsrooms have tangible and specific options for moving forward towards an AI-mediated information ecosystem, but these strategies and projects are insufficient. Producing professional AI-enabled news products, using AI-enabled workflows, at sufficient scale to make a difference, day-after-day and month-after-month, requires something more. It requires infrastructure.
Infrastructure for an AI-ready newsroom
Old-school AI infrastructure
: In the decade prior to the rise of large language models, infrastructure for AI in news organisations meant something different than it does today. It meant data warehouses and data lakes, a well-structured and well-maintained metadata schema, libraries of embeddings, an expensive data science team, a large monthly AWS bill and a product roadmap focused on training small, specialised ‘machine learning’ models from scratch using small volumes of proprietary data. These roadmaps often included business-focused models predicting propensity to subscribe, propensity to churn or willingness-to-pay, journalist-focused models enabling a host of special functions useful for a handful of special stories, and audience-focused models such as semantic search and, of course, different kinds of recommender systems. This kind of AI infrastructure is still very valuable and useful, even if affordable only by a small number of elite news organisations, but it is very different from the infrastructure required to apply generative AI.
Professionalised prompt management
: A fundamental and permanent requirement for applying generative AI to news work is infrastructure than enables the professional development, testing and deployment of prompts. AI models will be a permanent part of the future of news, and controlling those models in the service of useful journalistic work will be a central function of editorial organisations. That control will be exercised through prompts. Whether back-end tasks, language tasks or knowledge tasks, and whether employed as part of an efficiency strategy, a product expansion strategy or a product differentiation strategy, all applications of generative AI in newsrooms are fundamentally dependent not just on the models used to execute them, but also on the prompts used to direct those models. Permanent, professional mastery of prompts does not look like journalists casually cutting-and-pasting from a dozen options in a Google Doc into ChatGPT, but instead looks like a professional ‘prompt-to-publish’ pipeline that enables systematic and quality-controlled management of every aspect of prompting, outputs, editing and deployment. This includes prompt design, assembly, evaluation and testing, ‘certification’, metadata, storage and retrieval, versioning, iterative improvement, usage tracking, analytics, output editing, training and more. Even just prompt design can involve explicit task definition, application of system roles and ‘custom instructions’, development of few-shot examples, management of context size, prompt templating, multi-prompt staging and more. Just evaluating the outputs of prompts applied to stochastic models fed by diverse source documents presents a combinatorial editing challenge unlike anything previously seen in news work. All of this requires infrastructure — databases, tools, user interfaces, schemas, integration, processes, analytics, training and documentation.
Interfaces between prompts and journalistic tasks
: Assuming that adequate ‘prompt-to-publish’ infrastructure is in place, a newsroom still requires an interface between this infrastructure and its journalists. For many journalists this is unlikely to be the raw prompt, which for useful tasks will likely be long, quite complex and probably ‘certified’ as tested and reliable according to some accepted quality control process. Instead most journalists will likely access prompts via buttons and controls that produce draft outputs for editing and refinement. Such interface infrastructure could consist of either stand-alone journalistic ‘co-pilot’ tools with separate experiences for prompt management and prompt use (already seen in 
Google’s ‘Genesis’ interface
, in various 
‘News AI’ tools from start-ups
 and in some in-house tools developed by newsrooms), a comprehensive integration into an organisation’s existing content management infrastructure or possibly even as an entirely new form of CMS designed specifically for AI workflows. Maintaining control of these tools, of the buttons and functions they provide, and of the prompts behind those buttons will likely be critical for journalistic independence in an AI-mediated news ecosystem.
The Newsroom AI beta interface, with buttons for journalistic tasks performed by a large language model
Infrastructure for Personalised Experiences
: Even with a coherent strategy, a set of applications that support that strategy and a prompt management process that professionalises the execution of those applications, the extent of AI-enabled news production will still be limited by the available content management, serving and distribution infrastructure. This is especially true of a product expansion strategy executed using language tasks or medium-to-medium transformations. Producing 5, 10 or 20 different variants of every story, suitable for a wide variety of audiences, situations and consumption contexts, might be far easier than actually serving each of those variants to the right user at the right time. This is a personalisation challenge, but one that is quite different from the usual interpretation of ‘personalization’ in most newsrooms — which tends to focus almost exclusively on personalised recommendations of one-size-fits-all content artifacts. This new requirement is, instead, for personalisation of the story experience, and it therefore requires infrastructure that can store, select and serve different variants of a story to different users or different user segments in different situations — in addition to continuing to recommend relevant stories. A simple form of experience personalisation, being pursued by several newsrooms, is to set up a separate channel or even an entirely separate brand from which to serve a newsroom’s stories in new AI-generated formats. A more advanced form is to offer direct user control over their consumption experience, for example in the way that the Artifact news app offers article consumption as summaries, simplified text, emoji stories, poems or as audio readings by celebrities (see illustration above). A more complex form of experience personalisation is to automatically adapt a user’s experience using behavioural data and contextual signals. Each of these approaches can open up access to a newsroom’s journalism to more people, but each also requires infrastructure that can support the management and serving of story variants.
Newsgathering infrastructure
: Infrastructure is also required to take advantage of opportunities to use LLMs to substantially scale newsgathering by monitoring and analysing large volumes of source documents — perhaps as part of a differentiation strategy focused on competitively defensible coverage of a particular news domain. A starting point is infrastructure that enables constant access to source material, which might exist in different text formats, or in audio or video form that must first be converted to text transcripts using AI speech-to-text tools, or possibly even in large datasets that are frequently updated. Such ‘monitoring’ infrastructure might range from a simple list of sources to a complex web crawler that continually ‘reads’ portions of the internet to maintain an awareness of domain events. The monitoring that such infrastructure performs will depend on a system of prompts and a system that manages the summaries and assessments that are the outputs of such prompts. This all requires databases, data schemas, access tools and filtering functionality. Such functionality can be built at a small scale and used as a supplement to a manual monitoring workflow, and several newsrooms are already doing so.
Infrastructure flexibility
: Despite our efforts to identify a useful way forward amid the uncertainty of the present moment, there remains a real possibility that AI might enable an entirely new interface to journalism, delivering experiences of news that are not centred on discrete text, audio or video artifacts. We see early signs of this in the form of chat interfaces, generative search and early conversational voice interfaces enabled by language models. Despite the caution provided above regarding an archive chatbot strategy, and despite evidence from audience research indicating a preference for passive rather than active news consumption, it is still possible that we may soon be interacting with news in entirely new ways. Given this uncertainty, it may be useful for news organisations to re-examine their technical architectures and infrastructure strategy from the perspective of an increasing need for flexibility. This might involve re-examining build-vs-buy decisions, refactoring brittle, tightly coupled architectures, or even attempting to identify various possible product scenarios and a path to infrastructure that could support those scenarios if they develop. Any such re-evaluation should consider the many new options for AI-enabled software architectures and AI-enabled software engineering that are appearing. The increasing need for flexible infrastructure is accompanied by new AI-enabled techniques that may make it easier to design, build and maintain such infrastructure.
Organisational structure for AI-empowered teams
News organisations will need to operate differently
: A news organisation with a coherent, well-articulated strategy for responding to generative AI, delivered via a portfolio of well-conceived applications supported by a professionalised prompt management process and delivered to audiences via a personalised publishing stack, would clearly be well-placed to adapt to an AI-mediated information environment. But such an organisation would also likely need to operate differently from a traditional digital news publisher, and its organisational structure would probably need to change substantially in order to support those differences. Furthermore, the skills and talent required to operate successfully in this environment are likely to also be different. Discussions of likely changes in the structure of news organisations often, and understandably, focus on the potential for AI to either replace or augment traditional jobs, however the reality is likely to be more complicated than that. It is, I think, quite certain that many newsroom tasks will be either replaced or made moot by AI, but it is also certain that many new tasks will appear. It is obviously difficult at this stage to predict how old and new tasks will be assembled into individual jobs, teams and department-level functions, and different newsrooms with different strategies will clearly do this differently. Nonetheless we can, I believe, make some informed speculation using the likely drivers of organisation change as a starting point.
Drivers of organisational change
: If we consider what generative AI can do, how it is being used for news applications in a nascent way (see above) and how news organisations talk about it then we can make some plausible assumptions about how news organisations might change in response to it: It is likely, obviously, that technology will play a more central role in news organisations than it currently does; It is likely, less obviously, that the accessibility of generative AI will cause the use and control of technology to be dispersed throughout the organisation rather than concentrated within a team of specialists. It is likely that news organisations will place a higher priority on adaptability and constant entrepreneurial innovation; It is likely that substantial differences in the productivity of teams and individuals may appear, caused by differential effectiveness in using AI; It is likely that, as competition for audience attention increases, news organisations will increasingly value a deep understanding of audiences and their information needs. It is likely that, at least for some categories of news, news organisations will focus less on producing individual stories by hand and more on overseeing systems and processes that produce or help produce stories. The collective organisational influence of all these assumptions can perhaps be summed up in a single word: autonomy. Those teams that adopt and master AI will be able to do much more, with many fewer dependencies on other parts of the organisation.
An AI-native news organisation
: The enablement of increased autonomy by AI suggest to me that the productive units of AI-native news organisations might be small, AI-empowered, multi-disciplinary and self-directing teams operating relatively independently from each other and each focused on serving a specific audience or audience need. In this scenario the organization itself becomes somewhat federalised — providing an environment within which self-directing teams can be productive and impactful but not directing their work. The federal organisation provides brand, values, certification of quality, monetization, financial stability, enabling infrastructure, training and, of course, general strategy. The atomic teams provide adaptability, fast decision making, audience and competitive awareness and, of course, routine production of valued content via AI-augmented workflows. Implementing such a federalised organisational structure would clearly be challenging for many news organisations, not least because of that eternal and vaguely-defined bugbear of newsroom change management — culture. A practical, near-term response being explored by some pioneering newsrooms is the possibility of becoming a ‘two-speed organisation’, taking advantage of the new autonomy available from AI to set up teams that are loosely coupled to existing workflows. A longer-term response, already underway at several newsrooms, is to re-evaluate hiring and performance criteria to emphasis the skills and talents needed to form a more autonomous AI-empowered culture.
Skills and talent
: The most valued skill in an AI-empowered news organisation will likely be the same as it has been in traditionally configured news organisations. Editorial judgement — the ability to maintain a keen awareness of the deep informational needs of an audience or society, identify stories that meet those deep needs, verify and contextualise those stories, and then communicate them to audiences in clear and engaging forms — will probably remain the foundation of journalism. How editorial judgement is exercised, however, may change in ways that requiring substantial new skills and different talents. These might include an ability to work with abstractions and systems, to analytically understand audiences and their needs, to engage with complexity, and to remain curious and to learn continually. Organisations will obviously need to identify, support, incentivise and retain their most innovative and adaptable employees, but they will also need to supply those AI-empowered employees with leadership that is perhaps more entrepreneurial, more skilled at motivating and coaching, and perhaps less managerial or political. Specific technical skills with specific AI tools or models may, surprisingly, be less important due to the near-universal accessibility of the interfaces. Merely operating AI will likely be much easier and therefore less valuable than wielding it skillfully as a genuine superpower.
So, what’s next?
The AI moment
: The development of generative AI has placed journalism at the cusp of significant change, variously equated to the iPhone moment, the birth of the internet and even the appearance of the printing press. The significance of the moment has been understood and appreciated by the senior leadership of most newsrooms, and many are already moving forward towards specific initiatives and experiments aimed at preparing for an AI-mediated future. Considerable attention is rightly being given to potential harms, to the ethics of using AI in journalism, to influencing regulation and legislation, to the potential for AI-created misinformation and disinformation, to education of audiences and ‘AI literacy’ and to the development of early guidelines to orient news organisations as they begin this new transformation of their industry. This attention is necessary and valuable, but it is not enough. Journalism must also engage with these new tools, explore them and their potential, and learn how to pragmatically apply them in creating and delivering value to audiences. There are no best practices, textbooks or shortcuts for this yet, only engaging, doing and learning until a viable way forward appears. Caution is advisable, but waiting for complete clarity is not. So-called ‘
second mover advantage
’ is only available to those who are well-prepared to move when the time comes.
The next information ecosystem
: Looking further ahead, the need for hands-on familiarity with applied AI in journalism becomes even more critical because of the likelihood that the entire information ecosystem within which journalism exists will undergo transformation. What will journalism look like, for example, in an environment in which text, audio and video is fluid and malleable to the preferences of each individual consumer? What should the tangible output of a newsroom be in an environment in which that output is consumed primarily by machines? How will a coherent record of news — an archived ‘first draft of history’ — be maintained in such an environment? What might news become when useful reporting can be done on almost every word of text or speech, or every byte of data, produced in public by society? How will newsrooms capture value from their work in such an environment? What will that work be? These questions, and others that are similarly fundamental, may not become relevant for years, or even decades, but merely participating meaningfully in that discussion will require newsrooms to possess far more tangible expertise of AI-augmented journalism than any now possess. Progress in the face of uncertainty depends on developing and maintaining options, but options require situational awareness, and situational awareness comes from authentic engagement with the environment. For journalism that environment will almost certainly be shaped by AI.
In this article I have attempted to describe a few ways in which news organisations might build on the awareness of AI that they have developed since the launch of ChatGPT with specific strategies, projects, infrastructure and organisational changes. These suggestions are my own interpretation of what I am observing and hearing, and they are by no means exhaustive or complete. It would be reasonable to expect people who staff or lead news organisations to exhibit frustration or even resentment at the prospect of even more impending change, but this is not what I have observed. Instead the predominate tone of my conversations about AI has been one of optimism and excitement, largely at the opportunity presented by AI to further realise an ideal of journalism that motivates many of us who work in and around newsrooms. If you could bring that ideal to life, without regard for the scarcity of resources, where would you start?
(This article was written entirely by hand, without the use of AI)]]></content:encoded>
</item><item>
<title><![CDATA[Finding Evidence of Memorized News Content in GPT Models]]></title>
<link>https://generative-ai-newsroom.com/finding-evidence-of-memorized-news-content-in-gpt-models-d11a73576d2?source=collection_home_page----df04cefca135-----60-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Copyright]]></category><category><![CDATA[News]]></category><category><![CDATA[Memorization]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-09-05</pubDate>
<atom:updated>2023-09-05</atom:updated>
<content:encoded><![CDATA[Finding Evidence of Memorized News Content in GPT Models
Nick Diakopoulos
Follow
6 min read
·
Sep 5, 2023
--
2
Listen
Share
Effectively training large language models (LLMs) like ChatGPT requires a substantial amount of data. Model developers like OpenAI, Meta, and others collect data from many different sources, including Wikipedia, books, and online 
crawls of web pages
. Some of this data is 
undoubtedly copyrighted
. And there are 
lawsuits
 that have been filed. Recognizing that their content has also been fed into these models, some news organizations have begun 
negotiating and signing license agreements
, 
calling for more transparency
, or even 
thinking about their own lawsuits
.
For the most part there’s little transparency from model developers about what data they used to train a model. New legislation in the EU 
may potentially address this
 by requiring developers of such models to include a summary of copyrighted data used to train a model. But in the meantime, it’s hard to know if your site’s data has been ingested. A 
recent report
 from the Washington Post evaluated one dataset, called C4, that is known to have been used in training some of Google’s and Meta’s models. The “News and Media” category of sites was the third largest in the dataset, and the New York Times alone accounted for 0.06% of all data in the dataset. The 
original article describing GPT-3
 also talks about how the model was trained on the Common Crawl which is the same root dataset underlying the C4 dataset investigated by the Washington Post. GPT-3 was further trained on a dataset constructed by looking at millions of links from Reddit, which we can assume included a lot of news. It does seem likely that there’s a substantial amount of news content that has been ingested by popular models like GPT-3 or 4.
In the absence of transparency, how might you determine if your site’s content has been incorporated into a model like GPT-3 or 4? In this post I’ll demonstrate a method that can be used to provide evidence in some cases.
Previous 
research
 has explored how models can memorize and regurgitate verbatim copies of text strings that they’ve ingested during training. In particular, the research has shown that (1) larger models, with more parameters, are more likely to memorize, (2) memorization is more likely the more a text has been repeated in the training data, and (3) memorized data is more discoverable if the model is prompted with longer strings of context.
These findings suggest concrete strategies for discovering memorization. First, we should see more memorization in GPT-4 than GPT-3 as it is a larger model, although this may be complicated by 
observations that GPT-4 is non-deterministic
. Second, we should look for memorization of text that has been repeated many many times in the training data, such as boilerplate or repeated phrases that may be unique to a publication. We know that training data cutoffs were in 2021 (June for GPT-3 and September for GPT-4) and so any memorized text would come from before those cutoffs. And finally, to discover memorization we need to prompt the model with sufficient context so that it completes the text using material it has memorized.
I first set up an experiment to test whether GPT3 and 4 had memorized some boilerplate from the New York Times. The NYT Opinion section includes the following text at the bottom of almost every opinion article: “The Times is committed to publishing a diversity of letters to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips. And here’s our email: 
letters@nytimes.com
.” This string is unique to the New York Times and it appears before the training cutoff dates in 2021.
To test for memorization I prompt the model with a prefix of the string of varying length (e.g. “The Times is committed to publishing”), and check whether the model outputs the rest of the string such that it matches the original. A match (or perhaps even a very similar completion) could be said to reflect memorization and demonstrate the likelihood that the training data included that text (probably many many times). I prompted the model once with a temperature of zero to get the most deterministic output, and then prompted the model 20 times with a temperature of 0.7 to understand variability. I tried both GPT-3 (model name: “text-davinci-003”) and the latest version of GPT-4 (model name: “gpt-4” as of Sept 5, 2023)
Here is an output completion from GPT-4 that demonstrates the full string was reproduced by the model, indicating memorization of the content.
OpenAI’s GPT-4 Model Completes a Boilerplate String from the New York Times.
There were many variations of the input prompt that produced the complete memorized output, though the model appears more likely to return the memorization when the input prompt includes a complete sentence. There were several different lengths of prefix that produced the memorized string though it was important to systematically test each variation as some prefixes triggered the memorization and others didn’t. GPT-3 did not produce as many complete memorizations. It was often able to return the memorized text up to the end of “Here are some tips.” however in only one test it did then complete the next sentence (“And here’s our email: letters@nytimes.com.”) accurately. 
These results suggest to me that GPT models from OpenAI have memorized text from the New York Times, and that GPT-4 demonstrates this memorization more readily than GPT-3.
 Furthermore, systematically testing different input prompt context lengths was important to finding instances of memorization.
I tried this with some boilerplate from other publications. For instance, the Washington Post includes the following text at the bottom of the 
Miss Manners column
: “New Miss Manners columns are posted Monday through Saturday on washingtonpost.com/advice. You can send questions to Miss Manners at her website, missmanners.com. You can also follow her @RealMissManners.” In this case I did not observe an exact complete match, but GPT-4 got very close. When prompted with “New Miss Manners columns are posted Monday through Saturday on washingtonpost.com/advice. You can send” it completed “questions to Miss Manners at her website, missmanners.com. You can also follow her on Twitter @RealMissM” which, besides truncating the Twitter handle, also includes the text “on Twitter” which doesn’t appear in the original. GPT-3 didn’t do quite as well. When prompted with the same prefix it completed: “questions to Miss Manners at her website, missmanners.com, or to her email address, dearmissman” which is a partial match, but includes a hallucination about an email address that wasn’t in the original.
Finally, I looked at the tagline that The Economist includes at the bottom of stories in its 
Science & Technology section
: “This article appeared in the Science & technology section of the print edition under the headline [HEADLINE]”. For the string tested I excluded the actual headline since that wouldn’t have been repeated in the training data. In this case I found evidence of memorization with GPT-3. When prompting GPT-3 with “This article appeared in the Science &” the model completed the rest of the string perfectly. This was also the case for longer prefixes. However, GPT-4 did not produce the original string when prompted with any prefix.
Overall, the results here are somewhat varied. In two of the cases GPT-4 produced more memorized strings than GPT-3, as was expected according to the previous research. But the fact that in one case GPT-3 produced a memorization and GPT-4 didn’t indicates that it still makes sense to check both models. Based on my observations it also seems clear that you need to try many different prefix lengths in the prompt to see if the model will complete a memorization based on that prefix. Extremely short prefixes are unlikely to result in a memorization, though in a couple of the cases 6 or 7 words was enough. Your test string will need to be long enough to explore a range of prefixes as sometimes it could take a longer prefix to trigger a memorization. Since the strings that were observed as memorized are the kinds of boilerplate strings that would be repeated many times in the training data, this could be what’s driving the memorization, and would be consistent with prior research. These bits of boilerplate may be repeated not only because they appear at the bottom of different stories by the same publication, but also because they have been excerpted on other websites that appear in the training data.
If you’d like to test memorization in GPT-3 and 4 with your own string, you can use 
this Colab Notebook
. Just input your OpenAI key and the string you’d like to test. Let me know what you find!]]></content:encoded>
</item><item>
<title><![CDATA[Generative AI in Media & Journalism: Think Big, But Read the Small Print First]]></title>
<link>https://generative-ai-newsroom.com/generative-ai-in-media-journalism-think-big-but-read-the-small-print-first-375f2ecb1256?source=collection_home_page----df04cefca135-----61-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Terms Of Use]]></category><category><![CDATA[Ai Act]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[Natali Helberger]]></dc:creator>
<pubDate>2023-07-18</pubDate>
<atom:updated>2023-07-18</atom:updated>
<content:encoded><![CDATA[Generative AI in Media & Journalism: Think Big, But Read the Small Print First
Natali Helberger
Follow
18 min read
·
Jul 18, 2023
--
Listen
Share
Image created with Adobe Firefly.
Terms of Use — typically hidden away at the bottom of a page, barely readable with the naked eye and notorious for their use of legalese– are where the use of generative AI systems is currently being governed. The Terms of Use of generative AI providers such as Open AI, Midjourney, Stable Diffusion, and others 
lay the ground rules
 for the responsible use of generative AI, instruct downstream users how to mitigate potential safety risks and risks to the realisation of fundamental rights, and create 
binding legal obligations
. Recent legal initiatives such as the 
draft AI Act
 are likely to increase the importance but also the weight of the Terms of Use and Usage policies because it is here that providers of generative AI instruct users how (or how 
not
) to use the technology and distribute responsibility. At the same time, the AI Act is also likely to trigger closer scrutiny and trigger a debate on the fairness of some of those terms.
In the following post, we will critically and thematically review the Terms of Use and usage policies of five generative AI providers and recommend a number of terms to the attention of media organisations that are planning to use the models.
[1]
 We included proprietary and non-proprietary, open systems and platforms in the selection, including 
Open AI
, 
Midjourney
, 
Anthropic
, 
Hugging Face
, and 
StabilityAI
.
Before we get into the specifics, here are some of our key take-aways
:
Before starting to use or integrate a particular generative AI model media organisations would do well to carefully read the Terms of Use. Different providers make their models available under different conditions.
Of particular importance are the conditions on who is liable for the output of the model, who owns the rights to any prompts or contents shared with the model, the output, and to what extent the provider reserves the right to use any pictures, articles, and other content from a media organisation to further train the model. Some providers of generative AI also prohibit the use of their models for particular use cases, or stipulate additional conditions for the generation of news content (such as OpenAI).
Some conditions are potentially problematic from the perspective of media organisations, and merit a broader discussion within the sector of what acceptable or unacceptable commercial practices are, also with view to the pending AI Act.
Open-source models typically offer media organisations the possibility to exercise more control, have more transparency and adjust a model to their own needs and preferences, provided the necessary in-house skills are present, or there is a commitment to invest in such skills.
Using generative AI responsibly includes choosing a responsible provider of generative AI. Additional information to look for includes any actions that a particular provider has undertaken to address issues of bias, support for research, the lawfulness of training data, to reduce the ecological footprint of the model, and guarantee fair labour conditions of information workers.
Fair division of responsibility
The division of responsibility between providers of generative AI models and downstream users for compliance with the legal obligations under data protection law, copyright law or the pending AI Act is not always clear-cut and will require in many instances cooperation and a 
workable division of tasks
. For example, for media organisations to be able to do a risk assessment or ascertain that content generated by an LLM does not infringe upon copyrights or data protection rights of others, they require information from the provider on how the model has been trained and what initiatives were taken to account for the rights of third parties. Vice versa, in order to be able to demonstrate that the copyrights of a media organisation or journalists have been infringed upon in the process of training the model, the media organisation depends on 
the cooperation of the developer
 of the model to prove that the latter had access to that particular piece of content. Ideally, the contractual agreements between providers and downstream users therefore 
include provisions
 on the mutual rights and obligations of parties, including issues around transparency, access to data, support, but also mutual assistance and cooperation when it comes to conformity assessment, risk management and legal responsibility. The need for a fair division of tasks and some form of cooperative responsibility is 
reportedly
 also an important point of departure in the European Parliament’s 
compromise text
, demanding a division of responsibility that reflects the different parties’ levels of control.
An initial review of the Terms of Use of the five providers of generative AI reveals that the contractual reality is far away from this ideal of mutual assistance. Typically, the Terms of Use determine that downstream users bear full responsibility for making sure their use of the technology complies with legal requirements from e.g. data protection and copyright law or the AI Act and is otherwise responsible. All of the Terms of Use examined contain the extensive disclaimers of responsibility typically known from other software contracts. In addition, all include indemnification clauses obliging the downstream user to defend the provider against any claims that may arise from their use of the model, typically at their own costs. None of the terms analysed addressed the situation that safe use of the model or compliance with the applicable laws by downstream users is not possible without the cooperation of the developers of the models themselves. None of the Terms of Use examined included any language of assistance, described a responsibility on the side of the providers of the models to cooperate or any rights to transparency on the side of downstream users. A notable exception was the Terms of Use from Hugging Face, committing to “all reasonable assistance and cooperation in connection with the defence and settlement of the claim, at its own expense”, albeit limited to a situation of litigation.
And in case of questions? Stable Diffusions FAQs are hopefully not symptomatic: “If you can’t find what you’re looking for, email our support team and if you’re lucky someone will get back to you.”
Dealing with dependency
A potentially problematic term from the perspective of downstream users is the 
lack of guarantees regarding the continuation and support of the model
, particularly to the extent that a media organisation is deciding to rely on external technology instead of building their own model. Midjourney’s Terms of Use are most outspoken and warn all media organisations that want to rely on that model: “Please do not create any dependencies on any attributes of the Services or the Assets. We will not be liable to you or your downstream customers for any harm caused by your dependency on the service.” In other words, Midjourney reserves the right to change or even cease the service, without an obligation on the part of the developer to even issue a prior warning. While understandable from a business point of view, the fact is that seeing the resources needed to build a generative AI model, at least today only a few companies will have the capacity, finances, skills and data to do so. The rest will depend on the use of generative AI models by the few companies that are in a position to deploy their models. This does not only mean that large parts of the downstream market may end up building their futures on extremely unstable ground, it also means that without some safeguards to transparency, due notice and continuation, downstream providers will have little to no negotiation power, and find themselves in a constant state of economic uncertainty. The situation echoes earlier concerns about problems with the lack of transparency and predictability of some of the decisions of social media platforms, including the Facebook Newsfeed, and the 
consequences
 for those that depend on the platforms — a situation that has ultimately resulted in regulatory action in the EU in the form of the 
Digital Markets Act
.
The lack of predictability and guarantees is potentially further exacerbated in a situation in which Terms of Use are used as an instrument to chill competition and alternatives. An example is the inclusion of non-competition clauses (which can be found e.g. in the Terms of Use of all the proprietary models examined). For example, Open AI does not allow to use the output of the model to develop models that compete with Open AI, and according to the Terms of Use of Anthropic, using prompts and results from the model to train other models will be considered fraudulent use in breach of the Terms of Use.
To the contrary, open-source models leave their users 
more possibilities
 to scrutinise the model and data sets, adjust existing open source models, and exercise control of their own. Doing so requires a certain level of in-house technical expertise, or the willingness to invest and build up such expertise. From the competition standpoint, encouraging investment in skills and alternatives, and reducing dependency by leveraging platforms for open-source models are worth serious consideration in case the lack of predictability and dependency on external providers is a concern.
Research, auditing and critical reporting
Importantly, and rather ironically, all proprietary models examined have extensive language that bans the reverse engineering or scraping of the model and its output — ironically because scraping the public internet is what has allowed training some of those models in the first place. Open AI does not allow reverse assembling, compiling, decompiling, translating or otherwise attempting to discover the source code or underlying components of models, algorithms, and systems; or the automated extraction of data or output, including scraping, web harvesting, and web data extraction. Similarly, Midjourney and Anthropic ban any attempts at reverse engineering and scraping.
Practically, this makes auditing and research into the system to understand its affordances and limitations difficult and potentially a breach of the Terms of Use, depending on how the provider will interpret these restrictions. Also, testing the systems for critical reporting about their functioning and investigative data journalism can potentially fall under the contractual restrictions. Clauses like these echo very similar concerns regarding the clauses of social media platforms that restricted access to data and that were the basis for attempts of platforms to shut down research projects (
here
 and 
here
). In Europe, these debates resulted ultimately in the adoption of 
Article 40 DSA
, which however, only binds Very Large Online Platforms and Very Large Search Engines in the sense of the DSA, and only benefits researchers, not journalists. The restrictive provisions on reverse engineering could be another important reason to choose an open source model, at least for downstream users that are not willing to use a model that they have no means to understand or audit, or that wish to invest in building in-house skills and capacity.
This is not to say that providers of generative AI do not engage in, and stimulate research. Anthropic, for example, makes research into responsible development of generative AI an explicit part of its mission. And OpenAI writes in its 
Charter
: “We seek to create a global community working together to address AGI’s global challenges”. But it is the providers of the models that dictate the conditions of research. Besides some sporadic blog posts and white papers, providers like Anthropic and OpenAI seem to keep most research in-house. An important element of OpenAIs strategy to addressing generative AI’s global challenges is conducting and 
publishing details on its alignment research
 and 
safety research
, as well as running a 
researcher access program
 in which third parties can apply for credits to use the API. Grounding those models in (their own) critical research is certainly to be applauded, as is supporting third party research but it is also important to flag that the OpenAI research program or its Terms of Use do not give an independent right of access for research purposes. It is thus entirely up to the discretion of OpenAI to decide what research to support or not.
Managing Data Rights on Inputs and Outputs
Over the past months, we have seen 
controversies around the IP rights
 of the original creators of material that the models have been trained on, as well as concerns around data protection and the extent to which the models were trained on 
personal data without consent
. The use of generative AI is as privacy-friendly as the data it has been trained on, and problems with the lawfulness of the content used to train the model can trickle down into downstream uses (e.g. in the case of training the model on contents that are protected by 
copyright law
). Similarly, if equality and diversity are important editorial values, the ability to realise those values also critically depends on the external technology provider. Therefore, responsible use of generative AI starts with 
responsible procurement and selecting providers
 that can substantiate how in the development and subsequent deployment process, the rights of data subjects and right holders are taken into account.
Again, most providers are relatively silent on this point. Some Terms of Use mention the possibility for original right holders to file a complaint. Hugging Face offers a dedicated 
DMCA email address
 for rightsholders, and also Midjourney 
foresees a notice-and-take-down procedure
. In both cases, the burden of proof that the model has been trained on copyrighted content without consent is with the respective rightsholder. It will be interesting to see how the European Parliament’s 
recent suggestion
 to mandate a publicly available summary on the use of copyright-protected works for training the models may help rightsholders to arrest their claim.
While such a distribution of responsibility may be common practice elsewhere, in the case of generative AI and without transparency about the training data, making and documenting such a claim may impose insurmountable burdens on the right holder. Neither Midjourney, Open AI nor Anthropic volunteer any transparency about the training data, at least not on the website or Terms of Use. In contrast, Stable diffusion 
explains
 that the model is trained on the LAION dataset. Maybe most detailed and possibly the best practice so far (at least among the providers examined) is HuggingFace’s approach to its open-source Bloom model. Hugging Faces provides 
detailed information
 about training data, but also foreseeable users, model objective and architecture, risks, and limitations.
Then there is the question of how the Terms of Use handle ownership in the data acquired
 through
 the use of the system. For example, who has the right to content that a media organisation uploads into the system, for example to make summaries or rewrite in a different format? Again, for media companies planning to use generative AI to (co)produce output this can be a relevant question, both in terms of their ability to control the publication of the output they create, as well as in terms of confidentiality of the prompts or content uploaded. Here, the picture is diverse. At one end of the spectrum is Stable Diffusion: “Images created through Stable Diffusion Online are fully open source, explicitly falling under the CC0 1.0 Universal Public Domain Dedication.” In other words, users of Stable Diffusion’s image generator must be aware that the images they create end up in the public domain by default. Similarly, Midjourney stipulates that non-paying users cannot claim any ownership, either in the prompts or in the output generated. Instead, Midjourney issues a CC Noncommercial 4.0 Attribution Licence, meaning that third parties are free to use and remix images and prompts, and by default, all content generated is publicly viewable. If Midjourney users wish to exercise more exclusive control, they need to sign up for a paid Pro Membership.
At the other end of the spectrum are providers like Open AI that assign users full ownership over prompts and output, including for commercial uses. Interesting is OpenAI’s definition of when exactly a user owns the output. Whereas under copyright law, it is the creator of an original work that owns the IP rights, in the realm of OpenAI the owner of a prompt is the one who requested and prompted the system, unless someone else used the same prompt. Since it is impossible to know if other users used the same prompt this condition does not contribute to much legal certainty. In addition, the debate is still ongoing on how Open AI’s “right to the prompt and its output” relates to competing claims of, for example, the original creators whose content the generated output may resemble.
At the same time, some of the Terms of Use try to carve out their own exception for training data. According to OpenAI’s Terms of Use, for example, content collected from the API is not used for training the models in principle, but users can opt-in to share data. In contrast, in the case of web-based services such as ChatGPT and DALL-E, the default is that data input can be used to improve the model unless users opt out. In other words, if a media organisation would rather not like its content or prompt to be used to train the system (by default), it should access the model via the API. A variation on this is the Terms of Use from Anthropic or Hugging Face that in principle recognise that users retain all rights to prompts and results but that nevertheless reserve the right to use all 
publicly available
 prompts and results to train their systems (Anthropic) or those of others (Hugging Face). This could mean, for example, that content created with a generative AI model and published online could be used for training purposes. On the far end of the spectrum is Stable Diffusion, reserving the right to train their model on all kinds of data and offering no opt-in or opt-out for the LAION 5b model data because the model “is intended to be a general representation of the language-image connection of the Internet.”
Defining responsible use of AI
Next to delineating rights and responsibilities in the downstream relationship, the Terms of Use of generative AI providers will also have a critical role in defining what responsible or irresponsible use of generative AI is, not only in relation to professional users but also end-users. Being at the centre of the debate around possible risks to society, arguably, generative AI providers are not only well positioned to identify and mitigate societally harmful uses of AI by both professional and individual users, they arguably also have a societal obligation to do so. Apart from that, terms of use and usage policies are also important means of risk management and governing the responsible use of AI.
Accordingly, the creators of the RAIL (Responsible AI Licence) have
 argued
: “licenses could serve as a useful strategy for creating, expanding, and enforcing responsible behavioural norms given the limitations of self-regulation and governmental legislation. […] we advocate for the use of licensing as a mechanism for enabling legally enforceable responsible use.” A case in point is Stable Diffusion that explicitly characterises usage restrictions as a tool to govern downstream responsibility: “Referring to the downstream responsible use, we added use-based restrictions not permitting the use of the Model in very specific scenarios, in order for the license to be able to enforce the license in case potential misuses of the Model may occur.” It is worth mentioning that Stable Diffusion also determines that the original use-based restrictions must also be included — ‘at minimum’ — in downstream derivative versions. And in a (leaked) 
policy response
 of Open AI to the AI Act, the company made repeated references to its terms of use as one of the ways in which Open AI addresses concerns around the responsible use of its model.
Having said so, the potential of Terms of Use to set legally enforceable conditions of responsible downstream use critically depends on the way those conditions are defined. The Code of Conduct of Midjourney is relatively short, for example, and can be summarised by its first rule “Don’t be a jerk.” In other words, Midjourney primarily appeals to the responsibility of end-users to respect the rights and interests of others and also leaves it to the imagination and responsibility of users to define what that means. In practice, this also means that legally enforcing these conditions will be difficult, if not impossible due to their vagueness. In contrast, all of the other providers examined provide more or less extensive attempts to identify prohibited uses and use cases. OpenAI, for example, includes a very detailed 
list of non-permitted
 uses and use cases. Particularly relevant for the media will be its 
guidelines
 for co-authoring content with Open AI: according to these guidelines, content co-authored with Open AI must respect Open AI’s terms of use, must not offend others, clearly disclose the role of OpenAI’s model, provide a disclaimer to users informing them that AI is being used and of its potential limitations. In practice, doing so can be difficult to the extent that Open AI does not share what the limitations of the model are, and prohibiting the use of generative AI to create content that offends others can potentially limit the fundamental right of the media to report content 
even if it shocks or offends
.
Comparing the different instances of prohibited uses in the terms of use and usage guidelines, some identified use cases are more uncontroversially irresponsible than others. There seems to be broad agreement, for example, that generative AI models may not be used to create unlawful content, and that deals with sexual abuse of minors, as well as hate speech, harassment, defamation, bullying, and discrimination. Interestingly, the list of non-permitted content reflects more or less broadly the types of content that are 
not permitted
 to publish or distribute on social media sites. A more nuanced picture arises around adult content. While OpenAI, for example, does not allow its models to be used to create adult content, Hugging Face does not banish synthetic adult content generally but requires securing access to such content via opt-in mechanisms.
Next to certain forms of content that may not be produced with generative AI, certain uses or use cases are prohibited. Here, the picture varies more broadly. And while some of the categories of prohibited uses are reminiscent of some of the “high-risk areas” that the European draft AI Act defines (such as the use in high-risk government decision-making, including law enforcement and criminal justice, migration and asylum), others are new. For example, Open AI does not allow the use of its models in the context of political campaigning, or for giving legal, financial or health advice. Anthropic, too, banishes the use in political campaigning and lobbying but also the use of generative AI to track or target individuals. Stable Diffusion adds the production of disinformation to the list of prohibited uses, but also using generative AI to make fully automated decisions and exploit the vulnerabilities of individuals. Anthropic created an additional category of “restricted business use cases” for which a special “Restricted User Agreement” is needed.
Finally, if the providers of OpenAI define the ground rules for the use of generative AI in society, they are not particularly democratic about this. None of the usage policies of the proprietary models explain how the providers came to the conclusion to ban certain uses, while permitting others, though some point more clearly to, and facilitate a role for the public to participate in that process. Or as Midjourney explains: “We are not a democracy. Behave respectfully or lose your rights to use the Service.” And Open AI simply stipulates: “Our policies may change as we learn more about use and abuse of our models.” In contrast to this top-down approach, open source models like Hugging Face’s invoke the community’s responsibility for developing standards of fair behaviour: “As we as a community develop better norms around training data and AI-related rights, we also ask that you take reasonable steps to honour other people’s wishes regarding systems trained primarily on their data.”
In Sum
In the discussions around the ethical and legal uses of generative AI and the need for defining some guardrails on what (ir)responsible use of generative AI means, there is currently a lot of attention on the role of formal regulators but also the 
emerging ethical guidelines that many media organisations are working on
. So far, there has been relatively little attention on the Terms of Use of the providers of generative AI. And yet, it is the Terms and Use that are currently one of the most important sources of legal obligations between media organisations that wish to use generative AI, and the providers of these models. Also, the Terms of Use play an important role in defining what responsible use of generative AI is.
A review of the terms of use of five providers of generative AI models or platforms has shown that:
There are certain conditions in the Terms of Use that are particularly relevant for media organisations that want to use generative AI. These include the provisions around the division of responsibility for the output generated, as well as the rights to input and outputs of the model. The conditions can differ between providers. Before using a particular model, media organisations probably want to carefully read what the Terms of Use stipulate about who is responsible in case content generated is inaccurate or infringes the copyrights and data protection rights of third parties (typically the media organisation), who owns the rights to any prompts or contents share with the model, the output, and to what extent the model reserves the right to use any pictures, articles and other content from a media organisation to train the model, if there are particular labelling or other obligations when generating news content (like in the case of Open AI), if there is a possibility to contact the provider, and what happens if the model is changed or terminated.
There are certain conditions in some of the Terms of Use that are potentially problematic from the perspective of a media organisation, including provisions that make auditing and critical reporting unlawful because of a ban on scraping or reverse engineering, the lack of any prior warning in case a model is changed or terminated, or provisions that place full legal responsibility for the output generated and the use of the model with the media organisation even in situations in which a media organisation lacks the necessary information or cooperation from the provider of the model.
Finally, there are issues that are typically not addressed in the Terms of Use but that media organisations may want to know about to be able make a responsible choice. This is because the responsible use of generative AI also means choosing a responsible provider. Examples include any initiatives on the side of the provider of the model to address concerns about biases, transparency about the underlying infrastructure (e.g. with which cloud service provider a particular model is affiliated, a question that could influence for example where data will be stored), the possibility to adjust a model to adhere to a media organisation’s own editorial values, but also what a provider has done to reduce the ecological footprint of a model, address concerns around the working conditions of information workers or issues around the security of a model.
Regarding the latter two categories, more discussion and potentially also cooperation among media organisations is needed to bundle and exercise sufficient negotiation power to address potentially problematic Terms of Use.
Finally, with the pending AI Act, there is momentum to have a broader discussion on what “good commercial practices” are with respect to the provision and use of generative AI in the media. According to one of the 
newest proposals
 in the AI Act, for example, a contractual condition that is contrary to good faith and fair dealing or creates a significant imbalance between the rights and the obligations of the parties in the contract shall be considered unfair.
[1]
 The analysis of the Terms of Use took place in the months May and June 2023.]]></content:encoded>
</item><item>
<title><![CDATA[Towards Guidelines for Guidelines on the Use of Generative AI in Newsrooms]]></title>
<link>https://generative-ai-newsroom.com/towards-guidelines-for-guidelines-on-the-use-of-generative-ai-in-newsrooms-55b0c2c1d960?source=collection_home_page----df04cefca135-----62-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Guidelines]]></category><category><![CDATA[Newsroom Innovation]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Hannes Cools]]></dc:creator>
<pubDate>2023-07-10</pubDate>
<atom:updated>2023-07-10</atom:updated>
<content:encoded><![CDATA[Towards Guidelines for Guidelines on the Use of Generative AI in Newsrooms
Hannes Cools
Follow
19 min read
·
Jul 10, 2023
--
2
Listen
Share
Note: This post was co-authored with 
Nick Diakopoulos
.
Generated with Adobe Firefly.
The emergence of generative AI has highlighted the need for newsroom guidelines for these technologies. In this post, we’ll delve into a sample of newsroom guidelines that have already been shared. In the first part, we’ll describe some of the more and less prominent themes and patterns we see. In the second part, and based on the analysis, we’ll suggest some guidelines for crafting guidelines as a news organization. Whether you’re a curious journalist or a newsroom leader, we hope that this “guideline for guidelines” document can function as an overview of potential guardrails for generative AI in newsrooms.
The selection of guidelines we analyzed covers a range of larger and some smaller organizations mostly in Europe and the US with a few from other parts of the world [1]. The current sample of 21 can be found 
here
 [2]; please be in touch via 
email
 if your organization has published guidelines on generative AI that we should add to our list. We’ll regularly update the list of guidelines, and our analysis here, as it grows.
The guidelines we analyzed vary in specificity, and are sometimes named differently as “editor’s note”, “protocol”, “principles” or even “deontological charter”. The tone of some of the guidelines are restrictive where specific uses are banned. Other documents are more examples of governance, where news organizations are committing to specific responsibilities to make AI less risky. Below, we will discuss some of these overarching patterns with examples from specific guidelines.
Observations from published guidelines
Oversight
Guidelines mention oversight and link it deliberately to the importance of meaningful human involvement and supervision in the use of AI, including through additional editing and factchecking of outputs before publication. News organizations also reject the idea of replacing journalists with machines and highlight the importance of the decision-making role of humans when using generative AI tools.
AftonBladet
 and 
VG, 
two news outlets from Sweden and Norway, are both owned by the news corporation Schibsted and you can see some clear similarities in the guidelines. Aftonbladet’s guidelines stated that “all material published has been reviewed by a human and falls under our publishing authority”, whereas VG’s state: “all use of generative AI must be manually approved before publication.”
Reuters
 describes oversight as “striving for meaningful human involvement, and to develop and deploy AI products and use data in a manner that treats people fairly.” Similarly, 
The Guardian
 says that the use of generative AI requires human oversight, stating in their guidelines that it needs to be linked to a “specific benefit and the explicit permission of a senior editor”. 
ANP
, The Dutch News Agency, has similar statements in their guidelines on human oversight, stating they can use AI or similar systems to “support final editing, provided that a human is doing a final check afterwards”. They describe this process as Human>Machine>Human, where the agency and decision-making is overseen by a human.
CBC
, an important news corporation in Canada, states that “no CBC journalism will be published or broadcast without direct human involvement and oversight.” 
De Volkskrant,
 a quality newspaper in The Netherlands, states that their content is created by human editors, reporters, photographers and illustrators and content generated by AI may not be used under the name of de Volkskrant without a human’s approval. The above guidelines by de Volkskrant are very much in line with what 
Heidi.News
, a Swiss news organization, and 
Le Parisien
, a French newspaper are stating. In general terms, they state that no content will be published without prior human supervision.
Nucleo
, a digital native news outlet in Brazil stated that they will never “publish AI content without human review in stories and notes on the site” nor will they “use AIs as the final editor or producer of a publication”. 
Ringier
, a media group from Switzerland with news brands across 19 countries, says that “results generated by AI tools are always to be critically scrutinized and the information is to be verified, checked and supplemented using the company’s own judgment and expertise”. Similarly,
 DPA
, the German Press Agency, describes that the final decision on the use of AI is made by a human. They add: “We respect human autonomy and the primacy of human decisions.”
The German Journalists’ Association (
DJV
) refers to AI tools as colleagues, they state: “Under no circumstances should “colleague AI” be allowed to replace editors. Even if the use of artificial intelligence changes tasks or eliminates them altogether, it does not make people in the newsroom superfluous.” 
The Financial Times
 says that their journalism will continue to be reported, written and edited by humans who are the best in their fields.” 
STT
, a Finish press agency, links oversight to decision-making: “Don’t let the AI decide for you, but always assess the usefulness of the answers yourself”.
Insider
, an American online media company, states that their journalists should always verify their facts. They add specifically: “Do not plagiarize! Always verify originality. Best company practices for doing so are likely to evolve, but for now, at a minimum, make sure you are running any passages received from ChatGPT through Google search and Grammarly’s plagiarism search.” They also call on editors to challenge reporters to verify facts. “It is necessary to step up your vigilance and take the time to ask how every fact in every story is known to your colleague.”
Transparency
Mentions of transparency are often interconnected with the requirement that content should be labeled in a way that is understandable for audiences. However, it is often far from clear from the guidelines how these mentions of transparency will take shape in practice.
Aftonbladet 
and 
VG
 state that in the rare cases where they publish AI-generated material, both text and images, it will be clearly labeled that it was generated by AI. The guidelines of 
VG
 specifically add: “AI-generated content must be labeled clearly, and in a way that is understandable”. In the same light, 
Reuters
 states that they will implement AI-practices “to make the use of data and AI in our products and services understandable”.
The Guardian
 says that when they use generative AI, they will be open “with their readers when they do this”. 
CBC
 talks about “no surprises” for their audiences, stating that they will label AI-generated content: “We will not use or present AI-generated content to audiences without full disclosure.” Similar statements can be found in the guidelines of the Dutch Press Agency (
ANP
), 
Mediahuis
, the Belgian Media Council (
RVDJ
), the German Press Agency (
DPA
), and the German Journalists’ Association (
DJV
), that AI generated content needs to be clearly labeled.
Ringier
, a media group in Switzerland, describes that their general rule is that content that is generated by AI needs to be labeled. Interestingly, this requirement is not necessary “in cases where an AI tool is used only as an aid” suggesting a different approach towards transparency when AI is used by people as an augmenting part of their workflows. Similarly, 
STT 
says that the use of AI in news reporting must always be communicated to the public: “This applies both to situations where technology has been used to help produce the news and to news where the source material has been created by machine intelligence.”
Banned vs. Allowed Uses
Banned and allowed uses of generative AI are often listed as ways to guide practices, though are sometimes also accompanied by exceptions. For these exceptions, conditions for transparency, responsibility, and accountability are made explicit in the guidelines. There is also a fair amount of attention given to the use-case of image generation.
Wired 
states that they do not publish stories with text generated by AI, except when the AI-generated character is the whole point of the story. They also say that they “will not publish text that is edited by AI”, as well as the fact that they will “not use AI-generated images instead of stock photography”. The rationale behind it refers to the idea that editing is inherently linked to determining what is relevant, original, and entertaining. Despite the restrictive tone of Wired’s guidelines, they state that they may try AI for tasks in the news reporting process like suggesting headlines or generating ideas or texts for short social media posts.
Nucleo
 lists some allowed uses of generative AI, such as summarizing texts for suggesting “alternative posts on social networks”, and as a tool for research “subject and themes”. 
Aftonbladet
’s and 
VG
’s journalists, for example, may “use AI technology in their work to produce illustrations, graphics, models”. Although the VG’s guidelines add a ban on generating photorealistic images as “AI-generated content must never undermine the credibility of journalistic photography.”
Insider
, an American online outlet, states that their journalists may use generative AI for creating “story outlines, SEO headlines, copyediting, interview question generation, explaining concepts, and summarizing old coverage”. However, they state that their journalists must not use it to do the writing for them. For the use of AI generated images, they underline that they need “to have more conversations before they can decide if and how to use them”.
Hongkong Free Press (HKFP)
, an English newspaper in HK, is restricting all use of generative AI for any news writing, image generation or fact-checking, as “few A.I. tools include proper sourcing or attribution information.” Interestingly, HKFP does state that they might use internally-approved AI “for grammar/spell checking, for rewording/summarizing existing text written by our team, and for assisting with translations, transcriptions, or for research.” Similarly, 
De Volkskrant,
 a quality newspaper in The Netherlands, states that their editors do not publish journalistic work that has been generated by artificial intelligence.
CBC
 says that they will not use AI to recreate the voice or likeness of any CBC journalist or personality “except to illustrate how the technology works”. Interestingly, they link this exception to two conditions, namely the (1) “advance approval of our standards office”, and the (2) “approval of the individual being “recreated”.” Additionally, they will not use the technology for their investigative journalism in the form of facial recognition or voice matching, as well as not use it to generate voices for confidential sources whose identity they are trying to protect. They will continue practices that are understood by their audiences, such as voice modulation, image blurring, and silhouette.
The Swiss news organization, 
Heidi.News
, states that they will only “use synthetic images for illustrative purposes, not for information purposes, so as not to confuse real-world events”. They also add that they will not publish any synthetic image that could pass for a photograph, “except for educational purposes when the image in question is already public”.
Le Parisien
, a French newspaper, says that they reserve the right to use AI for the generation of text and images for illustrative purposes. They link the exceptional use of generative AI to the fact that they need to be transparent at all times: “We will make sure that the origin is explicitly stated for the reader.” Additionally, they describe the use of AI as an enrichment. “News workers may use these tools as they would a search engine, but they must always return to their own sources to guarantee the origin of their information.” 
The Financial Times
 underlines that they won’t publish photorealistic images generated by AI but they will explore the use of AI-augmented visuals (infographics, diagrams, photos) and when they do that, they will make it clear to the reader.
STT
, a Finish press agency, links the banned uses of generative AI directly to its limitations. “STT does not use AI for data mining. The sources used by AI are often obscure, which makes its use in editorial work problematic.” They also state that the reliability of sources and the verification of information remains vital.
Accountability & Responsibility
Accountability and responsibility are often mentioned in the guidelines in relation to the content published as well as values such as accuracy, fairness, originality, and transparency. The implementation of accountability measures for the use of data and AI products is also highlighted, as is using technically robust and secure AI systems to minimize risks.
AftonBladet
, a popular newspaper from Sweden, states that they are responsible “for everything they publish on the site, including material that is produced using, or based on, AI or other technology and falls under our publishing authority”. 
Reuters
 say that they will “implement and maintain appropriate accountability measures for our use of data and our AI products and services”. 
DPA
, the German Press Agency, underlines that they will only use AI “that is technically robust and secure to minimize the risks for error and misuse”. 
Le Parisien
, a French newspaper, states that they want to protect themselves against “any risk of error or copyright infringement, but also to support the work of artists, photographers and illustrators”.
The German Journalists’ Association (
DJV
) underlines that news organizations are responsible for their content and that editorial departments should establish regulated acceptance and approval processes for journalistic content when AI is involved. 
The Financial Times
 states that it’s their conviction that “their mission to produce journalism of the highest standards is all the more important in this era of rapid technological innovation”. They add: “FT has a greater responsibility to be transparent, to report the facts and to pursue the truth”.
Insider
, an American online media company, says that their audience should trust them to be accountable and responsible for each story’s accuracy, fairness, originality, and quality. They state that their journalists are responsible for the accuracy, fairness, originality, and quality of every word in their stories.
Privacy & Confidentiality
Privacy and confidentiality are often mentioned in terms of source protection and being careful with providing sensitive information to external platforms. Additionally, guidelines highlight that they should be careful about using confidential or unpublished content as input for generative AI-tools.
AftonBladet 
describes that they “protect source protection and do not feed external platforms such as ChatGPT with sensitive or proprietary information,” the sentiment of which is also reflected in slightly different terms by 
VG
: “journalists should initially only share material that has been approved for immediate publication on VG’s platforms with AI services.” 
Reuters
 underscores that they will “prioritize security and privacy in our use of data and throughout the design, development and deployment of our data and AI products and services.” 
CBC
 says that they will not feed confidential or unpublished content into generative AI tools for any reason.” 
Mediahuis
 states that they should comply with privacy laws and where required obtain user consent before using personal info.
Ringier
, a media group in Switzerland, says that their employees are “not permitted to enter confidential information, trade secrets or personal data of journalistic sources, employees, customers or business partners or other natural persons into an AI tool”. For code development, Ringier’s guidelines describe that code may only be entered in generative AI systems when they “neither constitute a trade secret nor does it belong to third parties”.
Interestingly, most of the guidelines we analyzed do not distinguish between generative AI services (e.g. OpenAI) versus developing and using a generative AI system hosted on computers operated by the organization itself, such as might be the case with open source models. It’s important to note, however, that the risks of privacy and confidentiality are more related to the use of generative AI services 
hosted by other organizations
, rather than the use of generative AI itself.
Cautious Experimentation
Mentions of cautious experimentation in the guidelines are often linked to being curious and critical while also underlining the potential for innovation. There is also emphasis on checking the veracity of the outputs that were generated by AI, as there is a focus on acknowledging the risks of misinformation and the “corruption of truth”.
VG
’s main rule is that their journalists must treat “the use of text, video, images, sound and other content created with the help of AI (generative AI) in the same way as other sources of information; with open curiosity and with caution.” 
CBC
 states that they will never rely solely on AI-generated research in their journalism: “We always use multiple sources to confirm facts.” 
ANP
 says that every member of the ANP editorial staff should look at AI and related systems “full of wonder, inquisitive, critical and open to developments”. They add that they continue to actively pursue innovations with an open mind. 
STT
, a Finish press agency, states that journalists should explore AI when they have the chance. They were one of the first news publishers to 
start experimenting with generative AI
 in the form of language models, and state that experimentation remains vital to uncover the possibilities and limitations of these models.
The Financial Times
, a British newspaper, states that in their letter from the editor that they will embrace AI to provide services for readers and clients. Interestingly, they also underline that it is necessary for the FT that a team in the newsroom is established that can experiment with AI responsibly. They also add that “every technology opens exciting new frontiers that must be responsibly explored. But as recent history has shown, the excitement must be accompanied by caution over the risk of misinformation and the corruption of the truth”. Lastly, FT says that all newsroom experimentation with AI will be recorded in an internal register, including, to the extent possible, the use of third-party providers who may be using the tool.
Strategic Intention of Use
In several cases guidelines documents also express the strategic goals of the organization in deploying generative AI. Motivations mentioned included a desire to enhance originality, quality, and even speed, while also sometimes reflecting desires for not replacing journalists and upholding core values like independence / impartiality.
The Financial Times 
says that AI has the potential to “increase their productivity and liberate reporters and editors’ time to focus on generating and reporting original content”. 
The Guardian
 states that they will also seek to use generative AI tools editorially only where it contributes to the creation and distribution of original journalism. In addition, they say that when they are using generative AI, they will focus on “situations where it can improve the quality of our work”. 
Mediahuis
, a Belgian news corporation, states that AI should enhance their journalism. They say that their goal is to “enhance the quality of our journalism for our audience”. 
Insider
 underlines that their journalists may, and even should, use AI to make their work better, “But it remains your work”, they state, “and you are responsible for it. You are responsible to our readers and viewers.” 
ANP
, the Dutch Press Agency, says that they want to remain impartial and independent and that they should be cautious with the use of generative AI.
DPA
, the German Press Agency, states that they use AI for various purposes and that they are open to the increased use of AI. They add that AI will help them to make their work better and faster. 
De Volkskrant
, a quality newspaper in The Netherlands, views artificial intelligence (AI) as a tool, “never as a system that can replace a journalist’s work”, and this sentiment of not intending to replace journalists with machines is echoed by 
Heidi.News
 and 
Le Parisien
. 
Heidi.News
, a Swiss news organization, states that AI systems should be regarded as tools, and when striving to be objective, their journalists should “not view these tools as sources of information”.
Training
Training is rarely mentioned in the guidelines, however, when it is mentioned, training and classes are mostly linked to mitigating the risks of generative AI and being accountable and transparent towards the audience.
Mediahuis
 states that training and qualification should be established for those responsible for AI decisions. They link it to the development of clear lines of accountability for AI development and use. The German Journalists’ Association (
DJV
) underscores that the use of artificial intelligence must become an integral part of the training and further training of journalists. They call upon media companies to create appropriate training that include the misuse of AI. The British newspaper, 
The Financial Times
, states that they will provide training for their journalists on the use of generative AI for story discovery, as it will be delivered in the form of masterclasses.
Bias
The Guardian
 explicitly deals with bias in generative AI and states that they will “guard against the dangers of bias embedded within generative tools and their underlying training sets”. 
Mediahuis
 states that their journalists should watch out for biases in AI systems and work to address them. 
Ringier
 states that their tools shall always be “fair, impartial and non-discriminatory”.
Adaptability of Guidelines
Several of the guidelines reflected humility in the face of rapid change and called out the importance of adapting guidelines over time as the understanding of risks evolves.
Nucleo
, a digital native news outlet in Brazil, states that their policy on generative AI is “constantly evolving and may be updated from time to time”. The same sort of statement can be found in the guidelines of 
AftonBladet 
and
 VG. Wired 
writes
 
that AI will develop which “may modify our perspective over time, and we’ll acknowledge any changes in this post”. According to an editor’s note at the bottom of the guidelines document it has already been updated once. CBC states that they told their journalists that these guidelines are “preliminary and subject to change as the technology and industry best practices evolve”. 
De Volkskrant
 states that they follow the developments with critical interest and that where they find it necessary, their protocol will be adapted. Similarly, 
Ringier
, a media group in Switzerland, states that their guidelines will be continuously reviewed in the coming months, and they will be adjusted if necessary.
Less mentioned topics in the guidelines
Overall, there were some topics that weren’t that prominent in the guidelines. Legal compliance as well as personalization, data quality, user feedback, and integrating generative AI in the supply chain are rarely mentioned.
Supply Chain
The AI supply chain gets at the idea of the network of suppliers for AI systems such as 3rd party models, collaborators, data providers, annotation providers, etc. The German Journalists’ Association (
DJV
) underlines the need for accuracy when it comes to the handling of data. They say: “Data collection, preparation and processing must meet high qualitative standards. Incompleteness, distortions and other errors in the data material must be corrected without delay”. Interestingly, the DJV calls on media houses to build their own and value-based databases and supports open data projects by public authorities and government institutions. They state that greater independence from commercial big-tech providers is desirable. 
Reuters
 is one of the few news organizations that mentions governing collaborations, as they state that they will “strive to partner with individuals and organizations who share similar ethical approaches to our own regarding the use of data, content, and AI.”
Legal Compliance
DPA
, the German Press Agency, says that they only use AI that complies with applicable law and legal requirements and that meets our ethical principles, such as human autonomy, fairness, and democratic values. The German Journalists’ Association (
DJV
) calls on legislators to make sure that the labeling of content generated by AI becomes mandatory. They call on the legislators to establish such labeling obligations in law.
Personalization
The German Journalists’ Association (
DJV
) states that personalization by means of AI must always be carried out in a responsible and balanced manner. They add that users should have “the option of changing the selection criteria and/or completely deactivating personalized distribution”.
User feedback
Mediahuis
 states that they encourage readers to give feedback and let them review their data. This reflects a commitment to transparency, accountability, and a customer-centric mindset in the evolving landscape of media consumption.
Some Guidelines for Guidelines
Based on our observations above, here we offer a few suggestions to journalists and news organizations that may be thinking about developing their own guidelines. By reviewing existing ethical guidelines and codes, adopting a risk assessment approach, and drawing on diverse perspectives within the organization, we think news organizations can improve the quality and helpfulness of their generative AI guidelines.
Reviewing existing ethical guidelines and codes
Several of the themes we found in our analysis, including things like accountability, transparency, privacy, and so on, reflect well-established values and ethical principles in journalism practice. Therefore, when crafting or updating guidelines in light of generative AI, we suggest that it would be valuable to 
review existing codes of conduct and journalism principles
 as a basis for thinking through how and whether those principles can be adhered to in the face of changes spurred by generative AI. For instance, although the idea of independence does come up a few times in the guidelines we analyzed it was not as prominent as one might expect given how central it is in, for instance, 
ethics codes such as SPJ’s
. How might the use of generative AI fully embody and reflect a normative commitment to journalistic independence?
We emphasize that the emergence of generative AI might result in new challenges for news organizations, but this should not entail that the crafting of guidelines should start from scratch. We suggest news organizations go over such codes of conduct and contrast them — one-by-one — with the potential use of generative AI and the risks that may pose. By systematically working through core values and principles of journalism it should suggest strategies and tactics for how to use generative AI in ways that are consistent with established norms.
Adopt a risk assessment approach
News organizations may benefit from adopting a systematic risk assessment approach in developing guidelines. Such an 
approach as developed by NIST
, may for instance, help 
map
, 
measure
, and 
manage
 risks, offering ways to efficiently develop policies for mitigating risks that are identified. What are the different risks of the uses of generative AI, how can those risks be measured and tracked, and how can these be managed with respect to the existing values and goals a news organization has?
For instance, traditional journalistic practices emphasize the importance of relying on credible sources and ensuring the accuracy of information. However, generative AI introduces a level of uncertainty regarding the origin and reliability of content that might be acquired in the course of reporting. In light of generative AI, this risk mitigation could for example consider how to check the veracity of documents that are being shared as source materials. Journalists could navigate this uncertainty by implementing rigorous verification processes and using corroboration and triangulation techniques to validate AI-generated content. By cross-referencing information from multiple sources, newsrooms could reduce the risk of disseminating unverified or false information.
Establish a diverse group to draft guidelines
Although generative AI can also be used to produce computer code and can therefore impact all sorts of different news products, we found few guidelines that distinguish how there may be different approaches between editorial and product usage (e.g. in software development). Moreover, other parts of news organizations such as sales and marketing could also benefit from using generative AI. Should they have separate guidelines? To cover the full ground here we suggest that it is important to establish a diverse set of stakeholders within your news organization to discuss and appropriately scope the guidelines. This will allow organizations to reflect on the risks that might arise in the newsroom, and it might help to determine broader, company-wide risks that transcend the day-to-day workflows of journalists.
Endnotes
[1] Note on the methodology: we have collected the guidelines and translated the ones that were not in English using DeepL or Google Translate. After having all of them in a document where we both have access to, we added open codes. After having read all of the guidelines, we discussed a more overarching coding scheme. This resulted in a more robust overview of the codes which was used as a general structure to write this blogpost.
[2] Note on the selection of guidelines: we have decided not to include the guidelines on AI that were already published by 
Bayerischer Rundfunk
 (BR) in Germany and 
British Broadcasting Corporation
 (BBC) in the UK, as we wanted to analyze the more recent guidelines that specifically deal with generative AI. Nevertheless, we acknowledge the pioneering work these news outlets have done in paving the way for the use of AI in newsrooms.]]></content:encoded>
</item><item>
<title><![CDATA[Decoding Bureaucracy]]></title>
<link>https://generative-ai-newsroom.com/decoding-bureaucracy-5b0c1411171?source=collection_home_page----df04cefca135-----63-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[ChatGPT]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Design]]></category><category><![CDATA[Community Engagement]]></category><category><![CDATA[Ethics]]></category>
<dc:creator><![CDATA[Andrew Rodriguez Calderón]]></dc:creator>
<pubDate>2023-07-06</pubDate>
<atom:updated>2023-07-06</atom:updated>
<content:encoded><![CDATA[Decoding Bureaucracy
Generative AI for Accessible Government Policies
Andrew Rodriguez Calderón
Follow
13 min read
·
Jul 6, 2023
--
Listen
Share
Starting last year, 
The Marshall Project
 requested, examined, and analyzed state corrections department’s publication policies across the United States. These policies establish rules and procedures governing what published materials people who are incarcerated are permitted to read, including dictating which books are banned.
This post details our work on this project, expanding on how we leveraged generative AI to produce summaries of the policy documents. We lay out our motivation for using AI, our technical and editorial processes, and elaborate our ethical considerations and larger takeaways.
Motivating the Need for AI
Our goal with this project is to publish stories and design news products for people affected by prison and jail book bans. We hope to emphasize the challenges incarcerated people face in accessing and holding onto books, promote accountability, and address information needs.
So far we have published a 
searchable database
 with banned book lists from 18 states, policy summaries from over 30 states using ChatGPT-3, 
takeaways from our policy analysis
, and most recently, a story based on a tip we received from someone who was formerly incarcerated about the 
Ohio prison system’s confusing book review process
.
A key approach in the project is to put the people affected by the system at the heart of our reporting, design and development processes, ensuring that our stories and our products stem from conversations where real needs, wants and harms (actual and potential) are surfaced by the people closest to the issue. A highly collaborative journalistic design practice has emerged that builds on the rigor, fairness, and independence of traditional journalism combined with new ways to approach our craft like 
co-design
, 
design justice
, product thinking, community engagement, and computational journalism.
When we started hosting community listening sessions earlier this year with prison educators, carceral librarians, people who were formerly incarcerated, and books to prison programs, we were told that there was a need for comprehensive policy summaries that facilitate comparisons between states and that clearly underscore gaps in rules and procedures.
So a question began to form in our minds:
Could ChatGPT do the dreary, under-resourced work of writing policy summaries? If it could, that would free us up to think about fact-checking, editing, and reporting more deeply based on what the summaries showed us.
We found that it worked well and we learned that:
ChatGPT excels at turning complex, bureaucratic text into simple summaries as a public service.
ChatGPT can do textual analysis to classify common topical features and themes in a body of text.
ChatGPT will take a data dictionary and use it to group relevant text based on the definitions in the data dictionary.
A human-in-the-loop approach to generative AI encourages accurate, reliable and vetted ground truth datasets, and outputs that are journalistically viable.
A machine-human hybrid approach opens up new reporting possibilities without compromising on editorial integrity, and helps already strapped newsrooms overcome resource constraints while allowing reporters, designers, and product teams to prioritize resource decisions by revealing what is must-have now, what is a need-to-add later, and what shouldn’t be greenlit at all.
Speaking to people with the most experience around an issue area is essential to working with new technology that doesn’t oversimplify complex social problems.
Involving historically marginalized groups and the people that serve them in the design process to solicit feedback and suggestions on tools and information will help ensure that your stories and products are responsive to real needs.
In the next sections, we delve deeper into the technical, editorial, and ethical and design approaches behind this work.
Technical Approach
After we gathered policies from every state, we reached out to various stakeholders, including books-to-prison programs, prison education groups, freedom of information nonprofits, librarians, and carceral librarians and heard that they wanted a way to understand and compare publication policies.
The question was whether ChatGPT could help us ingest the policies and generate summaries with uniform sub-heads that made it easy to compare one state to another, while highlighting any gaps, like whether a state had an appeals process.
After we considered the problem, we decided on a 
human-in-the-loop approach
, involving the following four steps:
1. Ground Truth (human)
A reporter familiar with these policies read every one of the first 14 we collected and extracted the sections relevant to publications. Some publication policies are bundled with the mail policy, and vice versa, so we could not pass in the whole document to ChatGPT.
We did not want to trust ChatGPT or any Natural Language Processing method to correctly capture key parts of the policy. We inserted human judgment at two key points in the process: establishing a benchmark for accurate, reliable, and vetted information to pass into the model, and validating the outputs of the model. It was crucial to the success of this workflow that we be certain we had extracted exactly what we wanted and needed to summarize.
Once we had the extracts in a spreadsheet, this became our 
ground truth dataset
.
2. Generate Policy Sections and Definitions (machine)
We then ran all of the extracts from the policies through ChatGPT (using version 4) in a single chat. We were essentially feeding in our ground truth data and asking it to identify common aspects of the policies to then be able to generate uniform sub-heads with definitions.
While we lost the initial prompts we used for this task (keep a prompt record!), we did manage to generate a useful list of generalized sub-heads along with definitions, which we used in the subsequent step to guide the summarization. Here’s the list of sub-heads and definitions:
Publication Sources: This includes information about authorized publishers, distributors, retailers, and other sources of books and publications.
Publication Specifications: This covers all details about the physical condition, content requirements, and prohibited features of publications.
Review and Approval System: This includes details about the process of reviewing publications, the entities involved in review and approval, and the criteria for approval.
Delivery and Receipt of Publications: This covers all details about book delivery timelines, receipt requirements, book handling procedures, and limitations on book ownership.
Prohibited Publications and Restrictions: This covers the list of prohibited books, restrictions on personal correspondence, and content that can lead to rejection of publications.
Appeals Process and Notifications: This would include details on how rejected books can be appealed, timelines for appeals, details on the entities handling the appeals, and how and when notifications are sent to publishers, inmates, and other relevant parties.
Record Keeping: This includes details on how long records of denials are kept.
Special Considerations: This includes any unique rules or considerations such as handling of publications for inmates in disciplinary detention, special rules for non-English publications, and conditions under which publications can be redacted or content removed.
3. Prompt & Output (machine)
With the definitions in hand, we re-processed the policy extracts for each policy. For each set of policy extracts, we created a fresh chat interface, because we noticed that compounding tasks in a single chat polluted the outputs.
We told ChatGPT to parse and then group the policy extracts using the sub-heads and definitions generated in the earlier step in a single, 
chained prompt
. We also instructed the model to write a default sentence whenever it did not find information relevant to the sub-head in the policy extracts, since highlighting these gaps was a product priority.
Here’s the main prompt we used:
you are an expert in reading excerpts of prison policies and summarizing them into predefined sub-heads with definitions and using the “important instructions.”
I will provide you with a list of sub-heads with definitions. When you summarize, you will group them under the subheads based on relevance.
Important instructions:
1. When summarizing the notes into the sub-heads, if you do not find information relevant to the sub-head in the notes, include a default sentence “There is no information relevant to this sub-head in the policy.
2. You will ignore any special or text formatting in the notes. You will also ignore bullet points of any kind in the notes.
3. When you are ready to receive the sub-heads with definitions, say, “ready for sub-heads”
4. When you are ready for the notes, say “ready for notes”
Arriving at this prompt was an interactive process. Initially we had a version without special instructions, and it was working. But with certain extracts, we noticed that bullet points or special categories would alter the output. So we rewrote the prompt to account for these edge cases as we went along.
To ensure consistency, when we added special instructions, we went back and reprocessed prior extracts. In retrospect, we don’t think that this was necessary because the new and old outputs varied only slightly. However, it allowed the human in the loop to know that they hadn’t inadvertently altered the output of the model in non-edge cases.
4. Validate (human)
Once we had the summaries, we went back to our ground truth extracts and compared the output to the actual policy extracts, searching for hallucinations or other errors.
Eventually, the summaries were shared with our styles & standards editor who ensured that they complied with The Marshall Project’s rigorous standards, especially those around people-first language. Lastly, the data editor read them once more.
By the end, at least four people had read, edited and verified each output for accuracy and style.
Editorial Value Proposition
In a previous iteration of this workflow, we had read through each policy and taken notes on aspects of the policy that we thought were noteworthy. We then wrote those notes up into simple, one paragraph summaries. We took about two weeks to read through the 50 policies and take notes. When we started writing through the notes, it was so slow-going that our reporter got concerned about time to release and burnout.
That was when the idea of using ChatGPT came up. In about 45 minutes, the reporter individually gave our notes to the model (version 3.5 at the time) and generated simple, one-paragraph summaries that we then fact-checked twice and copy-edited.
Those are currently on 
The Marshall Project’s website
.
In this current, more robust iteration of our workflow, it took us about one and a half weeks to manually extract relevant parts of the policies and generate new summaries for 14 policies. We are still checking and editing them so we don’t have a timestamp on that part of the work process yet. The biggest limitation to our progress was ChatGPT-4’s cap on 25 messages every 3 hours. We submitted a petition to OpenAI to expand our access, but moving forward, we are also considering using the 
OpenAI API
 to process the remaining 36 policies since it doesn’t have the same limitation; though it incurs a different cost structure based on number of tokens (both input and output) rather than number of requests.
Here’s an example of what the new summaries look like:
Publication Sources:
Inmates can receive books, periodicals, and newspapers accepted for distribution by the USPS. A departmentally approved vendor is any publisher, bookstore, or book distributor that does mail order business. Books, periodicals, or other publications that are mailed from a religious organization bookstore are considered as coming from an authorized vendor. Personal correspondents cannot mail books, periodicals, or publications directly to inmates.
Publication Specifications:
Inmates may possess a reasonable number of publications, including books, magazines, and newspapers, as directed by the Authorized Personal Property Schedule in DOM 54030.17. Publications addressed to inmates shall be processed in accordance with CCR 3134.1. All incoming paperback and hardback books, and any enclosures within them, must be inspected prior to being altered and/or issued.
Review and Approval System:
All non-confidential inmate mail, incoming or outgoing, is subject to being read in its entirety by designated staff. All incoming mail shall be inspected for contraband prior to issuance. Mail shall only be disallowed if it violates CCR Sections 3006, 3135, any other applicable regulations, or DOM Sections 54010.13 and 54010.14.
Delivery and Receipt of Publications:
All incoming books, magazines, or newspapers must be inspected before being issued. Delivery by staff shall be completed as soon as possible, but not later than 15 calendar days after the institution receives the book, except during the holiday season and during modified programs of affected inmates.
Prohibited Publications and Restrictions:
Prison authorities may exclude obscene publications or writings, any manner of contraband as described in CCR 3006, and any matter concerning gambling or a lottery. Material is considered obscene when it appeals to deviant sexual groups and portrays explicit sexual content, non-consensual behavior, or violent conduct. The CDCR shall distribute a centralized list of disapproved publications that are prohibited as contraband.
Appeals Process and Notifications:
Disapproved material shall be referred to staff not below the level of Correctional/Facility Captain for determination and appropriate action. The CDCR Form 1819, Notification of Disapproval-Mail/Packages/Publications, shall be utilized by each institution/facility when incoming or outgoing mail/packages/publications addressed to or being sent by an inmate are withheld or disallowed. The CDCR Form 1819 informs the inmate of the reason, disposition, name of official disallowing the mail/package/publication, and the name of the official to whom an appeal can be directed.
Record Keeping:
A copy of the CDCR Form 1819 and the supporting document(s) shall be retained by each institution/facility for a minimum of seven years for litigation purposes. After seven years if the material is not needed it shall be destroyed. If a lawsuit has been filed as a result of mail being disapproved, the CDCR Form 1819 and the supporting document(s) will be retained for two years from the conclusion of the suit.
Special Considerations:
Correspondence in a language other than English to or from an inmate is subject to the same regulations governing all other mail and may be subject to a delay for translation of its contents by staff. When such delay exceeds normal mail processing by five business days, the inmate shall be notified in writing of the delay, the reason for the delay, and subsequent determinations and actions regarding that item of mail. If staff is unable to translate the letter and its contents within 20 business days of notice to the inmate, then the letter shall be delivered to the inmate untranslated.
The generative workflow saved us considerable time. It has also entailed a different editorial balance that emphasizes fact-checking, editing, and verification that met The Marshall Project’s rigorous standards. It has also enabled collaboration across teams within the newsroom that allowed us to start a conversation around our use of generative AI and the standards that we should have as an organization when designing reporting protocols with this technology.
We have been documenting our work with ChatGPT internally in a GitHub project, including prompts that worked and failed, lessons learned and tips for others in the newsroom. This documentation serves the purpose of creating a place to discuss this work, while also making it easier for us to explain it to people outside of the newsroom. It can also nourish broader industry conversations that center reporters in the machine process as fundamental components of accurate and reliable generative information gathering and distribution.
Ethical & Design Considerations
It is important to acknowledge the limitations and ethical considerations of AI deployment in newsrooms. The need for human oversight, the potential for biases in AI tasks, and the responsible use of AI technologies remain crucial topics, even in simple use cases.
Our thinking here is that by distrusting ChatGPT enough to leave nothing to chance, we designed a workflow that devotes the human side of the equation to sourcing, fact-checking, and revising the machine’s outputs, while the actual processing of the large amount of data gets handled by AI, leveraging its tirelessness and ability to take commands and iterate on them rapidly.
In the end, through this hybrid approach that combines AI capabilities with journalist expertise, we unlocked complex bureaucracy, making information accessible and actionable for a wider readership, which we believe is a foundational public service.
We are also motivated by a design practice that attempts to center historically marginalized people in the design, development, and deployment processes of news products. Community listening has been an important tool for us to get the feedback that we need to do this work. Some in the design space call this user research. At The Marshall Project, we prefer to think about the people we cover as part of a community, rather than as “users”. This helps move us away from the consumer-oriented language of traditional design and product methods.
Lessons for other newsrooms
Our experience has shed light on several insights and considerations that would be valuable to other journalists in the field.
First, crafting a well-defined prompt and refining categories through iterative interactions is crucial for generating accurate and relevant definitions (ultimately a data dictionary). Additionally, integrating AI technologies into newsrooms can streamline tasks like summarization, enabling journalists to focus more on vetting and fact-checking, thus enhancing the overall quality of reporting. It also highlights the importance of documenting your process and working in public as a part of using generative technologies ethically and responsibly.
Second, the ability to process and render complex government documents intelligible and actionable is a valuable service newsrooms can provide to their readers. By adopting a hands-on, hybrid process that leverages generative AI tools like ChatGPT, newsrooms can effectively summarize intricate policies, making them more accessible to a wider audience.
From a public service standpoint, this approach underscores the potential of generative AI in democratizing access to complex information and empowering communities historically excluded from intricate bureaucratic processes by breaking down barriers to understanding formal government documents. Newsrooms can serve as trustworthy intermediaries, ensuring the public’s comprehension of critical policies, fostering transparency and enabling communities to demand accountability.
Moving forward, we plan to think more about how to use ChatGPT to make sense of complex bureaucracies. One idea that we are actively experimenting with is to model a real-world system, like the judges in a court system, using a relational or graph database. We hope to have reporters fill in information about judges and judicial candidates like their terms in office, election history, roles they’ve occupied on the bench, campaign finance, and job history. We then want to ask ChatGPT to write sentences for different reading levels and languages about each judge, based on judgements we’ll draw from our reporting. For example, perhaps we learn that competitive races are notable and rare. If a judge has never run opposed, that’s worth a small mention. But if they’ve had recent, competitive races, then that should be a significant feature in the summary ChatGPT helps us write.
There are many ways in which ideas like this can fail, and we need to deeply consider them before we make anything public. The criminal justice system is a high stakes environment; people already experience trauma of all sorts and they sometimes rely on our stories and products to understand their experience. The harms even of something as simple as summarizing policies can range from inaccuracies to 
reflecting biases of the training set
 that run counter to our own reporting and values.
We believe that embracing generative AI responsibly means developing a practice that involves reporters in critical parts of machine learning workflow, while engaging people with lived experience in the design process, to better serve them and ease their access to information resources.
This journalistic-design approach to generative tech also provides a counterpoint to all of the emergent market–oriented AI products that ultimately seek profit rather than public service, a democratic value that journalism is uniquely poised to promote.]]></content:encoded>
</item><item>
<title><![CDATA[Using ChatGPT to Correct Spanish Grammar and Style in News Articles]]></title>
<link>https://generative-ai-newsroom.com/using-chatgpt-to-correct-spanish-grammar-and-style-in-news-articles-733c9c83de9a?source=collection_home_page----df04cefca135-----64-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[ChatGPT]]></category><category><![CDATA[Generative Ai]]></category><category><![CDATA[La Silla Vacía]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Karen De la Hoz]]></dc:creator>
<pubDate>2023-06-13</pubDate>
<atom:updated>2023-06-13</atom:updated>
<content:encoded><![CDATA[Using ChatGPT to Correct Spanish Grammar and Style in News Articles
Karen De la Hoz
Follow
8 min read
·
Jun 13, 2023
--
1
Listen
Share
Photo by 
Jonathan Kemper
 on 
Unsplash
La Silla Vacía, a well-known Colombian digital native media outlet focused on political coverage, has a section called 
En Vivo
 (Live). All the journalists in the newsroom work at least a 6-hour shift every 15 days to update this section. Its objective is to narrate, through short and concise text entries, the main news of the moment.
Reducing the number of writing, grammar and spelling mistakes in 
En Vivo
, and in all sections of the site, is one of the objectives of journalists and editors. To facilitate this task, we began experimenting with OpenAI models (GPT-3.5 and GPT-4).
The medium-term goal is to generate a virtuous circle: journalists who are more aware of their mistakes, who edit themselves in real time and pass fewer errors on to their editors, who in turn spend fewer hours correcting simple errors and have more time to focus on tasks that are more relevant to the audience.
The results of our tests are bittersweet. The system identifies and corrects most errors. However, it sometimes indicates that it has corrected a sentence, but when we check the “corrected” sentence, it is exactly the same as the original. Additionally, since we are working with a system that we did not train with our writing style book, some of its suggestions, although grammatically correct, are not relevant to our site. Last but not least, creating prompts takes longer than we initially estimated.
Despite the above, I am confident that we can reach a point where the system allows us to review the correct application of our styling guide. Collaboration and shared documentation and experiences are key in this process. For the time being, we do not contemplate a scenario where we would publish a text corrected by ChatGPT without these corrections having been approved by a human journalist or editor.
Below I describe my experimentation process with ChatGPT.
Randomness, system prompts and user prompts
When I started this experiment I wanted to achieve two things: I wanted the system to make corrections to a text and I wanted the system to tell me in bullet point format what it had corrected and why. Getting a list of errors and suggestions seemed to me the most optimal and the fastest way to integrate ChatGPT suggestions into a text.
This was one of the first prompts I used in the ChatGPT interface (with URL “
https://chat.openai.com/
” 
): 
Actúa como un editor de estilo. Identifica e indícame errores gramaticales como redundancias, errores de concordancia en género y número o errores en los usos de los signos de puntuación. También indícame palabras mal escritas o typos e identifica cualquier incoherencia en el estilo del texto. Al mostrarme los resultados indícame la frase original, el error y la nueva redacción que me propones.
(You are a style editor. Identify and point out grammatical errors such as redundancies, gender and number agreement errors, or errors in the use of punctuation marks. Also indicate misspelled words or typos and identify any inconsistencies in the style of the text. When you show me the results, please indicate the original sentence, the error and the new wording you propose).
I started to notice that, although giving the system the same instruction, the system did different things every time I interacted with it. At Nick Diakopoulos’ suggestion, I started testing on 
OpenAI Playground
, a different interface from the previous one. Under Nick’s guidance I understood that there was a variable called 
temperature
 to which I could assign a value between 0 and 2, and that the closer that value was to 0, the more coherent and predictable the system responses would be. I decided to set the temperature parameter to 0 in this experiment.
In GPT-4 I also started to distinguish between 
system prompts
 and 
user prompts
. The 
system prompt
 is the initial text given to the model to establish the context of the conversation. The 
user prompt
 is used to orient the model to the specific goal of the conversation.
These were some of the versions of 
system prompts
 that I tried out:
Actúa como un editor de estilo.
(You are a style editor.)
Actúa como un editor de estilo en un medio de comunicación. Eres un experto en gramática española y un editor en un medio de comunicación. 
(You are a style editor in a media outlet. You are an expert in Spanish grammar and an editor in a media outlet.)
Eres un experto en gramática española y un periodista y editor con amplia experiencia. Tienes habilidad para editar noticias, garantizar el uso correcto y preciso del lenguaje, la redacción y la ortografía. 
(You are an expert in Spanish grammar and an experienced journalist and editor. You are skilled at editing news stories, ensuring correct and accurate use of language, writing and spelling.)
Eres un experto en gramática y un periodista y editor con amplia experiencia. Tienes excelente redacción y ortografía. 
(You are a grammar expert and an experienced journalist and editor. You have excellent writing and spelling skills.) 
[This is my favorite so far.]
To build the 
user prompts
 I reviewed the 
La Silla Vacía Styling book
 and identified a list of guidelines that I wanted to check using ChatGPT. What worked best for me was to create a prompt for each rule and, when I checked that the prompt worked, I tried to build larger prompts that integrated instructions that had worked separately. In most cases I used 
zero-shot prompts
, those in which I give the system a description or an indication of what I expect it to be able to do, without introducing specific examples.
To test the prompts I prepared a set of five test texts and intentionally added the errors I wanted the system to correct. If the system did not correct the errors, I made adjustments to the prompt and tried again and again. In none of the cases did I tell the system what it was doing wrong, I just kept trying. When I succeeded, I ran a real time test with 15 articles. By real time test I mean that when an article was ready to be published I checked it with ChatGPT and took the suggestions that were relevant. This was one of the prompts I used on real time tests.
Corrige typos, redundancias y palabras repetidas. Corrige cualquier error en el uso de signos de puntuación. Nunca separes el sujeto y el predicado por una coma. Usa comas antes y después de la información adicional. Usa comas después de expresiones de enlace. Corrige cualquier error en la conjugación de tiempos verbales. (
Corrects typos, redundancies and repeated words. Correct any errors in the use of punctuation marks. Never separate the subject and predicate by a comma. Use commas before and after additional information. Use commas after linking expressions. Correct any errors in the conjugation of verb tenses).
Since my goal was to have the corrections in list format, I used a second prompt to accomplish this. My first intuition was to create a single 
user prompt
, but as I did not get good results, I decided to use separate prompts. This is how this second prompt evolved:
Indícame qué correcciones realizaste.
(Tell me what corrections you have made)
Indícame todos los cambios que realizaste en el texto.
(List all the changes you have made in the text.)
Lista, uno a uno, todos los cambios que realizaste en el texto
(List, one by one, all the changes you made in the text).
[This is my favorite so far]
Note:
 By the end of May, when I was checking the grammar for the Spanish version of this article, I noticed that the prompt above was not working as well as before. The system started to tell me “No specific text to correct was provided in the above request. Please provide text with errors so that I can make corrections and list the changes made”. I did a little adjustment to the prompt and it fix the problem:
Lista, uno a uno, todos los cambios que realizaste en el texto anterior.(List, one by one, all the changes you made in the previous text).
On the left, article with errors added; on the right, ChatGPT-4 corrections and suggestions.
Corrections and suggestions listed.
What caught my eye
Useless styling corrections:
 Our 
En vivo section 
uses Colombian political jargon. Since ChatGPT-4 was not trained with the writing style of La Silla Vacía, in each test we obtained suggestions that, although grammatically correct, were not useful.
Non-corrections
: in some cases, the system presented as corrections some sentences to which no modifications had been made, i.e. the version before and after the “correction” were exactly the same.
Handling of direct quotes:
 the system was making non-essential changes to direct quotes that were in quotation marks. I understand that I 
would need additional prompts so that the system could treat direct quotes differently
. I did not do additional testing for this issue.
No hallucinations or additional information:
 in none of the 20 tests (those of introduced errors nor the real time ones) did the system add or omit information different from that provided.
English grammar in the corrections:
 Spanish grammar indicates that the period, comma and semicolon are always written after the closing quotation marks, in English grammar they are written before. Although this whole exercise was done in Spanish (texts and prompts), when listing the corrections from the system, in some cases, it uses English grammar rules. 🤔
Unexpected version changes: 
this experiment and Nick’s feedback made me realize the importance of being aware of version changes in ChatGPT. As users, we don’t have any control over the underlying system and this could change versions without us even realizing which could impact performance of prompts.
Conclusions
In summary, the tests allowed us to correct errors in the use of commas, errors in the use of capital letters, conjugation errors, typographical errors, eliminate repeated words and clarify some sentences. We also received style suggestions that were not relevant to us, and the model presented non-corrections to us. These results are based on tests with GPT-4 in chat mode in the OpenAI Playground; the parameters used were Temperature 0, Top 1, Frequency penalty 0 and Presence penalty 0. The test was run between April and mid-May 2023.
Finally, and although I think the tool is useful, I wonder if Playground is the best interface to use on a day-to-day basis in newsrooms for grammar and spell checking. Perhaps this would be better done by another system, something with a more user-friendly interface and the desired parameters preset. At the same time, I wonder how many different instructions I can give the system in the same prompt without affecting the quality of the result. I will continue experimenting and sharing my findings in my blog 
nochesdemedia.com
***Acknowledgements:
 Thanks to Nick for his patient support, and to María José Restrepo, journalist at La Silla Vacía, for her help in testing the prompts.]]></content:encoded>
</item><item>
<title><![CDATA[Building a GPT-4 Powered Google Docs Extension for News Quiz Generation]]></title>
<link>https://generative-ai-newsroom.com/building-a-gpt-4-powered-google-docs-extension-for-news-quiz-generation-34b7e6bcdc4d?source=collection_home_page----df04cefca135-----65-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Prompt Engineering]]></category><category><![CDATA[Quiz]]></category><category><![CDATA[News]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nikita Roy]]></dc:creator>
<pubDate>2023-06-08</pubDate>
<atom:updated>2023-06-08</atom:updated>
<content:encoded><![CDATA[Building a GPT-4 Powered Google Docs Extension for News Quiz Generation
Nikita Roy
Follow
7 min read
·
Jun 8, 2023
--
Listen
Share
News quizzes offer an engaging way to connect with our audience and repurpose our news content. However, creating these quizzes requires a significant investment of time and resources to ensure that each question is carefully crafted and relevant. That’s where we saw the potential of generative AI in helping automate this workflow. 
The NRI Nation
 has integrated the capabilities of generative AI, specifically GPT-4 to streamline the process of crafting news quizzes, enabling us to deliver our news to our audience in an engaging format that extends our reach.
Developed in-house, ‘AI Assist’ is a Google Docs extension offering our team the ability to access the power of GPT-4 within their workflow. The tool is currently being tested for multiple uses such as creating headlines, summaries, SEO metadata, social media posts, and news quiz questions. As with simpler tools like spell check, we see this as just another tool to help our newsroom in producing news. For the Generative AI Newsroom Challenge, we specifically focused on creating an effective prompt to generate quiz questions, which was incorporated into our Google Docs extension.
A demo of how our custom Google Docs extension AI Assist works to create quiz questions
For our news quizzes, we decided to focus on creating multiple-choice and multiple response questions. Before we began testing out prompts, we clearly defined a set of criteria to ensure the quality of the generated question. We used the following criteria to judge the question:
Relevance
: Is the question directly relevant to the main objective of the article?
Clarity
: Is the question written clearly and is it easy to understand? Does the question have any ambiguity, jargon or misleading phrasing?
Answerability
: Are the incorrect answer choices plausible and relevant? Is the correct answer clearly distinguishable from the rest of the choices based on the information in the article?
The quiz questions generated were categorized into three groups: publishable as is, publishable with minor edits, and not publishable. We also recorded the criteria missed in the question to help refine the prompts.
We began systematically experimenting with crafting different prompts using ChatGPT to produce the desired output and decided to focus on multiple choice questions first. Initially, we started off with a general prompt that asked ChatGPT to generate a multiple-choice question based on a news article. But, the questions generated were often not publishable, as ChatGPT frequently missed the article’s main point and would focus on an obscure detail within the article.
We then revised our strategy by specifically giving ChatGPT the criteria mentioned above as part of the prompt. While the results improved, there were still many instances where the AI would generate questions based on obscure details in the article.
The breakthrough came when we asked ChatGPT to generate a prompt for itself to create a multiple-choice question for the news article. This approach resulted in high-quality multiple choice questions consistently. When comparing the ChatGPT generated prompt to our previous prompts, the key improvement made was explicitly providing a one-line summary that focused on the main objective of the article. This made us realize the importance of being specific when prompting ChatGPT in order to help guide it towards producing more accurate outputs. However, from time to time, the questions were still not meeting the relevancy criteria or were having slightly unclear options.
So based on guidance received as part of the Generative AI in the Newsroom Challenge, we transitioned to using GPT-4 directly in the 
OpenAI playground
 and added a system prompt which said “You are a precise journalist and editor”. The reason behind this was to guide GPT towards creating more accurate questions.
A breakdown of the prompt used to generate multiple choice questions
The majority of the questions generated using this prompt [1] were good enough to be published as is. A few of the questions still required slight revisions for clarity and relevance to the objective of the article. We noticed a unique situation where one question that was about a new bill being introduced didn’t work well as a multiple choice question as it had multiple focus points in the article, but the question only focused on the final point of the bill. This showed that in some cases, questions with multiple responses are needed.
Our attention then shifted to multiple response questions. We reused our successful prompt for the multiple choice question and just replaced “multiple-choice” with “multiple response” in the prompt. However, these attempts were unsuccessful, as GPT was often generating questions where all answers were correct. This led us to further modify the prompt, specifying the requirement to provide at least one incorrect answer choice [2]. As a result, we observed an improvement in the quality of generated multiple-response questions.
In order to further streamline the process of generating a quiz question, we attempted to automate the selection between multiple-choice and multiple-response questions by asking GPT to decide. However, this attempt was unsuccessful as GPT-4 seemed to have a clear bias towards producing multiple choice questions, consistently generating them for all the ten articles we tested. This highlighted the continuing need for human editorial oversight in the final decision-making process.
In response, we concluded that our AI Assist tool would generate both types of quiz questions, leaving the editor to review the options and choose the most suitable question. We also automated the task of a one-line summary of the article for inclusion in the prompt.
Process Diagram of the Google Docs Extension Workflow for Generating Quiz Questions
We tested out our first AI-Assist generated quiz questions with ten articles. Based on the experiment, we noticed that the output of multiple choice questions outperformed the output of multiple response questions in terms of quality. The multiple response questions were still not able to focus on the exact objective of the article, highlighting that not every news article was appropriate for a multiple response question.
Evaluating the quality of generated questions based on question type for 10 news article
Based on our experiments with prompt engineering for news quizzes we had a few takeaways:
Have an iterative process
: Continually refine and adjust prompts until you consistently obtain desirable results.
Leverage GPT’s own power: I
f the prompts are not yielding the expected results, consider requesting ChatGPT to create a prompt for itself. This can inspire new ideas and further refine the prompt.
Precision is key:
 Ensure that the prompts are specific and clearly define the article’s objective, the quality criteria for the question, and the number of incorrect answers required. A detailed prompt is more likely to generate the expected output.
Optimize with a system prompt:
 Assign a specific role to GPT via the system prompt as that tends to help create more accurate responses.
The integration of generative AI into the newsroom shows promise for automating several traditionally manual tasks, including the creation of news quizzes. Our experiments highlight the significance of a systematic approach to prompt engineering in achieving high-quality output from LLMs.
But even though LLMs such as GPT-4 demonstrate extraordinary capabilities in content generation, we find that their general training tends to fall short in producing outcomes that align precisely with our publication’s unique tone, voice, and editorial standards. As a result, our team has adopted a two-pronged approach: refining prompts for a diverse range of use cases using GPT-4 to enhance our AI Assist tool, and simultaneously, fine-tuning open-source LLMs for specific tasks within our newsroom.
By training LLMs on data that is representative of our newsroom, we aim to generate outputs that align more closely with our publication’s standards. We anticipate that a fine-tuned LLM will progress beyond mere contextual comprehension to gain a profound understanding of our target audience, the unique nature of our content, and our journalistic principles and guidelines.
So one of our next major steps is to experiment with creating a customized, fine-tuned model for our newsroom, specifically designed to boost the performance of the LLM on specific tasks for our newsroom.
[1] Prompt to create the multiple choice question: “Create a multiple-choice news quiz question based on the following news article about ${summary}. Ensure that the question is directly relevant to the main takeaway of the article, clearly written, and easy to understand. Provide four answer choices, with the correct answer clearly distinguishable from the rest based on the information in the article. Specify what the correct answer is and provide a brief explanation why the corresponding option is correct by referencing facts from the article. Write the explanation as if you were a journalist explaining the news to the public. ### ${article}”
[2] Prompt to create the multiple response question: “Create a multiple responses news quiz question based on the following news article about ${summary}. Ensure that the question is directly relevant to the main takeaway of the article, clearly written, and easy to understand. Provide four answer choices. There should be more than one correct answer and at least one incorrect answer. The correct answers should be clearly distinguishable from the incorrect one based on the information in the article. Specify what the correct answer is and provide a brief explanation why the corresponding option is correct by referencing facts from the article. Write the explanation as if you were a journalist explaining the news to the public. ### ${article}”]]></content:encoded>
</item><item>
<title><![CDATA[Towards Accurate Quote-Aware Summarization of News using Generative AI]]></title>
<link>https://generative-ai-newsroom.com/towards-accurate-quote-aware-summarization-of-news-using-generative-ai-b786493e0c3d?source=collection_home_page----df04cefca135-----66-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Ai Prompting]]></category>
<dc:creator><![CDATA[Alessandro Alviani]]></dc:creator>
<pubDate>2023-06-02</pubDate>
<atom:updated>2023-06-02</atom:updated>
<content:encoded><![CDATA[Towards Accurate Quote-Aware Summarization of News using Generative AI
Alessandro Alviani
Follow
8 min read
·
Jun 2, 2023
--
1
Listen
Share
“A typewriter, quotation marks flying out of it, watercolour painting”, Bing Image Creator
Attribution is a fundamental principle of journalism. Correctly quoting a news source without distorting the sense of what was stated, or worse, adding information the journalist inferred, is an essential skill for any reporter. Even just recognizing a quote can be 
a struggle for algorithms
. And large language models (LLMs) introduce a new challenge: they can potentially make up quotes or misattribute accurate quotes to the wrong sources. This is because of the way they work by predicting the next most likely word in a sequence based on the previous text. These kinds of errors could potentially erode trust in the media and should be avoided.
Our Goal
At 
IPPEN.MEDIA
, we’ve been experimenting with numerous use cases for Large Language Models (LLMs). Some of them include suggesting headlines and lead variations, as well as summarizing or rewriting an article to target different audiences. When it comes to handling quotes, things can easily go wrong in generating summaries or text variations.
During our first round of testing, we discovered that ChatGPT tends to rewrite quotations, even when explicitly instructed not to. When we tried to summarize an article while keeping all the quotes unchanged by adding specific constraints to the prompt, ChatGPT just ignored those constraints and rewrote the quotes. Worse, while the prompts sometimes worked as expected and all the quotes were reproduced correctly, most of the time they were not.
This inconsistency — partly inherent in LLMs, which are probabilistic rather than deterministic models — undermines our goal of building AI-powered tools that both our editors and readers can trust. Even if we have adopted a double-check policy for all texts edited with ChatGPT and other LLMs and follow a human-in-the-loop approach, made-up quotes could still make it through the editing process. This jeopardizes accuracy and trustworthiness.
Our Approach
When it comes to quotes, it turns out that one of the fundamental concepts of prompt engineering — building a prompt that is as specific and clear as possible to define the desired output — may not be enough. As we’ll see later, a multi-step approach combined with instruction redundancy is required.
The instruction we initially added to our ChatGPT prompts for summaries and article versions [
1
] failed in two ways: The original quotes were either rewritten and put in quotation marks or paraphrased. Only in a few inconsistent cases were they correctly detected and left unchanged. After studying a sample set of 12 articles, we discovered that only in a very small fraction of texts (zero out of twelve for GPT-3.5 and six out of twelve for GPT-4) were all citations effectively detected and left unaltered in the new text variation (see Table 1 below in the right two columns).
Next, we tried to break the initial prompt into two steps. We also provided more context by using the system prompt to assign the model the role of an experienced news editor [
2
]. Again, the model usually ended up paraphrasing the original quotes.
Prompt Iteration is Key
In the context of the 
Generative AI in the Newsroom Challenge
 and the feedback we received, we began refining the prompt and moved to OpenAI’s 
Playground
 to take advantage of the additional parameters available there. For instance, we set the temperature parameter to 0 to reduce the variation in the output.
Again, we used a step-by-step approach, but this time we did it differently: we instructed the model to first extract all quotes using the format “” (i.e. look for anything in between quotation marks), and then generate a summary or new text version that included the previously extracted quotes. For clarity, we added the original article at the end of the prompt in step 2. [
3
]
The outcome was much better: no quotes were fabricated. However, even if it effectively extracts all quotes in step 1, the model could still deviate from the prompt and rewrite quotes such as (in German) incorrectly using the present tense rather than the subjunctive verb tense.
Worse, for longer articles with multiple quotes, the model can make two general errors: either it fails to extract all quotes, or it mistakenly identifies non-quote sentences that appear next to or in-between actual quotes as quotes. In general, the longer the text, the fewer the number of quotes recognized. This is particularly true for the GPT-3.5 model.
The real game changers in this iteration process were the next two adjustments. First, the addition of a simple system prompt [
4
] which can be provided to OpenAI’s Chat-based models in the 
Playground interface
. The results improved significantly and support the idea that providing LLMs with more context increases their performance.
The second big improvement came from using GPT-4. Our tests show that OpenAI’s latest model outperforms GPT-3.5 at handling quotes. Using our two-step approach, almost all quotes were correctly recognized. In eleven out of twelve sample articles, all quotations were treated accurately — compared to seven out of twelve with GPT-3.5 (See Table 1). In total, 44 out of 45 single quotes were correctly inserted in the new output text — both for summaries and article variations — compared to up to 32 quotes using our original prompts (without the two-step approach).
Table 1: Original Sample of 12 articles. Task: Rewrite
We then performed testing on a fresh sample of 10 articles and 39 quotes and found that the detection and handling rates were remarkably similar (See Table 2). Using GPT-4, 37 out of 39 single quotes were properly integrated into the new output text in the case of writing article variations — compared to 21 quotes using our original prompts (without the two-step approach).
Table 2: New Testing Sample of 10 articles. Task: Rewrite
When it comes to summaries, GPT-4 is far superior to GPT-3.5. In 11 out of 12 articles, all quotes were correctly included in the AI-generated summary. With GPT-3.5, this rate dropped to 2 out of 12. On the fresh test set of 10 articles performance was lower for GPT-4 though, a result that means there’s still iteration to do on the prompt.
Table 3: Original Sample of 12 articles. Task: Summarization
Table 4: New Testing Sample of 10 articles. Task: Summarization
The instruction redundancy worked quite well. In our two-step approach, we asked GPT-3.5 and GPT-4 in the second step to rewrite or summarize a text and pasted not only all quotes extracted in step 1 but, once again, the original article at the end of the prompt. Even when GPT-3.5 and GPT-4 failed to extract all quotes from the original articles or delivered false positives in the first step (for example, the name of a news outlet published in quotes like “Handelsblatt” was detected as a verbatim quote), they were often able to reconstruct the original quotes in the second step correctly. Out of 12 articles, GPT-3.5 could only extract all quotes in five texts accurately — but managed to include all quotes in a new output summary or article variant in seven texts. GPT-4 was able to process quotes correctly in 11 of 12 texts, while quotes were only properly extracted in the first step in 8 cases.
More importantly for us, no quotes were fabricated, as they were with the very first prompts we had tried in our process.
Limitations of GPT Models
Still, the results are not always perfect. Sometimes both GPT-3.5 and GPT-4 models still recognize single words between quotation marks as verbatim quotations, such as a newspaper’s name, a figure of speech, or the name of a TV show. Even worse, in terms of accuracy and source transparency, GPT-4 sometimes fails to provide the name of the original source, such as a newspaper, even when it has successfully extracted and included all quotations. This is particularly noticeable in summaries and is an important error since without proper attribution a reader may not know the source of the quote. To preserve all quotes, GPT-4 inserted a grammatical error in one case — i.e. it simply put a comma between two quotes. Stylistically, GPT-4 sometimes repeats the words from a quote in the summary, as in the case of the following sentence:
Merz stressed the need for Berlin to have a 
stable government
: “Berlin must have a 
stable
, good 
government
” (original article: Merz continued: “Berlin must have a stable, good government”).
Comparing to Another Model
We decided to re-run the tests with 
Claude
, a ChatGPT competitor developed by Anthropic. We found similar performance to GPT models (See Table 5). Almost all quotes appeared correctly in the new article variant (44 out of 45 quotes) or the new summary (41 out of 45). The main difference to GPT-4 is a stylistic one in the summary task: to preserve as many quotes as possible, Claude sometimes just wrote two quotes side by side, without any introductory or explanatory words (such as “Olaf Scholz said …”). This reduces the quality of the output texts and makes them less than ideal for use in a journalistic context.
Table 5: Original Sample of 12 articles. Tasks: Rewrite and Summary. Tool: Claude.
Main Takeaways and Next Steps
Our results emphasize two crucial learnings. First, having a human-in-the-loop is essential for accuracy. As with all use cases involving LLMs, triple-checking the output remains a cornerstone, especially for summaries. Second, there are no silver bullets or quick fixes; iteration is the best way to get as close as possible to the desired outputs when it comes to prompting.
Looking forward, we are eager to improve the quote process within the new Generative AI team that has recently been established at Ippen. For example, the summary task on the new 10-article sample has highlighted certain gaps and inconsistencies described above that we want to investigate further. As we iterate on the prompt to address these issues, we’ll then sample yet a new set of test articles to evaluate generalized performance.
We also plan to evaluate the quote process on additional Large Language Models, including Open Source LLMs.
[1] 
The text contains quotations; they are enclosed in quotation marks. Quotations must remain as in the original.
[2]
 
You are an editor with 30 years of experience. You need to rewrite the following article into a new text. Think step by step.
Step 1: Rewrite the following original title using vivid but neutral language; Step 2: Make sure that all quotations within quotation marks are reproduced in the new content in the same way. Nothing in quotation marks may be rewritten.
[3] 
1st step:
Extract all quotes between quotation marks such as “” in the following text:
###Text##
2nd step (after the model has extracted the quotes):
Rewrite the article and make sure the following quotes remain unchanged:
“quote”
“quote”
“quote”
Article: ###Text###
[4] 
You are a precise journalist and editor.]]></content:encoded>
</item><item>
<title><![CDATA[How the YESEO App Tested GPT Models for Headline Generation with Users]]></title>
<link>https://generative-ai-newsroom.com/how-the-yeseo-app-tested-gpt-models-with-users-94838bad328?source=collection_home_page----df04cefca135-----67-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Gpt]]></category><category><![CDATA[OpenAI]]></category><category><![CDATA[Headlines]]></category><category><![CDATA[Evaluation]]></category>
<dc:creator><![CDATA[Ryan Restivo]]></dc:creator>
<pubDate>2023-05-11</pubDate>
<atom:updated>2023-05-11</atom:updated>
<content:encoded><![CDATA[How the YESEO App Tested GPT Models for Headline Generation with Users
Ryan Restivo
Follow
6 min read
·
May 11, 2023
--
Listen
Share
Since my 
RJI Fellowship project
 embraced helping newsrooms with SEO best practices via Slack, the 
YESEO app
 has been working to practically apply OpenAI’s GPT technology for newsrooms.
Whereas before users had to think about their keywords, 
YESEO
 allows users to enter their stories, whether published or unpublished, and get analytic help with SEO.
Using the 
/analyze
 command in Slack you can enter a URL and get the main entities extracted and counted as to how prominent they are in your story. YESEO seeks to analyze how relevant your story is to what you put in your headline, helping dig into the keywords in the story and providing data to help think about overall trends.
An example of a URL using the /analyze command in the YESEO app
Meanwhile, the 
/prep
 command centers around pre-publication. A user can enter their text and learn about the most prominent keywords and how they should position their story.
An example of a story using the /prep command in the YESEO app
Over the course of the 
eight month fellowship program
 through the University of Missouri, I dug into the problem that I wanted to solve, but in that timeframe along came a technology that I thought could also provide real value to users: OpenAI’s GPT models.
When I 
started to integrate GPT
 into the workflow for my RJI Fellowship project, the first idea that came to mind was to figure out how to feed the GPT API a story and get some friendly suggestions for newsrooms to try: headlines, descriptions, and subheads. That’s where the green “Suggest Headlines” button in the figure above came in.
Whereas before newsrooms would have to spend time workshopping headlines in a channel, the YESEO app now can spit out five headline ideas that are diverse and give a team options for how they could best attract an audience. The app is really about offering ideas to spur editorial thinking. A user could take a suggestion and adapt it to their publication’s style or specific audience interests. The underlying premise is that the AI’s suggestions can help editors do their job faster and better, freeing up time for other tasks.
This feature I built started out simply, as a “Suggest Headlines” button within a user’s experience. What I found in early data was that over half of all command requests were clicking this button. They wanted to see what the GPT language model would cook up in headlines for their stories.
The GPT option as “Suggest Headlines” in the User Experience
Since 
launch
, 
75.4%
 of both the /
analyze
 and /
prep
 commands have resulted in a user clicking on the “Suggest Headlines” button. Clearly there is solid demand for this feature.
A set of headline examples, generated from GPT, that one newsroom found helpful.
As OpenAI’s GPT models changed from version 3 to 3.5 and now 4, I wanted to test out how effective each model was at giving users what they wanted. From a business perspective, it was important for me to compare since GPT 3.5 and GPT 4 have different cost structures. Is it that much better to use GPT 4 over GPT 3.5 given that 
it’s 15 times as expensive
?
Since March 27, when a user clicks “Suggest Headlines”, they are placed in an A/B test where their request for headline suggestions either goes to the GPT 3.5 or GPT 4 model without them knowing which model is being used. When they click for the suggestion, the script in the app randomizes which model they will get and then shows their headlines using that model. A user is not told what model is being used so that their feedback isn’t influenced by having that information.
After they get their result, a follow-up survey question asks whether the suggestions helped their content and they get three options to choose from:
Survey prompt after “Suggest Headlines” is used.
After every GPT headline suggestion, around 30 seconds later this survey appears, giving a user the option to answer. So far the response rate has been 42.2% to this survey method, spanning across 103 users in 59 different Slack workspaces among 469 requests for headlines run. All of the user data is anonymized in Slack and events come through only when surveys are answered.
The user responses to the survey indicate that GPT 4 suggestions are helpful 61.2% of the time and GPT 3.5 suggestions are helpful 61.0% of the time. While there are three options, the distribution of the choices (Yes, No and Not Sure) is fairly even across both models, with slightly more “not sure” responses and slightly fewer “no” responses for GPT 4.
Overall, it appears that the GPT 4 and GPT 3.5 models produce similar metrics in terms of helpfulness to users. Any the differences between GPT 4 and GPT 3.5 so far in the data appear to be quite small, at least in terms of users’ perception of helpfulness.
These tests were only done using the headline data, but in the YESEO app there are ways to generate descriptions and some subheads for a story as well, so there will always be more opportunities for future experiments.
I would also like to experiment with the prompt more in the future. Currently the prompt is 
“Write five SEO friendly headlines between 50 to 60 characters for this story: <article text>”
, but after learning all this, I may work towards changing the prompt using some of the 
qualitative data I learned through my RJI fellowship project
 as the next step to drive results. I have also yet to use the page’s title data in my prompt, so one opportunity could be to either improve the headline or create other headlines for the 
other titles that we know are important on the page for search
.
In the 
OpenAI API Documentation it states
 that they “do not use data submitted by customers via our API to train OpenAI models or improve OpenAI’s service offering.” Since requests made into YESEO are using that API, they won’t be used to train any models unless I explicitly opt-in to allow OpenAI to use that data, per their policy. The 
YESEO FAQ’s
 document this as well, so users know that my goal is to protect users’ privacy and content.
Since GPT has been a part of the YESEO app, I am proud to see that users are not only getting use out of the generative AI capabilities, but that they are also responsive to helping determine the best model for the Slack app by offering their feedback. Best of all, they are finding the headline suggestions effective in their daily use to help shape their stories.
Ryan Restivo is a
 2022 RJI Fellow
 and creator of the 
YESEO app
, a free Slack bot to help newsrooms with SEO best practices. Follow the YESEO app on 
Twitter
, 
Facebook
, 
LinkedIn
 and 
Instagram
.]]></content:encoded>
</item><item>
<title><![CDATA[The State of AI in Media: From Hype to Reality]]></title>
<link>https://generative-ai-newsroom.com/the-state-of-ai-in-media-from-hype-to-reality-37b250541752?source=collection_home_page----df04cefca135-----68-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Hype]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Generative Ai]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-05-09</pubDate>
<atom:updated>2023-05-09</atom:updated>
<content:encoded><![CDATA[The State of AI in Media: From Hype to Reality
Nick Diakopoulos
Follow
9 min read
·
May 9, 2023
--
Listen
Share
Note: The following was adapted from a keynote that I delivered at the 
Nordic AI in Media Summit
 on May 9, 2023 in Copenhagen.
AI has been a hot topic in the media industry for years, often generating hype and inflated expectations. IBM launched its first automatic news summarization demo in 1958. Given its long lineage, as the technology matures and becomes more integrated into news production, it’s helpful to take a step back and assess the real impact of AI on the media landscape. In this post I’ll talk about AI’s pervasiveness and hype in the news media, exploring what I think are some of the driving factors and implications.
Towards the Plateau of Productivity
Over recent years, AI has been steadily making its way into various aspects of news production, from data mining and automated content generation to distribution, many of which I documented in my book, 
Automating the News: How Algorithms are Rewriting the Media
. Recent examples of successful AI implementations include the 
Washington Post using data mining to generate tip sheets during the 2020 US elections
 and 
Gannett’s Localizer project
 which uses automated content generation to create local content for its 230+ newsrooms. 
Techniques like A/B headline testing
 which leverage machine learning to determine the best-performing headline from a set of headlines, have proven to be effective in improving click-through rates for publishers and are in routine use in thousands of newsrooms. Likewise, smart paywalls, like the one the 
New York Times uses
, are something that can be used to optimize the conversion of casual to paying subscribers.
For a while, at least for the past few years, it’s seemed to me like we were coming out of the hype curve and folks in industry were just doing the work to pin down exactly how AI technologies like natural language generation, recommender systems, and other machine learning approaches could produce actual value, both editorially and for news businesses.
AI applications like A/B testing, automated content production, audio transcription, language translation, and optimized paywalls have become fairly commonplace and increasingly inconspicuous. You don’t usually hear news organizations bragging about that stuff so much anymore. AI’s pervasiveness is evident in everyday tools like Gmail’s spelling, grammar, and personalized writing suggestions. As information processing increasingly involves some aspect of AI, avoiding its influence on news media will be unavoidable. Eventually, AI will become as ubiquitous as the phones in our pockets, and we’ll hardly notice its presence.
Generative AI Hype
Image generated using Adobe Firefly.
Despite these advancements and tendency towards integrating “classic” AI in productive ways, generative AI has brought hype back to the forefront, with concerns about potential harms and misuse of the technology. Generative AI has already been used in various ways in news media and beyond, such as by creating 
synthetic news anchors
 or developing 
video
 content for advertising, including for 
political ads
. Generative models do offer a range of capabilities which can support news work, from analytic tasks like data extraction and 
rating
 to generative tasks such as rewriting and 
summarizing
 documents, or for 
illustrating articles
.
However, generative AI’s limitations, such as inaccuracies in output and potential for 
misinformation
 and 
pollution of the information environment
 are major factors. For instance, 
in some of my initial tests of Bing’s chat search results
 I found there were accuracy issues (including in source attribution) in about half of the queries tested. As generative AI becomes more prevalent, it’s important to focus on how to deploy the technology responsibly and distinguish between hyped uses and real risks and capabilities.
As we look out at all the possibilities that are getting hyped, like generative AI in search, in advertising, and in misinformation, really what we need is a more level-headed approach to thinking through the specific capabilities of generative AI, and how they enable specific newsroom tasks. What I think we need is to very quickly move into the stage of trying to “kick the tires” and see where it could actually be useful. That’s, in part, why I launched the 
Generative AI in the Newsroom Project
.
Hype and the Level of Automation
So how do we make sense of the fact that there is both ongoing AI hype as well as AI technologies that are now routinely used throughout media production?
I think that the degree of automation achievable is a significant factor in distinguishing hyped AI from inconspicuous AI. Highly automatable tasks, which can substitute human labor, tend to fade into invisibility. Once we can delegate a task to AI and trust it to perform at our expectations, we forget about it. At the lower end of the scale, you have automation which complements human labor using interactive interfaces that keep the user in control.
Adapted from “A Model for Types and Levels of Human Interaction with Automation” IEEE Trans. On Systems, Man, and Cybernetics. 2001
The hype-cycle feeds on the uncertainty of whether or not a complete task (or a whole collection of tasks, such as comprises a job) is automatable. Capital likes hype because in uncertainty you can marshal resources based on unclear benefits and competitive fears. At the same time the hype is reinforced by fears from labor, which would prefer lower levels of automation (in general) to maintain autonomy and a sense of control, and also not to lose a sense of purpose or gainful employment.
To be sure, sometimes AI does advance enough to move a task up in automatability. But we constantly need to assess whether some task is REALLY automatable or if maybe parts of it are, or if perhaps it’s unreasonable to (try to) automate it at all. The hierarchical nature of tasks means we need to dig into tasks and sub-tasks to see if there’s some part that can be done with automation.
In short, pervasive and invisible applications of AI have graduated into the upper reaches of automation by proving largely reliable and trustworthy. Tasks that continue to generate hype, like summarization (which, again, IBM began working on in 1958), are not yet ready for high levels of automation while upholding rigorous editorial standards. But AI might be on the cusp, which is always the big question.
Automatability and Generative AI
So where is generative AI in terms of its ability to move up the automation hierarchy?
We can ask it, which is what I did here. I prompted it for tasks involved in news production, then asked it to do a hierarchical task analysis and rate automatability using generative AI while maintaining professional standards.
A task analysis of news production tasks, including color-coded ratings of task automatability as provided by GPT4
Where you see green that’s a task that GPT4 thinks could be automated while maintaining high professional standards. What you’ll see immediately is that there’s not a lot of green. It’s for the stuff that you’d probably consider to be pretty boring, certainly not the hyped stuff.
But the other trend is that you see a lot of yellow and orange. These are tasks where AI can definitely help but where you’ll need a human in the loop to supervise. Something like creating graphics, charts, or infographics. Yes, generative AI can definitely help with that, but you’ll need an editor.
And you’ll see some red which are tasks that AI is really just not going to be helpful for at all right now.
And of course, if we drill into these tasks, because tasks are always hierarchical, some sub-tasks will also be more or less automatable.
The main point here is that the vast majority of news tasks can be helped to some extent with AI, but they’re not yet at a point of high automatability. As a result, AI will often only disappear to the extent that it becomes invisible within the tools that journalists use.
Now you shouldn’t necessarily trust GPT-4’s ratings of these tasks. A good bit of the work before us is actually figuring out which of these tasks actually CAN be automated and which truly do need a human in the loop. But I think overall it’s probably a reasonable starting point.
Newsroom Tools: Design, Labor, and Management
Where this all leads is that what we really need to be creating are AI driven tools where people using the tools are left in control of overall tasks but are supported by AI that can automate sub-tasks at a high degree of reliability and trustworthiness. The key word to emphasize here is really DESIGN.
Newsrooms need to be thinking about designing the future of work and how to hybridize human abilities, control, and oversight with the state of the art in AI capabilities. As the frontier shifts in what can be fully automated, tasks may need to be re-architected. When the first electric motors were developed, it took years for factories to realize that they needed to entirely redesign their layout to make best use of those motors. The same is true here, we need to, in some cases, entirely re-imagine how news is made. This needs to be creative and iterative, and will require building specialized interfaces so that people can retain autonomy and control.
And newsrooms need to think about how to design AI technologies that incorporate news values, so that the outputs of these systems are trustworthy with respect to editorial needs, and so that ethical commitments of the profession are still met. In addition this could be important so that the news media can retain some independence from big tech companies.
This hybridization of work has significant implications for labor in the media industry. AI technologies often create new types of work related to maintaining and validating systems. As a result, individuals may need to reskill or upskill to adapt to these changes.
Additionally, managing the transition to AI-driven news production requires careful consideration of the impact on workers and the value derived from AI. Management is crucial here.
Let’s consider the management around something like transcription AI. If I’m a reporter using transcription AI, I can do, let’s say, twice as many interviews in a day because I don’t need to transcribe them myself. Management could say, well, now you’re going to write twice the number of stories. Or, it could say, now you’re going to have twice as many sources for each story, and improve the quality and diversity of the sourcing. Or it could also say, you’re going to write the same number of stories, but you’re also going to be asked to do some other job that wasn’t part of your original job description. All of these options have different implications for the news organization and the type of value it reaps from AI, and for the individual worker and how it changes their experience of the work.
Management scenarios for transcription AI
We’re already seeing strikes in Hollywood from the Writers Guild of America, with AI being a main topic where they want to bring management to the table for negotiations. Unionized journalists will be better off in the long run (unionize now if you haven’t already!). Now could be the time to bring management to the table to ensure the transition to AI-augmented news production is managed in ways that are not unfavorable to labor.
In closing, moving past the hype cycle and focusing on the real potential of AI in media is crucial. This involves assessing the automatability of tasks, designing AI tools and interfaces that support human workers, support skills development, and managing the transition strategically and humanely. By doing so, the media industry can reap the benefits of AI technology while maintaining editorial standards and ensuring a smooth transition for its workforce.]]></content:encoded>
</item><item>
<title><![CDATA[Adobe Firefly for Generating Visual Illustration?]]></title>
<link>https://generative-ai-newsroom.com/adobe-firefly-for-generating-visual-illustration-209d12e4457e?source=collection_home_page----df04cefca135-----69-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Illustration]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-05-08</pubDate>
<atom:updated>2023-05-08</atom:updated>
<content:encoded><![CDATA[Adobe Firefly for Generating Visual Illustration?
Nick Diakopoulos
Follow
4 min read
·
May 8, 2023
--
Listen
Share
As we recently wrote about on the blog 
here
, legal concerns have created some uncertainty around whether popular image generation models like Stable Diffusion, DALL-E, and Midjourney could be violating artists’ and creators’ copyright in the course of their training. Whether the fair use provision for text and data mining is valid will likely be decided in court cases.
With that as background, Adobe has recently entered the market with its 
Firefly
 generative image tool, which they’re largely 
touting
 as a more responsible and ethically-conscientious alternative. Its 
FAQ
 notes that “The current Firefly generative AI model is trained on a dataset of Adobe Stock, along with openly licensed work and public domain content where copyright has expired.” The presumption then is that all of the content used to train their model has been used legally. They note, “For Adobe Stock contributors, the content is part of the Firefly training dataset, in accordance with Stock Contributor license agreements.”
Here’s the 
license agreement
 for Stock contributors. But as this 
tweet
 indicates, not every creator agrees it was kosher for Adobe to train on their Stock contributions. So, even if Adobe is legally covered, the ethics here are still murky. As a show of good faith, Adobe suggests that it is “developing a compensation model for Adobe Stock contributors, and will share the details of this model when Firefly exits beta.” TBD what actually happens here but it’s an interesting direction.
Content Labeling
One aspect of the system that Adobe is pushing is that there is metadata included in generated images that label them as the output of AI models. They write, “When downloading an image generated in Adobe Firefly or submitting it to the Firefly Gallery, minimal, non-identifying Content Credentials that let people know it was generated with AI will be included. This is meant to help promote transparency around content generated with AI tools like Adobe Firefly.” This 
process
 “use[s] cryptographic asset hashing to provide verifiable, tamper-evident signatures that the image and metadata hasn’t been unknowingly altered.” Image metadata can be read using an 
online content authenticity tool
.
Content authenticity information is a potentially nice feature for newsrooms interested in being transparent with their audiences about whether and how they’ve used Firefly generated imagery as part of illustrations. If a newsroom editorial policy was, for example, to always disclose any use of generated imagery, then the content authenticity process and standards that Adobe uses would allow them to disclose that information.
Bias Mitigation
Another dimension of responsible AI that Adobe is aiming to address with Firefly is the representativeness of the images generated such that social stereotypes aren’t perpetuated. This is a common issue with image generators since they output images in-line with social biases they’ve learned from their training data. Just look at what Midjourney gives you when you prompt it with “a professor teaching a classroom full of students”. Apparently only white guys (usually old white guys) are professors.
Output from the Midjourney model prompted with “a professor teaching a classroom full of students”
In contrast, the Adobe Firefly FAQ notes that “In addition to training on inclusive datasets, we continually test our model to mitigate against perpetuating harmful stereotypes.” The Firefly results for the same query reflect greater diversity of individuals in the role of professor. To get the same diversity of results from Midjourney you’d need to explicitly prompt it to represent a greater diversity of people as professors.
Output from the Adobe Firefly model when prompted with “a professor teaching a classroom full of students”
A Step Forward with the UI
As others have 
pointed
 
out
, a chat interface is not at all an intuitive interface for creative work. A classic critique of these interfaces (which hearken back to early command line user interfaces) is that they’re hard to learn. They demand that the user 
remember
 commands, or at least constantly go look them up in documentation. Graphical user interfaces were a major advance because they made it easier for people to point and click at what they wanted and receive interactive feedback so they could understand and intervene in what the system was doing.
I do think that the interface for Adobe Firefly is easier to get started with than either 
MidJourney
, which is accessed using commands in a Discord chat, or Stability AI’s 
DreamStudio
, which has recently improved but is still not easy to get the hang of. Here’s the UI for Firefly:
An overview of the Adobe Firefly user interface.
What’s nice is that Firefly gives you a visual palette of common options, including the content type as well as a whole slew of style options corresponding to artistic styles, techniques, effects, and so on. There are also menus to select from color and tone, lighting, and composition. This makes it a lot easier to explore different visual options since you don’t need to remember specific terms for prompting the model. The level of “prompt engineering” needed is reduced a bit.
Firefly might be interesting for newsrooms to try, especially if they’re new to generative imagery and want a more approachable user interface, to have generated images verifiably labeled, and to use a model that tries to address visual representativeness issues. Still, the tool can’t yet be used for commercial media, with Adobe 
emphasizing
 that “generative imagery and text from Firefly should not be used for commercial purposes” during the beta.]]></content:encoded>
</item><item>
<title><![CDATA[Document Summaries in Danish with OpenAI]]></title>
<link>https://generative-ai-newsroom.com/summaries-in-danish-with-openai-cbb814a119f2?source=collection_home_page----df04cefca135-----70-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Naturallanguagegeneration]]></category><category><![CDATA[Automation]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Tore G. C. Rich]]></dc:creator>
<pubDate>2023-05-04</pubDate>
<atom:updated>2023-05-04</atom:updated>
<content:encoded><![CDATA[Document Summaries in Danish with OpenAI
Still a need for a human in the loop
Tore G. C. Rich
Follow
8 min read
·
May 4, 2023
--
Listen
Share
Example of a published automatically generated article. In the blue box on the right, you can see an AI generated summary of a management report.
Introduction
On Denmark’s most visited news site Ekstrabladet.dk, we 
publish
 automatically generated local news articles in Danish as a part of the 
Platform Intelligence in News (PIN) project
. In these automatically generated articles we cover topics including companies’ financial reports, inspection reports for food establishments, and real-estate sales. The articles are generated with rule-based NLG (Natural Language Generation) using data retrieved automatically via APIs.
Yet this rule-based approach using structured data doesn’t offer the complete story that we want to share with our readers. For instance, companies’ financial reports often includes information from management that explains why revenue, results, and equity turned out the way they did. This is important context to include in our articles. At the same time, these documents are often too long, not always formatted with high grammatical quality, and are sometimes also written in English.
To incorporate information from these documents into our automated articles we have been experimenting with the latest GPT models (text-davinci-003, gpt-3.5 turbo and gpt-4) from OpenAI to generate clean, well-written summaries in Danish to include in the articles. However, these AI-generated summaries are not always acceptable according to our editorial standards. And while we publish the rule-based auto-generated articles directly on Ekstrabladet.dk, we do not add the AI-generated summaries to the articles before they have been reviewed by a human.
I hope to reach a point where we can trust the summaries and publish them directly without a human in the loop, but we’re not there yet. In the rest of this post I examine the quality of these AI-generated summaries and the ongoing need for a human in the loop.
Setup
The rule-based automated articles on financial reports were automatically generated and published on 
Ekstrabladet.dk
. For this, I built a pipeline called MAGNA (Monitoring and Auto-Generation of News Articles) to automate and monitor the process, and to monitor and collect data from APIs, and I used online NLG software from 
AX Semantics
 to generate the articles from the collected data. At the same time, a summary was automatically generated with the use of the OpenAI API. We always used the latest available GPT-model, and set the temperature at 0.5.
For the human in the loop, I built a user interface in MAGNA to evaluate the summaries, leading to an “accept” or “reject” decision. Each rejected summary was accompanied by a reason for its rejection. For each rejected summary, a new summary was generated, the reason for the rejection was not given to the model.
The prompt was continuously updated with learnings from the failed summaries, which I explain further below.
Data overview
The data is exclusively from summaries of management reports in financial reports in the period 20 January to 11 April, 2023. In this period 92 articles were given an accepted summary. 54 of the articles were given an acceptable summary in the first attempt (59%), while the remaining 38 articles took several attempts. A total of 193 summaries were generated, of which 101 summaries were rejected. We didn’t give up on any articles.
The rejected 101 summaries fell into the following categories [number of articles] (number of summaries):
Missing important information [14] (28)
Incorrect summary [14] (23)
AI evaluates content [9] (17) 
(e.g. the model states that the company has had a successful year)
Poor language [9] (12)
Irrelevant information [4] (6)
AI adds own content [2] (7)
Wrong translation [2] (4)
Mentioning what is missing [1] (2)
(e.g. the model adds: “the company does not provide further information about the reason for the unsatisfactory result”)
AI not aware of public knowledge [1] (1)
(the model has limited knowledge of the world and events after 2021, which is evident in the summary. In this case, the model is unaware of a fatal accident that is widely publicized.)
Summary too long [1] (1)
Analysis
In the end, 100% of the articles received a summary, however only 59% of the articles had a summary accepted on the first attempt. 41% of the articles went through 2 to 12 attempts before an acceptable summary could be included in the article.
A total of 193 summaries were generated, of which 48% were accepted and 52% rejected. However since the model was not told why a summary was rejected, follow-up summaries often contained the same errors. For example in the case of the one article with 12 summary attempts before a summary was accepted: Six times a summary was rejected because text-davinci-003 did (the same) evaluation in the summary (in this case the model wrote, that the company had had a strong year), and in the five remaining rejected summaries the same information was missing in the summary. With a temperature setting of 0.5 the model was expected to generate variations using also less likely tokens. If lower probability tokens had generated the erroneous parts of the summary, there should be a chance that that part of the summary would be changed in a new summary.
In the case of the one article with 11 summary attempts (again text-davinci-003): Eight times the summary was missing the same word: “Covid 19”, one summary contained an evaluation and one summary had poor language that made part of the text unclear.
In general, the main reasons for rejected summaries were incorrect summaries, i.e. summaries with errors of some kind, and summaries missing important information present in the original text, i.e. information erroneously omitted from the summary.
But articles were also often rejected due to poor language and unwanted evaluation of the content. We did not decide whether the model’s evaluation of the content was correct or not (e.g. that the company had had a successful financial year), as we had asked for a sober (Danish: “nøgternt”) summary, all evaluation made by the model was seen as an error.
Model comparison
As mentioned we went through three GPT-models in the period of our testing: text-davinci-003, gpt-3.5 turbo and gpt-4. Most of the summaries (73%) were generated with text-davinci-003. A small number (6%) with gpt-3.5 turbo and a greater share with gpt-4 (21%).
If we look at the summaries generated by each model, we see that text-davinci-003 showed a much greater error rate than both gpt-3.5 turbo and gpt-4. Of the summaries generated with text-davinci-003, 57% were rejected while the numbers were 36% with gpt-3.5 turbo and 39% with gpt-4, suggesting that the overall quality may be higher in the newer chat models.
However with a total of 193 summaries: 141 text-davinci-003, 11 gpt-3.5 turbo, and 41 gpt-4 the dataset is too small to say anything conclusive about this. In addition we continuously improved the prompt and the system prompt which likely reduced the number of rejected summaries.
Prompt evolution
During the period we also experimented with 12 different versions of the prompt. In addition, gpt-3.5 turbo and gpt-4 also have a system prompt, of which we used four versions.
The prompts generally get longer with each version, adding instruction based on failed summaries. The additions include statements like: “without typos”, “focus on unusual conditions”, “how the year has been”, “what the future holds” and “ignore irrelevant information that does not relate to the company”.
Also, the system prompt grows larger with each new version. At the beginning it just said “You are a skilled journalist who is about to write an article for Ekstra Bladet” but at the OpenAI presentation of gpt-4 on March 14th I noticed that in an example they wrote “​​You are a TaxGPT, a large language model trained by OpenAI” in a system prompt, although they really had not trained a TaxGPT. I therefore changed the system prompt to: “You are JournalistGPT, a large language model trained by OpenAI, you are writing an article for the Danish newspaper Ekstra Bladet.” This prompt, however, could not be used with gpt-4 as the result was too tabloid style, and I therefore replaced “Ekstra Bladet” with “a large Danish newspaper”. This indicates that gpt-4 has more emphasis on the system prompt than gpt-3.5 turbo and/or that gpt-4 better knows Ekstra Bladet.
In sum
100% of the articles were given a summary, 59% in the first attempt. It is clear that GPT models now have the capability to generate acceptable summaries in Danish, both when the original language is English and when it is Danish. In this way we were able to add very useful content to the rule-based auto-generated articles, and augment the articles with insights and explanations presented in the company’s financial reports.
However the goal of reaching a point where we can trust the summaries and publish them directly without a human in the loop seems very far away. A custom user interface for accepting and rejecting the summaries combined with a database of accepted summaries, rejected reasons and prompts used proved to be a useful tool for learning and improving the prompting.
A few of the texts took many summary attempts before a summary was accepted as the model was not given the reason for the rejection. I have subsequently successfully experimented with giving gpt-4 the rejected reason and telling it to write a new summary. This approach promises to greatly reduce the number of summaries per article.
The quality of summaries seemed to improve with newer models, but it is worth noting that the data set was too small to say anything conclusive about this, and in addition the continuously improved prompts likely led to a reduced number of rejections.
Below are the main categories of rejected summaries, in future approaches it will be worth keeping an eye on whether the model delivers similar types of errors:
Missing important information
Incorrect summary
AI evaluates content 
(e.g. the model states that the company has had a successful year)
Poor language
Irrelevant information
Appendix 1
Summaries per model
Appendix 2
Number of summaries per article
54 articles have only 1 summary
15 articles have 2 summaries
8 articles have 3 summaries
etc.
Appendix 3
Reject types
Articles can have multiple summaries with different reject types.
Only one reject type per summary is registered.
Example: 9 articles have rejected summaries due to poor language, but 12 summaries are rejected because of poor language. This means that multiple summaries with poor language have been generated for the same article.]]></content:encoded>
</item><item>
<title><![CDATA[A Primer and FAQ on Copyright Law and Generative AI for News Media]]></title>
<link>https://generative-ai-newsroom.com/a-primer-and-faq-on-copyright-law-and-generative-ai-for-news-media-f1349f514883?source=collection_home_page----df04cefca135-----71-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Copyright]]></category><category><![CDATA[Generative Ai]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[Legal]]></category>
<dc:creator><![CDATA[João Pedro Quintais]]></dc:creator>
<pubDate>2023-04-26</pubDate>
<atom:updated>2023-04-26</atom:updated>
<content:encoded><![CDATA[A Primer and FAQ on Copyright Law and Generative AI for News Media
João Pedro Quintais
Follow
18 min read
·
Apr 26, 2023
--
1
Listen
Share
Note: This post was co-authored with 
Nick Diakopoulos
.
By now you might have heard of 
some
 of the 
lawsuits
 
filed against AI companies, alleging that they infringed on copyright in the training of their models. AI-generated vocals are 
roiling the music industry
, with platforms acting to take down infringing content. And some 
news executives are even calling for compensation for the use of their content
 in the training of such systems.
From the perspective of authors and copyright holders, there is a clear 
concern
 that generative AI tools are built on the unauthorized and non-remunerated use of their works, while at the same time negatively impacting their livelihood. At the same time, it is also 
noted
 that these tools benefit many artists and content creators, whose interests should be considered when regulating these technologies from a copyright policy perspective. Others still are 
concerned
 that legal intervention at this stage would lead to market concentration and “make our creative world even more homogenous and sanitized”.
Are AI companies really violating copyright law during the model training stage? And what then about the copyright issues on the outputs of these models? These kinds of questions reflect some of the most pressing and pertinent legal issues that journalists need to consider in their use of generative AI.
In this post we’ll parse these legal issues, first offering some background on copyright law and AI models, and then reflecting on some more specific and pragmatic questions that may impact how you think about using the models in different news production tasks. (
Note: the following is not legal advice and is meant for educational purposes.
)
Some context on copyright law
Despite what you might have heard on Twitter, there is not one copyright law for the entire world. Rather, copyright law is territorial. To be sure, there is significant international harmonization through a number of International Treaties that set out minimum standards on certain aspects of copyright law, and which are then implemented into national laws (see 
here
 for some basics). The same is true, for instance, at the regional level, where the EU has significantly harmonized copyright law through a number of 
Directives
 (listed 
here
); but even then each Member state must implement these directives into their national laws, sometimes leading to different rules. The upshot is that different countries have different laws to regulate copyright, including all of the questions we address below regarding generative AI tools. Moreover, due to different legal traditions, EU member states’ laws take a different approach to, for example, US law regarding key issues in this area. Our analysis below will mostly focus on EU copyright law and simplify the legal issues (so bear with us if you’re a legal expert!).
Inputs and Outputs: How to approach copyright law issues with generative AI
One way to consider the copyright aspects of generative AI tools is to divide them into legal questions that deal with the input or training side vs. questions that deal with the output side.
From the 
input
 perspective, the main issue relates to the activities needed to build an AI system. In particular, the training stage of the AI tools we are considering here requires text and data mining (TDM) of copyrighted works. In the EU, these activities are mostly regulated by two TDM exceptions in the 2019 
Copyright in the Digital Single Market Directive
, which cover TDM for scientific purposes (Article 3) and what is called commercial TDM (Article 4). For models like 
Midjourney
, 
Stable Diffusion
, 
Dalle-E
, or 
Firefly
, the relevant provision would be the commercial TDM exception.
In the US, absent a specific TDM exception, the legal question is whether these activities qualify as 
fair use
. In the aftermath of cases like
 Authors Guild v. HathiTrust
 and
 Authors Guild v. Google
, it has been argued that the US doctrine of fair use allows for a significant range of TDM activities of in-copyright works (see
 here
 and
 here
; for a critical framing of questions of fair use in dataset creation, see 
here
). The result is that US copyright law is arguably one of the most permissive for TDM activities in the world, especially when compared to laws that rely on stricter exceptions and limitations, like the EU (see
 here
). This arguably makes the US an appealing jurisdiction for companies to develop generative AI tools.
From the 
output 
perspective, a number of copyright questions are relevant. Is an output generated by Midjourney, Stable Diffusion, Dalle-E, or Firefly protected by copyright? Does such an output infringe on a copyrighted work of a third party, especially those works “ingested” during the training stage of the AI system? Under US law, is the output a “
derivative work
” of the “ingested” copyrighted works? Do any copyright exceptions apply to outputs that might otherwise infringe copyright?
Some of these input and output questions are already being litigated in the US and the UK, most notably in a 
class action litigation
 against providers of Stable Diffusion (see 
complaint
 and 
motion to dismiss
), as well as in lawsuits brought by Getty Images (reported 
here
). On the related topic of software generation, it is also important to mention the 
class action lawsuit
 against Microsoft, GitHub, and OpenAI concerning the GitHub Copilot (reported 
here
; case updates 
here
).
In the US, the US Copyright Office (USCO) has taken a position on some of these issues. For example, in its “Zayra of the Dawn” 
decision
, concerning a graphic art novel, the USCO refused registration of images generated by an AI tool while allowing registration of the accompanying human authored text (see also the earlier 
Thaler 
decision
). In a more recent development, the USCO issued a Copyright Registration 
Guidance
 on works containing material generated by AI. Although it is beyond the scope of this post to discuss this Guidance in detail, the document emphasizes the application of the 
human authorship requirement
 in this context and draws some concrete conclusions regarding the application of the same to popular generative tools. The following passage illustrates the approach taken to the question of whether an AI output is protect by copyright:
“when an AI technology receives solely a prompt from a human and produces complex written, visual, or musical works in response, the “traditional elements of authorship” are determined and executed by the technology — not the human user… When an AI technology determines the expressive elements of its output, the generated material is not the product of human authorship. As a result, that material is not protected by copyright and must be disclaimed in a registration application” (p.4).
In the EU, there has been no litigation to date that we are aware of. However, there seems to be an appetite for that, at least if one considers the 
manifesto
 of artists and rights holders in the European Guild for AI Regulation. In any case, there is plenty of scholarship and analysis on the topics above (reported e.g. 
here
), which we rely on in addressing the frequently asked questions below.
Q: For image generation models like Midjourney, Stable Diffusion, Dalle-E, or Firefly are the outputs copyrighted?
In theory, AI outputs may be copyrightable if they meet the legal requirements for protection. The general direction of both EU and US law is to emphasize human originality as expressed in an output.
In the EU, such activities would have to meet the originality requirement, as interpreted by the Court of Justice of the EU. The legal formula the Court relies on is that protected subject matter must express the author’s “own intellectual creation” and their “free and creative choices” (further analysis 
here
). In the US, judging from the USCO’s guidance, the approach is similar or at least leads to similar outcomes:
“…a human may select or arrange AI-generated material in a sufficiently creative way that “the resulting work as a whole constitutes an original work of authorship.” Or an artist may modify material originally generated by AI technology to such a degree that the modifications meet the standard for copyright protection.” 
(p.4)
In practical terms, both EU and US approaches lead to the conclusion that merely introducing prompts into a generative tool would not be sufficient to grant the prompter copyright over the output. The result is likely the same for most types of basic “prompt engineering”. Whether or not highly sophisticated prompts might change this conclusion is a gray area and would have to be assessed on a case by case basis. What could however probably lead to the copyright protection of an AI output is subsequent curation or redaction of the output by the prompter before its publication. In other words, it may very well make sense to do some further editing of outputs before publication.
Q: Do I own the output if I prompt the model? What am I allowed to do with the output?
This is not as simple a question as you might think. The reason is that authorship and ownership are different elements and may in practice diverge. That is to say, just because you are the author of a work does not always mean you are its owner. To a large extent, authorship and ownership will depend on how applicable law governs this question and, in many cases, on contractual arrangements.
Proving or enforcing authorship or copyright ownership of a work may sometimes be difficult in practice. For this reason, in the EU, many Member States provide for rules that establish a (rebuttable) presumption of authorship or copyright ownership, in that the person indicated on or with the published work as the author is deemed to be the author, unless proven otherwise. Naturally, in the case of AI generated output, this may lead to false attributions of authorship and ownership to a natural person, e.g. the prompter that publishes the output as their own.
But it is worth considering how this could play out for AI generated tools. As a rule, assuming the output is protected by copyright and the providers’ terms of use are silent on the topic, the prompter will be both the author and owner of the output.
The question then is, for most cases that we are discussing here, what do the providers terms of use say? To answer this question, we are assuming that these terms are valid and enforceable under different national laws, meaning that they will govern the question of ownership of the outputs. Let’s look at two examples, one for a text generator (ChatGPT) and the other for an image generator (Midjourney).
ChatGPT
OpenAI’s 
Terms of Use
 (ToU; updated 14 March 2023) apply to their large language models, such as ChatGPT and GPT-3. (For additional policies see 
here
, including the “content policy” applicable to DALL-E 
here
.)
As a general rule, section 3(a) the ToU states that (1) all input (e.g. a prompt) is owned by the user and (2) subject to compliance with the ToU, all rights to the Output (e.g. the generated text) are assigned to the user. In short, if a user complies with the ToU, they will own the copyright to the output (assuming the same meets the legal standard of originality). This does not include ownership of similar or identical outputs that may be generated by another user as a result of a similar or identical prompt.
The ToU also incorporates a 
Sharing & publication policy
 (updated 14 November 2022), which includes rules on (1) Social media, livestreaming, and demonstrations, (2) Content co-authored with the OpenAI API (3) Research. We’ll focus on (1) and (2).
● Rules on (1) refer to OpenAI’s policy on permitted sharing, including compliance with their 
Usage policies
 and rules that might have interest from the copyright perspective, namely: the need to attribute the output to a user’s name or your company; and to “indicate that the content is AI-generated in a way no user could reasonably miss or misunderstand.”
● Rules on (2), Content co-authored with the OpenAI API, are even more interesting. These apply to creators that “wish to publish their first-party written content (e.g., a book, compendium of short stories) created in part with the OpenAI API”. For such users, a number of conditions are set out, in particular: the published content must be attributed to the user or his company; the role of the AI in “formulating the content is clearly disclosed”; topics of the content are in compliance with the ToU and the content policy, and are generally not offensive. Some examples (including “stock language”) are provided on how to make these disclosures in e.g. a Foreword or Introduction.
In our view, these rules do not materially change the general ownership provision mentioned above. However, they do impose some restrictions on how the content is presented to the outside world in a way that may impact presumptions of authorship or ownership. For instance, there are clear implications for a publication’s approach to bylines so as to be compliant with the OpenAI policy. Furthermore, ownership of the outputs is conditional upon compliance with the ToU and incorporated policies, meaning that breaching these obligations may have consequences for ownership purposes.
MidJourney
Midhourney’s 
Terms of Service
 (ToS) state that (1) images and other assets generated with the tool (called the “Service”), or (2) prompts a user might enter into the tool are called the “Assets”. For our purposes, we are concerned with category (1) AI outputs, not the prompts.
Section 4 of the ToS regulates “copyright and trademark”. In our view, this is drafted in a bit of a misleading way. As a general rule, it states that the user owns all outputs it creates with the tool, “to the extent possible under current law”. That is to say, the prompter is the owner of the copyright in the output. Of course, if the output does not qualify for copyright protection under applicable law, then you will not own a copyright on it.
An important clarification of scope follows. This general rule does not apply to upscaling the images of other users. Such images or outputs remain owned by the original creator of the prompt.
Then the ToS operate a bit of a magic trick, which negates the general rule. In essence, it boils down to the following: only paying users own outputs. Two aspects are key here.
● First, if the users are (1) employees or owners of a company with more than $1,000,000 USD a year in gross revenue 
and 
(2) are using the tool on behalf of the employer, then (3) the users must purchase a “Pro” membership in order to own outputs.
● Second, if a user is not a “Paid Member” it does not own the outputs they create. Instead, Midhourney grants you a 
Creative Commons Noncommercial 4.0 Attribution International License 
to use the output, also called the “Asset License”.
In short, if you are not a “Paid Member” and are making a commercial use of these outputs, you might be in breach of your agreement with Midjourney. So, as practical recommendations go, if you want to make a commercial use of outputs generated by Midjourney, then become a “Paid Member” or get a “Pro” plan.
Q: What about the copyright of the artists/illustrators/photographers whose works the AI is trained on?
This question can be looked at from two perspectives. On the one hand, we can consider whether a certain output infringes the rights of the creators of works used during the training of the model. This is mostly relevant for users of AI tools, like journalists. On the other hand, we can examine whether the TDM activities to develop a generative AI model infringe on the rights of those creators. This is mostly relevant for developers of AI tools and is at the crux of ongoing litigation mentioned above.
Let’s focus on the first aspect, which is the most relevant for our purposes here. In theory, copyright protection for the output is separate from protection for the works in which the model was trained (see above the distinction between the input vs. output stages). Generative models are able to “memorize” content they are trained on, i.e. producing identity between output and input works. Although cases of identity are theoretically possible and 
have been reported
, they are rare. Even in the Stability AI class action lawsuit, the complaint recognizes that “none of the Stable Diffusion output images provided in response to a particular Text Prompt is likely to be a close match for any specific image in the training data” (see para 93). Still, if that occurs, then there is a likelihood that the output is infringing.
While this is a statistically rare occurrence, what may occur more frequently is that there is 
similarity
 between the output and one or several of the input works. Under many national laws, an output would be infringing if it is substantially similar to a pre-existing work in the training data (on copyright’s substantial similarity test in US law, see 
here
). In any case, 
whether this similarity between input and output is sufficient to lead to infringement would have to be assessed on a case by case basis
.
Let’s assume that the output generated by an AI model is not an exact replica of any works used during the training stage. It’s important to note that copyright only protects original expressions of human authorship, not ideas, procedures, methods of operation, concepts, or styles (on copyright and styles, see 
here
). However, in practice, the line between expression and style can be blurry, especially for works of popular or iconic creators. For example, if we use a generative AI tool to produce something in the style of a famous artist, and the output is similar to an existing painting of that artist in the training dataset, it can be difficult to distinguish between expression and style. Nevertheless, the legal question is whether the output is substantially similar to the work of the artist, according to applicable law. Merely copying or mimicking a pre-existing style will not per se be sufficient to establish infringement.
A related question being discussed in the US litigation mentioned above is whether output can be considered a “
derivative work
” of the copyrighted works the model was trained on. At least as construed in the 
class action litigation
 against providers of Stable Diffusion, the argument does not appear very strong, as it is based on an incorrect representation of how this AI model works (see 
here
 for more detailed criticism and the aforementioned 
Motion to Dismiss
 the lawsuit).
On this topic, it is important to note that EU copyright law does not contain specific rules on derivative works, although some national laws may contain regimes with some shared characteristics. As such, it is unlikely that a lawsuit on these exact grounds is brought in a EU Member State. From our perspective, the most likely grounds for a lawsuit against an AI provider in the EU would be that its TDM activities for training a model are infringing upon the reproduction right of copyright holders and not covered by an existing exception.
Q: Are there legal risks to publishing these outputs, and if so, how can those risks be minimized?
As many lawyers would say, “it depends”. From the discussion above, including existing litigation, most of the risk lies with the generative tool provider. This is especially the case for their TDM activities when training the model. From the perspective of the user that prompts the tool and subsequently uses the generated output, the risk is reduced, especially for textual output.
Assuming there are no contractual restrictions to further use of the output imposed by the provider of the AI tool (but see above the analysis of ChatGPT and Midjourney’s terms of use), the main risk is that an output is identical or close to an existing work. As we mentioned, exact replication of input and output will in principle be a rare occurrence. The risk therefore relates mostly to substantially similar outputs.
To check for substantial similarity journalists might make an effort to find related images by using a reverse image search tool such as 
Google
 or 
TinEye
 on the outputs of the model. If the image search returns results that are substantially similar then an alternative image could be generated. If a case were brought this process could potentially help in establishing good faith and, at the very least limit damages (depending on the applicable law).
From a prompting point of view, it would seem that the more sophisticated and elaborate the prompt engineering, the less likely that the output would be substantially similar to a pre-existing work, and the lower the risk. Moreover, as a rule, prompting norms should be established so that the name or distinctive style of an artist is not used, unless that artist’s work is fully in the public domain. This is to explicitly avoid the chance of copying that artist’s work and reduce the potential for impacting the market for their work. One important exception would be where the prompter wishes to engage in a transformative use of that artist’s work or produce some type of political, artistic or socially relevant commentary over their work; in such a case, it is likely that the use in question is protected by existing exceptions, both in the EU and US.
In any case, good faith uses for non-commercial or journalistic purposes would generally fare better than bad faith and (exclusively) commercial uses in a potential infringement scenario. Risk could also be mitigated by having mechanisms in place to remove the allegedly infringing content upon obtaining knowledge of this potential illegality (e.g. a notice and take-down system).
Q: Considering the last question, is the legal situation any different for generative text than for images?
From our perspective, the risk seems to be lower for generative text than images. The main reason is that infringement of a pre-existing work will generally be harder to establish for textual content than for images. Moreover, the closer the text is to factual material, public domain material, or other unprotected material, the harder it is to establish infringement.
Q: Is there something news organizations can do to protect their work if they don’t want it used to train AI models? Do news organizations (or individual journalists) have any recourse to protect their material if it has already been incorporated into learned models without consent?
In the EU at least, the above mentioned “commercial” TDM exception provides a clear avenue to do this under an “opt-out” mechanism. Article 4 of the 
CDSM Directive
 sets forth an exception for reproductions and extractions of lawfully accessed works/subject matter for the purposes of TDM. This exception is subject to reservation by rights holders, including through “machine-readable means in the case of content made publicly available online”, for instance through the use of metadata and terms and conditions of a website or a service. This is usually called the “opt-out” provision and is already being used in practice by some creators, for instance through tools like those provided by 
spawning.ai
. Some commentators 
consider
 that this approach has the potential to increase the bargaining power of rights holders and lead to licensing deals with (and remuneration from) AI providers, while others are more 
critical
, arguing this will lead to market concentration and exploitation of creative workers by big companies
.
 In theory at least, if they see this approach as promising, news organizations or individual journalists could do the same. To prevent future use of content not already included, they can also deploy technical restrictions to crawling or harvesting from their services, but it would be important to avoid that such technical blocks impact other welcomed uses of their content. Eventually, standards could emerge for both opt-outs and technical restrictions.
One shortcoming of the TDM opt-out approach is that it relies significantly on the public availability of training datasets (e.g. like 
LAION
’s ) in order to effectively opt-out. You have to have some way to know that your image was actually used in training. In response to this issue and in the context of the 
proposed AI Act
, EU lawmakers are currently considering requiring providers of generative AI systems (as types of “foundation models”) that they “
make publicly available a summary disclose the use of training data protected under copyright law
.”
Q: What are the biggest open legal questions surrounding generative AI right now?
Generative AI has legal implications beyond copyright law, such as liability (listen 
here
 and 
here
) and privacy violations as seen in the 
recent restriction of ChatGPT in Italy
. The biggest copyright law question in the EU and US is probably whether using in-copyright works to train generative AI systems is copyright infringement or falls under the TDM exception (in the EU) or fair use (in the US).
In the EU, the emergence of generative AI has disrupted the legislative process for the AI Act and forced lawmakers to reconsider how they categorize and assign responsibilities to AI systems (see 
here
). Although the AI Act is not intended for copyright law, it may mandate transparency about using in-copyright training data, as noted above. This could allow creators to opt-out and establish a market for their works in training generative AI models. Whether this is feasible on a large-scale or desirable remains to be seen. In the US, TDM activities to train AI systems were probably considered transformative fair use until recently. However, as current generative AI systems directly train on artists’ works and produce outputs that compete with them in the market, it is unclear whether copying in-copyright works for training purposes qualifies as fair use.
Implications for Practice
Given the legal uncertainty which will persist while lawsuits are being litigated, practicing journalists can take steps now to try to mitigate potential ethical harms and legal risks. Even if generative AI models are ultimately permitted under the TDM exception or fair use, ethical and responsible use is nonetheless warranted.
As described above, we suggest the following as a starting point for responsible use and mitigating legal risks:
● Consider doing additional editing or curation of outputs from generative models before publication. This increases the likelihood of copyright protection by meeting the originality requirement.
● Read the terms of use of the specific models you want to use carefully to assess whether you are compliant, since this could have implications for your ownership or use of model outputs (e.g. byline policies).
● Use a reverse image tool on any outputs from generative AI image models to search the web for copyrighted images that you deem substantially similar. If something matches and it could infringe on a copyright holder’s work, then you should generate an alternative image.
● In most cases, the name or distinctive style of an artist’s work which is copyrighted should not be used to prompt image generation models. This will reduce the chance of copying that artist’s work and reduce the potential for impacting the market for their work.
● News organizations in the EU can consider whether they want to opt out of having their content used to train generative models.]]></content:encoded>
</item><item>
<title><![CDATA[Finding Story Angles using Interactive Generative AI Tools]]></title>
<link>https://generative-ai-newsroom.com/finding-story-angles-using-interactive-generative-ai-tools-b1c16e5e5f7c?source=collection_home_page----df04cefca135-----72-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Press Release]]></category><category><![CDATA[Reporting]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-04-19</pubDate>
<atom:updated>2023-04-19</atom:updated>
<content:encoded><![CDATA[Finding Story Angles using Interactive Generative AI Tools
Nick Diakopoulos
Follow
4 min read
·
Apr 19, 2023
--
Listen
Share
Most journalists will relate to the challenge of an email inbox full of press releases. Everyone wants you to know about their “news”! You definitely don’t want to just copy, paste, and publish the release, uncritically passing along the message. But if you find an interesting one, it can make it easier to work up a timely story. Press releases can contain useful information that stimulates a meaningful story, provided that they’re approached critically, thoughtfully, and creatively.
Given the ubiquity of dealing with press releases in journalism I was thrilled to collaborate with a number of researchers led by 
Savvas Petridis
 and 
Lydia Chilton
 on a 
forthcoming paper
 that describes a system called AngleKindling. The system leverages generative AI (GPT-3 in this case) to help journalists come up with useful reporting angles based on press releases.
AngleKindling helps journalists cut through the fluff of a press release, summarizing it into main points and suggesting potential controversies and negative outcomes. This helps balance out the positive orientation of press releases and draw attention to more critical perspectives. The system also suggests potential areas of investigation to further stimulate thinking. AngleKindling is an interactive tool and allows journalists to explore the suggestions and also see the parts of the press release relevant to each angle.
Here’s what it looks like, as presented in the forthcoming research paper:
The interface shows the press release on the right side and extracts the main points, potential controversies, areas of investigation (e.g. questions to consider), and negative impacts that could arise. Users can click on a suggested angle (e.g. “b1” on the figure) and it will highlight the related area of the press release. They can also cycle through up to five related areas of the release. This makes it quicker to review the angles and see how they connect to the underlying document. Each angle is also accompanied by a related New York Times article from the past decade that can offer some historical context based on prior coverage from that perspective.
To generate the angles for each press release we had to develop some prompts. Since the whole press release couldn’t fit within the length limitation of the GPT-3 model at the time, we had to split each document into as many paragraphs as would fit and then run prompts on each of those sub-sections. We first tried zero-shot prompts to directly extract angles, with prompts like “Create a list of controversies that could potentially arise from the following article section.” This led to frames that were a bit generic sounding and the phrasing was a little too confident. We wanted the angles to sound less confident and not be in a generic tone. So we switched to a few-shot prompt which indicated a few examples of potential controversies and the tone we were looking for (you can see an example in the next figure). The angles from this few-shot prompt tended to be more specific and were hedged and framed as 
possible
 controversies.
To cut through the fluff for the main points extracted we also use a few-shot prompt. We first extracted the main points which often contained a lot of positive framing and fluff and then prompted the model with another few-shot prompt to rewrite it with fewer words and remove some of the bias.
We also evaluated the AngleKindling system with a dozen journalists and compared it to the features of another creativity support tool for journalists. I’ll refer you to the 
paper
 for all the details, but the basic results were that AngleKindling was perceived to be more helpful than the other tool in terms of coming up with story ideas. On a 7-point scale the participants rated the “pursuability” of the angles provided by AngleKindling as a 6.33.
Other feedback that we received from journalists indicated that the specificity of the angles was helpful. They were specific enough to help stimulate thinking about which questions to ask, what information needed to be gathered, and perhaps even who might be good to interview. The interactive interface connecting the angles to the relevant parts of the press release was crucial for allowing for verifiability and helping to build trust in the system.
While AngleKindling was already useful for journalists there are a number of directions to take this kind of work in the future. For instance, perhaps a tool like this should let journalists write their own prompts to pull out angles. Maybe you’re looking for more specialized or different angles if your beat is education vs. city government. A tool like this could consider how to help journalists prioritize the suggested angles in terms of which might be better for shorter or more longer-term reporting initiatives. How much effort would a particular angle take to pursue? Finally, we might consider ways to help journalists understand if they’re getting nudged to cover particular perspectives by the tool. Ideally we want reporters (and their editors) to be cognizant of things like bias and how a tool like this could eventually shape coverage if users became too reliant on it.]]></content:encoded>
</item><item>
<title><![CDATA[How to use GPT-4 to summarize documents for your audience]]></title>
<link>https://generative-ai-newsroom.com/how-to-use-gpt-4-to-summarize-documents-for-your-audience-18ecfe2ad6a4?source=collection_home_page----df04cefca135-----73-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-04-11</pubDate>
<atom:updated>2023-04-11</atom:updated>
<content:encoded><![CDATA[How
 
to use GPT-4 to summarize documents for your audience
Nick Diakopoulos
Follow
6 min read
·
Apr 11, 2023
--
1
Listen
Share
Editor’s Note
: This post has been edited to include an addendum about corrections made to one of the articles referenced.
There’s a lot happening in the field of generative AI. You could easily burn out trying to stay on top of it all. That’s why I wanted to see if I could use GPT-4 to help summarize some of the latest research and provide tailored summaries for the audience here. Today I published a couple examples of this which you can read 
here
 and 
here
.
To generate these articles I used a series of prompts to OpenAI’s GPT-4 model to (1) analyze the research papers and extract particular pieces of information, and then (2) write a summary based on those bits of extracted information. By breaking this process down into two steps I was able to better control the information that would be included in the final summary articles.
To define what I wanted the model to analyze in step 1 I first thought about the audience for this blog: journalists such as reporters and editors who might want to know what a new piece of research means for their practice and whether there are any limitations that would curtail its value or utility. I then experimented with some prompts and settled on three to extract information from the document text [1,2,3]. Each of these papers is short enough that I could include the entire text into GPT-4.
I fed the output from these three prompts into a final prompt to generate the first article [4]. I tweaked the prompt for the second article to aim for a more accessible blog-like style [5]. To see how I configured all the prompts including the system prompt, the document prompts, and other model parameters see the code in 
this Colab Notebook
.
Accuracy Checking
One of the biggest concerns about generative AI summaries is the potential for fabrication of information. An article about a research paper needs to ensure that facts are accurate and consistent with respect to that paper.
For the first paper, I read it thoroughly before I tried to summarize it. This allowed me to quickly assess whether the generated summary was accurate with respect to the paper. While I didn’t see any outright fabrication, one of the generations included a sentence that didn’t make sense to me and another sentence was a bit confusing or potentially misleading. This reaffirmed that you really do need to have a human in the loop. Having read the paper, which took me about 30 minutes, it only took about 5 minutes to read and assess the final output.
For the second paper, I summarized it automatically without reading it. But this time I spent more time factchecking the output, reading each sentence and checking whether it was consistent with the underlying paper. I added one snippet of text (“e.g. radio, speech, TV, etc.”) to help address what I thought was an issue of specificity in the writing. This editing process took about 10 minutes, overall less time than the first paper since I only read small excerpts of the paper as I was editing.
To create illustrations for the final articles I followed some of the 
advice here
 and manually prompted DALL-E until it generated some images that I thought were reasonable. That took perhaps another 10 minutes for each article.
So in total, the first article took me 45 minutes, and the second one took me about 20 minutes. In both cases it took about 5 minutes for the model to extract information and then output an article.
Further Reflections
I think GPT-4 is a viable technology for accelerating the translational coverage of research for particular audiences. It drastically reduced the time and effort needed to produce a tailored summary to about 20 minutes. This could be combined with 
a prior use-case I’ve developed on story discovery
 to filter new research articles of relevance to an audience and then automatically generate blog posts summarizing those papers.
The key way that journalists can differentiate summary articles is by focusing on extracting different pieces of information in the first step. In my case I focused on key findings of interest to the target audience (reporters and editors), including benefits, limitations, and critiques. But others could configure their own questions and frames of interest to different audiences and arrive at different outputs. Basically it’s up to the journalist to define 
what matters
 and use that to drive the summary.
The other key area where journalists still need to be involved is in editing and factchecking the resulting articles, including by checking for any sentences that are too similar to sentences in the original research paper and might need to be quoted in order to avoid potential plagiarism. I have a nagging feeling that even though I configured the AI to be critical that it probably has some blind spots and it could miss something. In an ideal world, the outputs of this process would not only get edited for accuracy but serve more as a first draft for a reporter to write-through, or even just to provide an impetus to go do more reporting.
I’ll also admit that the output articles are perhaps not the most interestingly written. A good writer could make them more engaging. Perhaps you could include quotes, excerpts, or more examples (saliency), or explore how the findings might actually be used in a specific news task (concreteness).
A final limitation here is that there is often visual information in research papers that isn’t currently being considered in the process. Modern storytelling is about more than text and at least for now there’s a need for a person to help illustrate and think about whether there are data or figures needed to convey the findings.
Addendum
: After publishing the articles a reader pointed out that one of them contained a few sentences that were similar enough to the underlying research paper that you would want to include quotation marks in order to avoid any claims of plagiarism. That 
article
 has now been corrected and an editor’s note included. The other article was also checked but no issues were found. This check was done manually but using an automated script to list any sentences that were above a threshold of similarity to the original document.
—
[1] 
What research question is the paper trying to answer? Explain what the researchers did to study that question, including the specific methods used and analyses performed. Explain thoroughly and be sure to include specific details.
[2] 
What are the key findings reported in the paper that are important for journalists such as reporters and editors? How might journalists such as reporters and editors benefit from these findings? Why might there still be limits to those benefits? Explain thoroughly and be sure to include specific details.
[3] 
Critique the findings of the paper, focusing on their validity and utility for journalists such as reporters and editors. Are there reasons not to trust any of the findings? Explain thoroughly and be sure to include specific details.
[4] 
Here are some important observations about that research paper: <extracted information>. Write a 600 word article about the paper using only the paper text and the important observations about the paper above and focusing on the benefits and limitations of the findings for journalists. Reduce scientific jargon and technical terminology in the writing.
[5] 
Here are some important observations about that research paper: <extracted information>. Write a 600 word article about the paper in the style of an online blogger, using only the paper text and the important observations about the paper above and focusing on the benefits and limitations of the findings for journalists. Reduce scientific jargon and technical terminology in the writing so that it is accessible to a broad audience.]]></content:encoded>
</item><item>
<title><![CDATA[Can AI Help Journalists Evaluate News Outlet Credibility?]]></title>
<link>https://generative-ai-newsroom.com/can-ai-help-journalists-evaluate-news-outlet-credibility-1bb749c17c9c?source=collection_home_page----df04cefca135-----74-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News]]></category><category><![CDATA[Credibility]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-04-11</pubDate>
<atom:updated>2023-04-11</atom:updated>
<content:encoded><![CDATA[Can AI Help Journalists Evaluate News Outlet Credibility?
Nick Diakopoulos
Follow
4 min read
·
Apr 11, 2023
--
Listen
Share
Editor’s Note
: This post was automatically generated and manually checked for accuracy. For details on the production process see 
here
.
In today’s fast-paced digital world, the credibility of news outlets is more important than ever. Journalists, reporters, and editors need to ensure they are using reliable sources to avoid spreading misinformation. A 
recent research paper
 explored the potential of ChatGPT, a prominent large language model (LLM), to evaluate the credibility of news outlets. The findings show promise, but there are limitations and concerns that need to be considered.
ChatGPT’s Potential in Assessing News Outlet Credibility
The study found that ChatGPT can rate the credibility of various news sources, with its ratings showing a moderate correlation with those of human experts. This suggests that LLMs like ChatGPT could be a valuable tool for journalists to assess the reliability of news sources.
One of the key benefits of using ChatGPT is its multilingual capabilities. The model can rate news sources in different languages, making it a helpful resource for journalists working with international news or in multilingual environments. Additionally, ChatGPT can identify satirical websites, helping journalists distinguish between genuine news and satirical content.
Another advantage of using LLMs like ChatGPT is cost-effectiveness. The study found that evaluating over 7,500 news domains took approximately two hours and cost around $3. This makes it an affordable and accessible reference for journalists, compared to more expensive services like NewsGuard or those with a narrower focus like Media Bias/Fact Check (MBFC).
Limitations and Concerns
Despite the potential benefits, there are several limitations and concerns that journalists should be aware of when using ChatGPT for credibility assessments.
Accuracy
: The accuracy of ChatGPT’s ratings is not perfect, which means journalists should not solely rely on them for assessing source credibility. There might still be discrepancies between the ratings provided by ChatGPT and those of human experts.
Model limitations
: The findings of this study are specific to ChatGPT, and the results may not generalize to all language models. Journalists should be aware of the specific capabilities and limitations of the LLM they use.
Rating variations
: Depending on the prompts and instructions given to ChatGPT, the results may vary. Journalists should be cautious about the phrasing of their queries and the interpretation of the results.
Potential biases
: LLMs like ChatGPT may have biases encoded within their architecture, which could potentially influence the credibility ratings. Journalists should be aware of these biases and take them into account when using LLMs for source evaluation.
Model generalization
: The study focused exclusively on ChatGPT, and the findings may not generalize to all other large language models. As newer models emerge, it would be prudent to test their performance in similar tasks before relying on them for credibility assessments.
Prompting
: The paper acknowledges that due to the flexibility of the prompts, there could be different ways to rate source credibility. Alternative approaches, such as binary classification or pairwise comparisons, might result in different outcomes. The study’s results are highly dependent on the specific prompt used, and other methods might lead to varying results.
Language model limitations
: ChatGPT, like other large language models, has certain limitations, such as a lack of up-to-date data, which could affect its accuracy. Additionally, the paper notes that ChatGPT’s performance on Italian-language sources is lower than its performance on other languages, indicating potential limitations in handling specific languages.
Satirical websites
: The paper found that ChatGPT identified the satirical nature of 77.4% of the satirical websites in the MBFC list. However, the remaining 22.6% were misclassified or yielded errors. This highlights possible limitations in ChatGPT’s ability to accurately assess the credibility of satirical news sources.
Conclusion
While LLMs like ChatGPT can provide valuable information to journalists regarding news source credibility, it is crucial to consider their limitations and potential biases. Journalists should use these tools in conjunction with their own judgment and other sources of information to make informed decisions about the credibility of news outlets.
The paper presents interesting findings on the ability of large language models, specifically ChatGPT, to evaluate the credibility of news outlets. With the potential to provide cost-effective and multilingual ratings, ChatGPT could be an asset for journalists, editors, and the general public. However, there are several limitations and concerns that need to be considered before fully trusting the findings. Journalists, reporters, and editors should treat these findings with caution, and consider corroborating the provided ratings with other sources or expert opinions before relying on them to assess the credibility of news outlets.]]></content:encoded>
</item><item>
<title><![CDATA[Can ChatGPT Help Journalists Fact-Check Faster?]]></title>
<link>https://generative-ai-newsroom.com/can-chatgpt-help-journalists-fact-check-faster-351e64a2ef32?source=collection_home_page----df04cefca135-----75-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[News]]></category><category><![CDATA[Factchecking]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-04-11</pubDate>
<atom:updated>2023-04-11</atom:updated>
<content:encoded><![CDATA[Can ChatGPT Help Journalists Fact-Check Faster?
Nick Diakopoulos
Follow
4 min read
·
Apr 11, 2023
--
1
Listen
Share
Editor’s Note
: This post was automatically generated and manually checked for accuracy. For details on the production process see 
here
.
Editors’s Correction
: An earlier version of this post included 3 sentences that were similar enough to passages in the original paper that they warranted quotations. Those quotations are now reflected in the article.
In the age of information overload, fact-checking has become a crucial task for journalists to ensure the accuracy and reliability of the news they report. With the rapid spread of misinformation, it’s essential to find ways to speed up the fact-checking process without compromising quality. Enter ChatGPT, a large language model (LLM) developed by OpenAI, which has shown potential in enhancing the efficiency and expediency of fact-checking. But is it the silver bullet we’ve been waiting for? Let’s dive into the pros and cons of using ChatGPT for fact-checking, based on 
a recent research paper
.
The Good: ChatGPT’s Potential in Fact-Checking
The research paper aimed to answer whether ChatGPT could help enhance the efficiency and expediency of fact-checking processes. The researchers conducted a systematic analysis using a dataset of fact-checked statements from PolitiFact, a popular fact-checking website. They submitted these statements to ChatGPT as a zero-shot classification task and compared its output with PolitiFact’s categorizations.
The results were promising: ChatGPT accurately categorized statements in 72% of cases, with a higher accuracy in identifying true claims (80%) than false claims (67%). The performance also varied over time and across different sources. As the authors write, “These findings demonstrate the potential of ChatGPT to help label misinformation.”
For journalists, this means ChatGPT could:
Speed up fact-checking
: ChatGPT can quickly fact-check statements and identify misinformation, aiding in content moderation and improving the overall quality of published content.
Complement human fact-checkers
: ChatGPT can function as a supportive tool to augment the work of human fact-checkers, enhancing the efficiency and expediency of the fact-checking process.
Focus on true information
: As ChatGPT is more successful in accurately identifying true claims, it could help journalists highlight accurate information and increase its visibility and impact.
The Not-So-Good: Limitations of ChatGPT
Despite its potential, ChatGPT is not without its limitations. Here are some concerns journalists should be aware of:
Imperfect accuracy
: ChatGPT’s overall performance is not perfect, with an accuracy rate of 72%. Journalists must be cautious when relying on such technology, as it may still produce a significant number of false positives and false negatives.
Varying performance across sources
: ChatGPT’s performance varies across different sources of information (e.g. radio, speech, TV, etc.), which means it might not be equally effective in all situations.
Complementary, not a replacement
: ChatGPT should not be seen as a replacement for human fact-checkers. It lacks “the nuanced understanding and critical thinking skills that are essential for effective fact-checking.” Human experts “bring a depth of experience, context, and judgment that are unlikely to be replicated by any machine.”
Additional Concerns for Journalists
Apart from the limitations mentioned above, there are other concerns that journalists should consider before fully adopting ChatGPT for fact-checking:
Limited scope
: The research is based on a specific dataset from PolitiFact, which may not be representative of all misinformation types or contexts.
Potential biases
: ChatGPT is trained on a vast corpus of text data, which may contain biases present in the training data. Depending on the nature of these biases, the model’s fact-checking performance may be influenced, leading to skewed results that may not be entirely trustworthy.
Generalizability
: The study’s findings may not generalize to other LLMs, languages, or countries. Journalists should be cautious when extrapolating these results to other contexts or LLMs.
The Verdict: A Helpful Tool, But Not a Replacement
In conclusion, while ChatGPT shows potential in enhancing the efficiency and expediency of fact-checking processes, it should be viewed as a complementary tool to support human experts, rather than a substitute for their work. Journalists can leverage ChatGPT to aid in their fact-checking efforts, but they must remain aware of its limitations and continue to rely on their expertise and judgment.
As technology continues to advance, tools like ChatGPT may become even more effective in assisting journalists in their fight against misinformation. However, the importance of human expertise and critical thinking in the fact-checking process cannot be overstated. For now, ChatGPT can be a valuable ally, but it’s essential to remember that it’s not a replacement for the irreplaceable work of human fact-checkers.]]></content:encoded>
</item><item>
<title><![CDATA[News Illustration with Generative AI]]></title>
<link>https://generative-ai-newsroom.com/news-illustration-with-generative-ai-4955a71cbd83?source=collection_home_page----df04cefca135-----76-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Illustration]]></category><category><![CDATA[Generative Ai]]></category><category><![CDATA[Midjourney]]></category><category><![CDATA[News]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Lydia B. Chilton]]></dc:creator>
<pubDate>2023-03-27</pubDate>
<atom:updated>2023-03-27</atom:updated>
<content:encoded><![CDATA[News Illustration with Generative AI
Lydia B. Chilton
Follow
7 min read
·
Mar 27, 2023
--
Listen
Share
Using AI to generate images is a potentially powerful way to illustrate an article and help draw in readers. If you don’t have the time or money for an artist, it can be better than stock photography. You wouldn’t use it when you have photos of real events to portray like a political event, a war, or a sports victory. But it is handy for illustrating opinion pieces, science/tech/finance news, and second day pieces reflecting on an event.
The goal is to generate an image that’s both 
relevant
 to the article and 
eye catching
. Here are a couple examples made with OpenAI’s generative image AI, DALL-E, using headlines I grabbed from the New York Times on 
libraries being a haven
 and 
social media changing brains
:
Right now, there are three main text-to-image generation systems that are all easy to use: 
DALL-E
, 
Midjourney
, and 
Stable Diffusion
. They currently all have a free trial tier. Midjourney is used through Discord, which you can learn more about in this 
introduction to using Midjourney on Discord
.
They all have strengths and weakness. DALL-E and Midjourney are the most reliable, but both will require trying multiple prompts to get a relevant, eye-catching image.
First, try prompting with the headline
Sometimes you can get lucky by just using the headline as the prompt to generate the image. Sometimes it does poorly. One system may do better than another.
Midjourney images are known for being eye-catching
. It has its own baked-in style: vivid colors, detailed, and uses dramatic lighting. And sometimes it captures the meaning from just the headline.
But it can often fail to capture the meaning, and generates something generic, or with odd AI-hallucinated “text.”
DALL-E is more flexible in style than Midjourney
. But it often does poorly with just a headline:
DALL-E does much better when you add a style to the prompt, such as ‘digital painting’.
Correcting Small Errors
Both systems have easy ways to correct small errors.
In DALL-E you can you 
can erase parts of an image
, and it almost always paints in the details better. Here I used it to “erase” undesirable text and regenerate the parts of the image where I erased.
In Midjourney, you can get variations to improve subtle problems like the asymmetrical eyes of the teen, and vague appearance of the social media tiles.
But both systems will sometimes struggle to capture the meaning or the tone of the article just from the headline.
Next I’ll walk through some other prompting strategies to try. I developed them mostly for DALL-E. I find DALL-E more flexible, so when Midjourney doesn’t work magic, DALL-E is my go-to.
Specify the subject(s)
A reliable way to get a relevant image is to specify the subject or subjects yourself. This could be one subject like “Library” or multiple subjects with a comma in between: “phone home screen, brain.” For DALL-E, add “digital painting” to give the image a coherent aesthetic.
Sometimes the subject is tricky to capture and these systems either get it or they don’t. So for any particular subject, give it a try and see how it does.
Overall it tends to do well with single concrete nouns like “brain”, “phone”, “book”, “man”, “woman”, “dog”, “cat”, “forest”, and “ocean”. It also does well for many scenes or environments: “library”, “people studying in a library”, “messy bathroom”, “Harlem”, “Wall Street”, “oil fields”, “chemistry lab”, “locker room”, and so on.
For abstract concepts like “love” or “hate”, it can be very generic, and symbolize it with a heart or two people hugging. You may want to think of a more concrete symbol of the subject: candlelight dinner, wedding bands, roses, holding hands, etc. 
This is where your creativity is important: translating the abstract concept into a more concrete set of objects and relationships
.
It also struggles with subjects interacting in specific ways. See below for more failure cases.
But subject alone is rarely enough: it’s not necessarily eye-catching, and it can fail to capture the meaning or tone of the article.
Specify the Style(s)
Once you have a decent subject, try specifying an 
artistic style
. “Digital painting” is a nice generic style, but you can try anything. Here are some diverse styles we like:
Styles can add to the meaning or tone of the image. Styles can be related to the subject, such as “glitch art” or “pixel art” related to technology, which can help reinforce the subject.
Styles can also be related to the tone. Watercolors, expressionism, and Monet painting evoke a calm, idyllic haven.
You’ll have to experiment with styles to find the right mood for your topic, audience, and publication.
There’s one more important strategy to make an eye-catching image…
Specify the composition
DALL-E images can be a bit basic — by default, they’re all static images, with normal daytime lighting, from an expected angle. You can make images more interesting and eye-catching by specifying less expected elements of composition such such as lighting, angles, colors, textures, or motion.
Here’s a basic generation for wine, and one with three compositional elements added to make it more interesting (wide angle shot, from below, dynamic).
There’s a lot of compositional tricks to try — and it helps to use multiple together. Here are some more examples for inspiration:
For color try
: “warm colors”, “cool colors”, “saturated colors”, “desaturated colors”, “vibrant colors”, “muted colors”, “neon colors”, “pastel colors”.
For lighting try: “
dramatic lighting”, “sunny”, “cloudy”, “candlelight”, “moonlight”, “spotlight”, “backlight”, “golden hour”.
“God beams” is a particular favorite to introduce some dramatic lighting.
Angle can really change the composition drastically. Subtle things like “wide angle” just get more of the environment in the shot. More dramatic angles like top down view or fisheye will change the whole perspective.
For angle, try: “
low angle”, “high angle”, “close-up shot”, “wide angle”, “top down view”, “fisheye view”.
By default, most subjects are fairly static in a DALL-E generation, but motion or dynamic words can add a sense of movement and energy.
For motion try: “
dynamic”, “motion shot”, “flying”, “dripping”, “tornado”, “explosion”, “twisting”, “bouncing”, “spinning”, “gliding”.
What doesn’t work
Although these systems can be very impressive, they also fail at many things. In general,
 if you have a specific image in mind, AI is rarely good at matching it.
 The exact image you have in mind might not be within DALL-E’s vocabulary.
For example, “brain made out of paper mache social media posts, digital painting.” I can imagine it, but DALL-E doesn’t seem able to generate it. It just glues envelopes to a brain. There are no social media posts and no paper mache.
Generative AI is much better if you’re open to options and willing to try 20–30 generations. You’re the editor.
Even so, two known shortcomings are:
Its spatial abilities is not great
. For example, “Person facing to their left” gives terrible generations.
It struggles to have subjects interacting in a complex way
. For example, it generations are pretty underwhelming for “a brain rewiring when connected to facebook,” “books fighting,” or “balloon popping.”
In summary, try it!
Take the headline and give it to DALL-E and Midjourney. See what it gives you. You might get lucky!
If not, spend 10 minutes trying different combinations of 
Subject(s) + Composition(s) + Style(s).
Here’s a reminder of options to try:
Concrete nouns:
 brain, phone, book, man, woman, dog, cat, forest, ocean.
Environments:
 library, people studying in a library, messy bathroom, Harlem, Wall Street, oil fields, chemistry lab, locker room.
Color
: warm colors, cool colors, saturated colors, desaturated colors, vibrant colors, muted colors, neon colors, pastel colors.
Lighting
: dramatic lighting, sunny, cloudy, candlelight, moonlight, spotlight, backlight, golden hour.
Angle: 
low angle, high angle, close-up shot, wide angle, top down view, fisheye view.
Motion
: dynamic, motion shot, flying, dripping, pouring, tornado, explosion, twisting, bouncing, spinning, gliding.
Styles
:]]></content:encoded>
</item><item>
<title><![CDATA[Can GPT-4 Help Cover Events?]]></title>
<link>https://generative-ai-newsroom.com/can-gpt-4-help-cover-events-564c1e87e987?source=collection_home_page----df04cefca135-----77-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[Law]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-03-24</pubDate>
<atom:updated>2023-03-24</atom:updated>
<content:encoded><![CDATA[Can GPT-4 Help Cover Events?
Nick Diakopoulos
Follow
4 min read
·
Mar 24, 2023
--
Listen
Share
Earlier this week the AI, Media, and Democracy Lab at the University of Amsterdam (where I’m on sabbatical), hosted a panel discussion on the 
Legal Aspects of Generative AI
. This is an important topic since legal questions are often top-of-mind for journalists thinking about using generative AI technologies in production.
As I was thinking about use-cases for generative AI it dawned on me that events like this happen all the time, not only as panels put on by universities and think tanks, but also in the form of community meetings, hearings, or speeches by public officials. And so I saw this panel as an opportunity to test whether generative AI could help cover these kinds of events. 
Could GPT-4 help summarize key takeaways from a lengthy event transcript?
From an ethics standpoint I first spoke to the event organizers to get permission to run this experiment. They then obtained consent from the panelists in order to process the audio and the transcript from the beginning of the event when each speaker made a 3–5 minute presentation. We decided to exclude the latter parts of the event that might include audience questions as we couldn’t obtain their consent. One panelist expressed a data privacy concern about using OpenAI’s transcription API since it would require uploading the audio recording to OpenAI servers. Thankfully the underlying 
Whisper model is available as an open source tool
 that I could run locally on my laptop to create a transcript.
Using the GPT-4 API I fed in the transcript and tried three prompts, a general summarization prompt: (P1) “Based on the following panel conversation, provide a list of the most important key points” and two aspect-oriented prompts focused on stakeholders of this blog audience: (P2) “Based on the following panel conversation, provide a list of the most relevant and important key points 
for practicing journalists such as reporters and editors
.” and (P3) “Based on the following panel conversation, provide a list of the most relevant and important key points 
for news organizations
.” To prime the model I used the system prompt: “You are an experienced journalist summarizing an event.”
I reviewed the summaries for accuracy and also asked the panel moderators Laurens Naudts and Stan Piasecki to take a look to see if there were any glaring issues. It’s clear that the summaries missed some nuanced points that the panelists made, while still being able to pull out some top-level key ideas. There were extensive overlaps in the outputs of the three prompts, with only a few distinct points raised with respect to different stakeholders. Sometimes the model made somewhat implicit ideas more explicit, in one case in terms of how responsibility was being assigned for data privacy. It seems prudent that if you are using GPT-4 for event summarization you still need a person to read and edit the summary before publication.
In the future, I’d like to experiment with more targeted summary prompts to try to pull out more of the nuances from each individual speaker’s presentation, or to make the writing more interesting by integrating quotes from the speakers to illustrate key points. Perhaps prompts could be written to 
identify newsworthy segments of the event
, like controversies, disagreements, or other surprising occurrences. If some excerpts of the event were particularly newsworthy they could then be fed into a second prompt for summarization.
Not seeing any major discrepancies, I edited together the final summary by combining parts of each of the three summaries:
In summary, the panel discussion on Generative AI raised several key points and concerns regarding the technology’s impact on various sectors, including media, journalism, and democracy. The main points discussed were:
1. The current regulatory approach in the EU towards generative AI systems, with concerns about overregulation and the need for a more focused approach.
2. The potential barriers to entry and expansion in the market for generative AI, particularly for smaller players, and the role of the Digital Markets Act in fostering fairness and competitiveness.
3. Intellectual property law concerns, particularly the relationship between human creators and generative AI systems, and the need for changes in rules on evidence for infringement.
4. The AI Act’s focus on high-risk applications and the need for a more cooperative approach to responsibility between developers and users of generative AI systems, as well as the need for transparency and monitoring of systemic risks.
5. Data protection and privacy concerns, particularly regarding the ingestion of personal data and the potential for discrimination and bias in generative AI systems.
6. Discrimination and inequality can be present in both the output and generation of generative AI systems, with concerns about biases, harmful stereotypes, and the exploitation of labor in the development process.
7. The role of private ordering and contractual terms in regulating generative AI systems should be considered, with a focus on fair dealing and good commercial practices.
Overall, the panel discussion highlighted the complex legal, ethical, and social questions surrounding generative AI systems and their potential impact on the media and journalism industries. As the technology continues to evolve, it will be crucial for journalists and other stakeholders to stay informed and engaged in the ongoing debates and discussions.]]></content:encoded>
</item><item>
<title><![CDATA[How Can Generative AI Help Journalists?]]></title>
<link>https://generative-ai-newsroom.com/how-can-generative-ai-help-journalists-5819c9a2e464?source=collection_home_page----df04cefca135-----78-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-03-17</pubDate>
<atom:updated>2023-03-17</atom:updated>
<content:encoded><![CDATA[How Can Generative AI Help Journalists?
Nick Diakopoulos
Follow
7 min read
·
Mar 17, 2023
--
Listen
Share
Sarah Gotfredsen
 from the Tow Center for Digital Journalism recently interviewed me for the Tow Center newsletter, which I’m re-posting below with permission. We talked about a range of topics relevant to the use of generative AI in news production, from help with programming to how to think about ethics and errors.
—
SG: For journalists and students who are novices to programming, how useful do you think ChatGPT can be in helping with computational tasks?
ND: 
As a user myself, I’ve noticed that ChatGPT can certainly be useful for solving certain kinds of programming challenges. But I’m also aware that you need a fairly high level of competence in programming already to make sense of it and write the right queries, and then be able to synthesize the responses into an actual solution. It could potentially be useful for intermediate coders as long as you know the basics, how to evaluate the responses, and how to put things together. But if you don’t know how to read code, it’s going to give you a response, and you’re not really going to know if it’s doing what you wanted.
There’s a reason why we have programming languages. It’s because you need to precisely state how a problem needs to be solved in code. Whereas when you say it in natural language, there’s a lot of ambiguity. So, obviously, ChatGPT is good at trying to guess how to disambiguate what your question is and give you the code that you want, but it might not always get it right.
SG: I’m wondering if journalism students will lose some fundamental knowledge if they use ChatGPT for assignments. When it comes to learning how to program, are students better off learning how to write code from scratch rather than rely on ChatGPT?
ND: 
One lens that I look at this problem through is substitution versus complementarity of AI. People get afraid when you start talking about AI substituting someone’s labor. But in reality, most of what we see is AI complementing expert labor. So you have someone who already is an expert and then AI gets kind of married into that person and augments them so that they’re smarter and more efficient. I think ChatGPT is a great complement for human coders who know something about what they’re doing with code, and it can really accelerate your ability.
SG: You started a project called 
AI in the Newsroom
 where journalists can submit case studies of how they’ve used ChatGPT within the newsroom. How is that project going?
ND:
 People have been submitting, I’ve had contact with more than a dozen people with ideas that are at various levels of maturity. From different kinds of organizations such as local news media, national, international, and regional publications, and startups. There’s such a range of people who are interested in exploring the technology and seeing how far they can take it with their particular use case. I have contact with some legal scholars here at the University of Amsterdam Institute for Information Law where I’m on sabbatical. They’re looking at issues of copyright and terms of use, which I know are quite relevant and important for practitioners to be aware of.
I’m also exploring different use cases myself with the technology. I’ve been writing a 
blog
 about it and to put the pilot projects out there to help folks in the community and to understand what the capabilities and limitations are. So, overall, I’m pretty pleased with the project. I think it’s progressing well. Hopefully, we’ll see some of these projects mature over the next month and start publishing them.
SG: Now that you’ve been looking at what journalists are submitting, do you have a better intuition about what things ChatGPT could help assist in the newsroom?
ND: 
There’s just so many different use cases that people are exploring. I don’t even know if there’s gonna be one thing that it’s really good at. People are exploring rewriting content, summarizing and personalization, news discovery, translation, and engaged journalism. To me, part of the appeal of the project is exploring that range. Hopefully, in a couple of months, these projects can start to mature and get more feedback. I’m really pushing people to evaluate their use case. Like, how do you know that it’s working at a level of accuracy and reliability where you feel comfortable rolling it out as part of your workflow?
SG: A main concern among computational journalists is that ChatGPT will sometimes ‘hallucinate’ data. For instance, maybe you use it to extract data from a PDF and everything works fine on the first page. But when you do it with 2,000 PDFs, suddenly errors are scattered throughout. How do you navigate that risk?
ND: 
Accuracy is a core value of journalism. With AI systems and machine learning systems, there’s a statistical element of uncertainty which means it’s basically impossible to guarantee 100 percent accuracy. So you want to get your system to be as accurate as possible. But at the end of the day, even though that is a core journalistic value and something to strive for, whether or not something needs to be 100 percent accurate depends on the kinds of claims that you want to make using the information that’s generated from the AI system.
So if you want a system that’s going to identify people who are committing fraud, based on analyzing a bunch of PDF documents and you plan to publicly indict those individuals, based on your analysis of those documents, you better be damn sure that that’s accurate. From years of talking to journalists about stuff like this they’re probably not going to rely only on a machine learning tool to come up with that evidence. They might use that as a starting point. But then they’ll triangulate that with other sources of evidence to raise their level of certainty.
There might be other use cases, though, where it doesn’t really matter if there’s a 2 or 5 percent error rate, because maybe you’re looking at a big trend. Maybe the trend is so big that a 5 percent error rate doesn’t hide it even with a little bit of error around it. So it’s important to think about the use case and how much error it can tolerate. Then you can figure out, well, how much error does this generative AI tool produce? Does it actually meet my needs in terms of the kinds of evidence I want to produce for the kinds of claims I want to make?
SG: Do you imagine some kind of AI class or tutorial for journalists in the future on how to use AI responsibly?
ND: 
I’d like to avoid a future where people feel like they can be fully reliant on automation. There may be some hard and fast rules about situations where you need to go through and manually check the output and situations where you don’t need to check it. But I’d like to think that a lot is in between those two extremes. The Society for Professional Journalists puts out a book called 
Media Ethics
 which is basically all of their case studies and reflections around different types of journalism ethics. It could be interesting to think about it this way, maybe that book needs a chapter on AI to start parsing out in what situations there’s more problematic things that can happen and in what situations there are fewer.
Maybe it’s not all that different from how it’s done now where we have these core journalism constructs like accuracy or the Do No Harm principle. When you’re publishing information your goal is to balance the public interest value of the information against the potential harm it could cause to someone innocent. So you have to put those two things in balance. When you think about errors from AI or generative AI summarizing something, applying that kind of rubric might make sense. Like, what is the potential harm that could come from this error? Who might be hurt from that information? What damage might that information cost?
SG: Yeah, and journalists make errors, too, when dealing with data.
ND:
 There’s a difference, though, and it comes back to the accountability question. When a human being makes a mistake, you have a very clear line of accountability. Someone can explain their process and realize why they missed this thing or made an error. Now, that’s not to say that AI shouldn’t be accountable. It’s just that to trace human accountability through the AI system is much more complex.
If a generative AI system makes an error in a summary, you could blame Open AI if they made the AI system. Although when you use their system you also agree to their terms of use, and assume responsibility for the output accuracy. So Open AI says it’s your responsibility as the user and they assign responsibility to you. They don’t want to be accountable for the mistake. And contractually, they’ve obligated you to be responsible for it. So now it’s your problem. Are you willing to take responsibility and be accountable as let’s say, the journalist or the news organization that uses that tool?
SG: How would a journalist keep track of using AI in case they had to track back to an error?
ND: 
That’s a great question. Keeping track of prompts is one way to think about it. So that as the user of the technology there’s a notion of what was my role in using the technology. What were the parameters that I used to prompt the technology? That’s at least a starting point. So if I did something irresponsible in my prompt, there would be an example of negligence. For instance, if I prompt something to summarize a document, but I set the temperature at 0.9t and a high temperature means that there’s a lot more randomness in the output.
You should know that if you’re going to use these models. You should know that if you set the temperature high, it’s going to introduce a lot more noise in the output. So maybe you do bear some responsibility if there’s an error in that output. Maybe you should have set the temperature to zero or much lower in order to reduce that potential for randomness in the output. I do think, as a user, you should be responsible in how you’re prompting, and what parameters you are choosing and be prepared to explain how you use the technology.]]></content:encoded>
</item><item>
<title><![CDATA[Finding Newsworthy Documents using Generative AI]]></title>
<link>https://generative-ai-newsroom.com/finding-newsworthy-documents-using-generative-ai-a43a41a90e30?source=collection_home_page----df04cefca135-----79-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[ChatGPT]]></category><category><![CDATA[News]]></category><category><![CDATA[Generative Ai Use Cases]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-03-06</pubDate>
<atom:updated>2023-03-06</atom:updated>
<content:encoded><![CDATA[Finding Newsworthy Documents using Generative AI
Nick Diakopoulos
Follow
7 min read
·
Mar 6, 2023
--
1
Listen
Share
What if AI could scan the world for events and information and send an alert when something looked interesting? This could accelerate reporting and guide journalists’ limited attention in a world of information overload. Sure, journalists would still need to do plenty of reporting to see if there was real news there, but the AI could provide the first inkling of a potential story.
In this post, I’ll describe my early experiences in prompting OpenAI’s models to help with this task, specifically in the domain of science and technology reporting. In particular I’ll focus on 
how to use AI to rate the newsworthiness of scientific abstracts
 published on the 
arXiv pre-print server
. Tens of thousands of papers are uploaded to arXiv every month. Ranking and filtering those papers could help journalists more quickly find interesting new research to report on.
To evaluate performance of GPT’s ratings against an expert baseline I’ll use some data from our 
prior work building machine learned models for news discovery
. Specifically, we paid two professional sci/tech journalists to rate 55 arXiv abstracts on a newsworthiness scale from 1 to 5. It’s worth mentioning that there’s a lot of variance in how journalists rate newsworthiness, and even amongst these experts there was only a relatively weak correlation (pearson r = 0.33), but this at least offers a comparison point. If GPT is able to produce reasonable assessments of newsworthiness, its ratings should correlate with the experts.
One of the key challenges with using generative AI effectively is coming up with the right prompt. The prompt is the main way of controlling the AI, providing your intent, and detailing context for the task. I was inspired by some 
previous research
 to try 
three
 distinct prompting strategies:
Direct Prompt.
 This prompt frames the task similarly to how it was framed for the expert raters [1].
Proxy Prompt
. This prompt attempts to implicitly steer the AI by describing an archetypical journalist whose judgment it is meant to mimic [2].
Explicit News Values Prompt
. This prompt makes various criteria for evaluating newsworthiness explicit, such as societal relevance and number of people potentially impacted [3].
I tested GPT-3.5 (“davinci-003” model) and ChatGPT (“gpt-3.5-turbo”) models from OpenAI with each of the three prompts. For each combination of prompt and model I rated the 55 abstracts from the ground truth dataset and computed the pearson correlation coefficient between those ratings and the average of the expert ratings. Here are the results:
All of these correlations were statistically significant (p < 0.05), but the highest correlation to the experts was using the Explicit News Values prompt with the GPT-3.5 model. The ChatGPT model did a bit better than GPT-3.5 for the Direct and Proxy prompts and in all cases it had a lower mean square error of predicted ratings. The average score that ChatGPT produced (2.77) was closer to that of the experts (2.73) than the average score from GPT-3.5 (3.75) suggesting that ChatGPT is a bit better calibrated to the experts.
Feasibility
This actually works! The correlations to expert ratings even exceed what we found in our prior work for expert correlations to crowd worker ratings. But I’m also not quite ready to say we should roll this out just yet either.
For one thing, we need to evaluate the 
biases
 in the model for this task. There’s a lot of variance in newsworthiness decisions even amongst experts — context is crucial. It will be important to understand where exactly the ratings are biased to know what the model might be missing. If we can better characterize those errors, then experts can better interpret the scores. The temporal biases of the model will also limit its ability to measure societal relevance for recent events and discoveries. For instance, if the model didn’t know about COVID because that wasn’t in the training data, then it might not pick that up as newsworthy in an abstract. As a result, we’ll always want journalists looking at multiple channels and tools for news discovery.
Another thing is that I’d want to test on a lot more (and more diverse) data than what we currently have. Ideally we would have ground truth expert ratings for development, and then some more separate data for testing. This would ensure that the prompts are generalizable and work for various inputs, such as for abstracts across many different scientific fields.
Finally, using these models to scan scientific literature for newsworthy abstracts would also be financially feasible for news organizations. Using OpenAI’s current cost of $0.002 per 1000 tokens, and estimates for the average abstract length, prompt length, and number of abstracts, it would only cost about $10 per month to scan all of arXiv.
Reflections on Prompting
It took a lot of experimentation with the prompts to get them to work. I actually tried even more prompting strategies than what I reported here, such as a few-shot prompt that included example ratings, other ways of making values explicit in prompts, and even seeing if a 
prompt optimization tool
 would improve on what I had written (it did not). Probably the most frustrating aspect is that it feels like you can always try another tweak, so it’s hard to know when you are really done. There could be other prompts that are even more effective than the ones I came up with. I didn’t track my time precisely but it was easily more than 10 hours. All of this iteration also cost about $80 worth of OpenAI credit, since each iteration required re-running the model on 55 different abstracts.
You can ask the model to also provide an explanation for its ratings. I would recommend this as a general strategy in developing prompts for rating documents. Examining explanations was valuable for debugging my prompts because I could see what criteria it was applying and how it was “understanding” newsworthiness. For the Explicit Values prompt I used the explanation to learn if you frame the prompt as a list of values (using “and” between the values) it will understand the values as a complete block instead of breaking them out separately. This led me to update the prompt to not use “and” as a conjunction between criteria: in the final prompt I list those as separate sentences. In comparing the explanations from the model to the explanations of the expert raters I also realized the model didn’t appear to be weighting “understandability” as highly as the experts. This led me to upweight it in the prompt, and this improved the correlation with the expert raters.
In Sum
I’m positively encouraged about the use of generative AI to evaluate the newsworthiness of journalistic documents. We found a moderate correlation to expert ratings of newsworthiness for scientific abstracts. It was relatively cheap to develop and it would be cheap to deploy even at a reasonably large scale. At the same time, there’s still more evaluation to do to understand how the ratings might be biased, and what the models might be missing. More expertly annotated data is necessary to fully evaluate those questions. Coming up with prompts is a pain, and you can iterate seemingly endlessly, but using the explanation capabilities of these models can also help point the way for how to improve prompts. Aligning prompts with known news values was also a powerful way to communicate what the model should pay attention to when rating. Overall, this task is worth pursuing, certainly for scientific abstracts, but possibly also for other documents that journalists might routinely find newsworthy.
Footnotes
[1] “Rate the newsworthiness of the following research abstract by evaluating whether it could be interesting for journalists to report on and potentially develop into a news article for a science-focused or technology-focused publication. Provide a numeric rating on a scale from 1 to 5 where 1 is a low newsworthiness rating and 5 is a high newsworthiness rating: <abstract>”
[2] “You are a prize-winning professional science and technology journalist. You have years of experience as a reporter and editor, extensive editorial knowledge, and excellent judgment for what makes a compelling news story. Rate the newsworthiness of the following research abstract by providing a numeric rating on a scale from 1 to 5 where 1 is a low newsworthiness rating and 5 is a high newsworthiness rating: <abstract>”
[3] “A research abstract has high newsworthiness if it is relevant to contemporary issues in society. A research abstract has high newsworthiness if it potentially impacts many people in society in positive or negative ways. A research abstract has high newsworthiness if it has potential for controversy. A research abstract has high newsworthiness if it can be easily understood by a general audience, and this counts two times as much as other newsworthiness criteria. Rate the newsworthiness of the following research abstract by providing a numeric rating on a scale from 1 to 5 where 1 is a low newsworthiness rating and 5 is a high newsworthiness rating: <abstract>”]]></content:encoded>
</item><item>
<title><![CDATA[The Generative AI in the Newsroom Project]]></title>
<link>https://generative-ai-newsroom.com/the-generative-ai-in-the-newsroom-project-2b6682baf2e8?source=collection_home_page----df04cefca135-----80-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[AI]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-02-28</pubDate>
<atom:updated>2023-02-28</atom:updated>
<content:encoded><![CDATA[The Generative AI in the Newsroom Project
Nick Diakopoulos
Follow
1 min read
·
Feb 28, 2023
--
Listen
Share
Welcome to the 
Generative AI in the Newsroom Project
. Generative AI refers to technology that can create new content (text, images, audio, video, etc) based on the content it was trained on. These powerful models can be controlled with natural language prompts by anyone with ordinary communication skills.
There’s a lot of excitement about what the technology 
could offer
 in terms of 
helping with news production
. But there are also a host of issues with using generative AI in news contexts. These include everything from the 
accuracy of information generated and attribution of facts to sources
, to concerns around confidentiality and privacy of data inputs, potential plagiarism and copyright violations of outputs, social and representational biases, and anthropomorphism of generative AI bots. There are also concerns around the cost and sustainability of the technology.
The goal of the project is to 
collectively develop the capacity for responsible use of generative AI in news production
. Follow along as we curate a repository of case studies where news professionals have implemented and evaluated the technology for fitness of purpose across a range of news tasks. If you’re interested in contributing a case study, 
see here
 and 
get in touch with me
.]]></content:encoded>
</item><item>
<title><![CDATA[Using GPT-3 for News Tasks]]></title>
<link>https://generative-ai-newsroom.com/using-gpt-3-for-news-tasks-82ac9cd3cc20?source=collection_home_page----df04cefca135-----81-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[AI]]></category><category><![CDATA[Gpt]]></category><category><![CDATA[Prompting]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-02-26</pubDate>
<atom:updated>2023-02-26</atom:updated>
<content:encoded><![CDATA[Using GPT-3 for News Tasks
Nick Diakopoulos
Follow
Feb 26, 2023
--
Listen
Share
I recently made a Google Colab Notebook that provides some examples of how to use GPT-3 for various tasks in journalism, like rewriting or summarizing text, brainstorming ideas, classifying documents, extracting metadata, and converting structured data to text. I also talk about some basic prompting tips as well as some of the limitations of the model.
I’ve embedded the notebook below, or see the original 
here
.]]></content:encoded>
</item><item>
<title><![CDATA[What Could ChatGPT Do for News Production?]]></title>
<link>https://generative-ai-newsroom.com/what-could-chatgpt-do-for-news-production-2b2a3b7047d9?source=collection_home_page----df04cefca135-----82-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Journalism]]></category><category><![CDATA[News]]></category><category><![CDATA[AI]]></category><category><![CDATA[ChatGPT]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-02-15</pubDate>
<atom:updated>2023-02-15</atom:updated>
<content:encoded><![CDATA[What Could ChatGPT Do for News Production?
Nick Diakopoulos
Follow
10 min read
·
Feb 15, 2023
--
Listen
Share
In early 2022 the Associated Press (AP) 
published a report
 on the overall level of AI understanding and readiness in US newsrooms. Fielding an extensive survey, more than a hundred participants responded with, among other things, their “wish list” of tasks where they really wanted some help from automation. The report aggregates and compiles those responses into a helpful set of tasks where there’s clear demand.
Here I walk through these tasks with an eye towards whether generative AI models like ChatGPT could feasibly help. In virtually all cases additional work is needed to evaluate the quality of output for the tasks. 
In general, I would not recommend using ChatGPT for these tasks without a strict and close editorial process for fact-checking outputs before anything is published
.
Based on my analysis of the tasks there are some patterns in terms of the types of tasks where such models can be helpful. These include tasks involving 
classification
 into potentially user-defined categories, 
rating
 of documents along dimensions of interest like newsworthiness, 
summarization
 of text in journalistically aware ways, and 
personalization
 of content based on end-user characteristics. Classification and rating are more analytic and can generate structured data that can be used in downstream filtering or ranking, whereas summarization and personalization are premised on outputting text that is either more 
efficient
, or more 
relevant
 to consume.
For each task below, I’ll quote the description from the AP report and then add my commentary for how ChatGPT could be useful, whether it has clear limits, and if there are other considerations at play. For implementation examples of some of these tasks see my 
Colab Notebook
, and if you’re excited to try things yourself, check out the 
Generative AI in the Newsroom Challenge
. These tasks are of course just a starting point — lots more creative applications are still waiting to be explored.
1. Content Discovery
“Flagging and gathering social media content like trends, quotes from newsmakers, flag and gather content on government websites, e.g., COVID-19 data, court records, law enforcement records”
The AP distinguishes between content discovery from structured data, like excel sheets or databases, and content discovery from unstructured data, like written documents. Traditional analytics methods are probably better for discovery from structured data, whereas ChatGPT seems well-suited to discovery from unstructured data.
Discovery from unstructured data basically boils down to scanning documents and classifying them according to some category of interest, like 
newsworthy
. Every content discovery task is a little bit different though, and there are 
a lot of different factors that play into whether a document is actually newsworthy
. A GPT-based classifier of news value will need to be carefully evaluated for each specific content discovery task. If you want it to flag interesting court records in comparison to, say, law enforcement records you’ll likely need to write different, specifically tailored prompts. And you’ll need to evaluate the accuracy of each specific classifier to assess things like false positives and false negatives.
2. Document Analysis
“Processing large sets of public records like campaign finance records, state legislation, civil complaints, municipal budgets; in combination with text summarization to help reporters”
Much like the content discovery task, one aspect of document analysis comes down to categorizing and flagging interesting documents for further reporter attention. Another aspect is in summarizing documents so they can be quickly scanned by reporters for relevance. These aspects can get blurry though. For instance, in some of our ongoing research on this topic, we use GPT-3 to summarize newsworthy angles from scientific abstracts, embedding a news judgment into the summarization process. Any document analysis task will also need a careful evaluation of accuracy, both of any news value applied as well as of the text of the summary itself.
When reporters work with documents, those documents sometimes need to be digitized. Text might first need to be indexed through sometimes Optical Character Recognition (OCR) processes that can introduce errors. Whether ChatGPT is robust to these kinds of errors in data is still an open question.
3. Translation
“Translating published stories into multiple languages and processing raw data in various languages”
If you want to publish translations of stories you’re probably better off using a tailored solution (e.g. Google Translate) rather than a general purpose language model like ChatGPT. Even better would be a translation done by a person who can make accurate linguistic and cultural interpretations. But, if you’re thinking more about a use case like multilingual content discovery or document analysis, then ChatGPT might be appropriate for doing a rough translation that can be scanned and evaluated for relevance or interest. This could be interesting, for instance, in multilingual engaged journalism. But ChatGPT is best used only for getting an initial gist of a translation, and is not for publishing. If you want to rely on a document published in another language as a piece of evidence in your reporting, you’ll want to check with a fluent colleague, or maybe even get a certified translation.
4. Tips Processing
“Moderating story idea submissions and questions; verifying tips.”
One aspect of tips processing is a sort of special case of content discovery from unstructured documents that come in via email or other information channels. In some of 
our work on engaged journalism
 we’ve looked at survey callouts and online comments as avenues for learning about community interests for follow up reporting. For these types of use cases ChatGPT can be used in all the same ways for detecting newsworthiness and relevance of a document, according to whatever categories of newsworthiness you want to set up. Again, you’ll need to do tailored evaluations of this for the specific kinds of “tips” you’re scanning.
In terms of verifying tips, ChatGPT is not able to help at all. Remember that ChatGPT is limited in terms of its training data. As search engines begin to integrate ChatGPT they’ll build in ways to allow the model pull from more updated search results, but this, again, will not be of any use in corroborating a tip: if there were data or documents out there to corroborate the tip that the model already had access to then it wouldn’t really be a very good tip, would it?
5. Social Media Content Creation
“Generating content (e.g., text, video, photo, audio) and scheduling optimized posts to Twitter, Facebook, Instagram; cropping photos and videos for different formats.”
It’s a complex communication task to find the right tone for social posts and frame them so they resonate with particular audiences. And so I wouldn’t necessarily recommend trying to automate this task using generative AI. But producing excerpts and summaries of textual content for social channels is something that ChatGPT can definitely 
help
 with as long as there’s a human in the loop to dynamically prompt the machine and evaluate the results before they’re published. Perhaps there are patterns of prompts that could accelerate engagement editors’ work.
ChatGPT won’t be able to help with the other aspects of this task category: scheduling posts and cropping visual media. Scheduling is an analytic function that’s better addressed with other types of machine learning based on audience characteristics and data on past publishing. No doubt there will be other types of generative AI models that can help with visual tasks of cropping and editing video, but those will need careful evaluation before they’re used for publishing.
6. Automated Writing (Structured Data)
“High school sports, college sports, weather, natural events (e.g., tides, fires), restaurant report cards, police logs, elections, agriculture grain bids, business licenses, real estate and community calendars.”
Automated writing based on structured data is a familiar use-case for news media going back a decade or more. But most of the industrial-scale automated writing in use today is based on more straightforward template-based approaches. ChatGPT can also render fluently written text based on structured data inputs, but it comes with more caveats because of the statistical sampling involved in its text production. They also generally can’t do math, and so if there are specific mathematical operations needed, it is better to do those with traditional analytic methods first and then feed those pre-computed data files into ChatGPT for language generation. The bottom line is that the accuracy of texts generated from data need to be evaluated for accuracy before those texts are published.
7. Automated Writing (Unstructured Data)
“Obituaries, press release briefs, event previews, etc”
Automated writing from unstructured data is more challenging than the structured data case. But ChatGPT can also work to extract structured data from unstructured data, and then use that to generate text. Perhaps there are press releases your newsroom receives about community events and the model can extract things like dates, times, locations, and short descriptions of those events. This structured data can then be fed back to the model to output a written description of the event. Much like for the structured case though, you’ll want to check the output for accuracy before publication.
The AP description also suggests that “obituaries” are an example where there is a need for automation. Ethically it feels wrong to delegate the task of selecting the most important and meaningful life events for a person to a machine. However, if an editor selected life events and wrote up a set of bullets, you could imagine having ChatGPT render that into a draft text. Then again, if you’re going to do the work of researching, selecting, editing, and writing the life points, I doubt ChatGPT can save much time.
8. Newsletters
“Personalize newsletters and optimize newsletter delivery times.”
There are better ways to do newsletter curation using more traditional recommender system approaches, and ChatGPT won’t be any help with optimizing delivery times either. But there’s a lot of potential for newsletter personalization in terms of tailoring the curated content to appeal to an individual’s interests. ChatGPT could be used to frame or rewrite headlines or summaries of the curated content to be more personally appealing, based on a user model. For instance, if I know that you’re the type of person who often reads articles about inflation, I could highlight that in the headline or blurb of an article in a newsletter. This use case is more about emphasizing the frames from an article which are particularly appealing to an individual. Much as for other automated writing use cases these personalized excerpts would need to be carefully evaluated though. To implement this you would probably define a limited set of audience segments for personalization, and then assign an editor to manually review how each audience segment would see the newsletter before publication.
9. Text Summarization
“Generate summaries (e.g., briefs, from government meetings and cut downs of broadcast packages)”
There’s a lot of information coming at journalists everyday, and summarization could help make scanning that information more efficient. But, the key issue here is whether the summarization is 
journalistically aware
. Is the summarization able to pick up on journalistic relevance in selecting and prioritizing what subset of information to include in the summary? You could certainly apply ChatGPT to summarize a government meeting, but it would be a complete failure if it left out a key controversial exchange. The main question is: in your specific content, what is your tolerance for missing something? There are possibilities here, but still much research to do on how to prompt ChatGPT to be sensitive to the right factors in summarization and to evaluate the system’s abilities for this task.
10. Comment Moderation
“Filtering by language, duplicate commenter accounts and compiling comments for marketing”
Comment moderation is a use case of great interest, not only for news organizations but also for social media platforms in general. Insofar as comment moderation can be reduced to classification (e.g. “hate speech”, “top-quality comment”) then ChatGPT can be used here, though with all of the disclaimers described above. For any particular category of comment that should be detected there should be an extensive evaluation of the accuracy, false positives, and false negatives, with an eye towards the cost to the community of any of those moderation errors. The AP report also mentions “compiling comments for marketing” and this starts to get into more of a summarization use-case. If the marketing is low-stakes it should also be possible to use ChatGPT here.
11. Content Transformation and Reuse
“Format articles as structured data to enable reuse in different platforms, format broadcast scripts for the web.”
This will depend on the exact transformations and data extractions needed to reuse content on different platforms, but in general ChatGPT should be able to help with these kinds of tasks. One example would be in taking an article and generating a headline and keywords for posting it to social media. If the metadata generated by the model is not visible to end users, such as with keywords, you can probably make do with lower accuracy. But still, any outputs of the model should be evaluated to ensure they’re high-enough quality and not totally off base.
12. Search Engine Optimization
“Integration with A/B headline testing, offering recommendations and integration with archives to recommend evergreen content”
Non-GPT AI technologies for content recommendation from archives and for A/B headline testing are already well-developed. The area where I see models like ChatGPT helping here is in suggesting variations of headlines for 
A/B testing
. In this case I expect fine-tuned models of large language models will have a better chance of recreating any particular publication’s headline writing style. Otherwise, you would want to come up with unique and tailored prompts to set apart the headlines generated. As with other use cases though you’ll always want to have a human in-the-loop. Much better to think of this as a headline 
brainstorming
 tool to help you come up with diverse headlines to test with your audience. More broadly, metadata useful for SEO purposes, such as keywords or a summary text, could also be generated. The way search engines integrate generated text from an SEO perspective will be an area of active scrutiny and development as companies like Google and Microsoft also figure out what it means to have the technology more firmly embedded in their search engines.
13. Push-Alert Personalization
“Extending story recommendation abilities to personalize mobile push alerts”
While you would want to use non-GPT AI technologies for the actual push-alert recommendation engine, ChatGPT could be useful in the same ways as discussed above for use cases in headline or newsletter generation. For instance, the writing of push alerts could be personalized to different audience segments. As with virtually all the other use cases you’ll want a person checking the text generations before publishing them.]]></content:encoded>
</item><item>
<title><![CDATA[The Generative AI in the Newsroom Challenge]]></title>
<link>https://generative-ai-newsroom.com/the-generative-ai-in-the-newsroom-challenge-9fe2dc5fb2a7?source=collection_home_page----df04cefca135-----83-----------------------------------</link>
<guid isPermaLink="false">https://medium.com/p/</guid>
<category><![CDATA[Generative Ai]]></category><category><![CDATA[Journalism]]></category><category><![CDATA[News Media]]></category>
<dc:creator><![CDATA[Nick Diakopoulos]]></dc:creator>
<pubDate>2023-01-14</pubDate>
<atom:updated>2023-01-14</atom:updated>
<content:encoded><![CDATA[The Generative AI in the Newsroom Challenge
Nick Diakopoulos
Follow
3 min read
·
Jan 14, 2023
--
1
Listen
Share
Have you ever toyed with generative AI models like 
ChatGPT
, 
GPT-3
, or 
DALL-E
 and wondered whether they could actually help you do some newswork? The technology is poised to disrupt many aspects of media and communication industries by making content — not only text, but also visual imagery — easier to create. For instance, these tools can be used to 
illustrate news stories with images or videos
, 
extract structured data from unstructured documents
, or do a range of writing tasks from simplifying text for different audiences to summarizing documents, writing potential headlines, and brainstorming angles or potential directions for reporting, like in the following request to ChatGPT:
The Challenge
I am a 
researcher
 interested in understanding the challenges facing journalism practitioners in implementing generative AI in the newsroom. If you have a creative idea for how to apply a generative AI model (e.g. ChatGPT, GPT-3, DALL-E, or something similar) to help with a journalistic task (e.g. related to editing, reporting, curation, distribution, etc.), please send me a short pitch of 200–300 words describing the idea. I will select a number of pitches and will offer to work with the selected projects over the period of one month as a technical consultant to help think through how to make the idea work and to evaluate it.
The pitch should motivate and describe the specific journalistic task you think generative AI can help with, why and how you think it will help, and how you’re going to test it to see if it works well enough that you could actually use it in practice. If you think there are particular challenges, ethical concerns, or other aspects you’re not sure about please describe these too. As you work on your pitch you should sign up for a free account with OpenAI’s 
ChatGPT
, 
GPT-3
, or 
DALL-E
 tools so you can test the initial feasibility of your idea. OpenAI makes available extensive 
documentation
 on their technologies or 
you can check out the Notebook I created
 which can help you understand the capabilities, limitations, and potential use cases. Originality, feasibility, and your plan for testing the reliability of your approach will be the key selection criteria for pitches.
Once selected you will have about a month to implement and evaluate the idea as a pilot / prototype using, for example, the free interfaces from OpenAI linked above. Coding is not required, but if you do code you can also implement your idea using available APIs as well as other open-sourced or fine-tuned models. As part of this work you will also write a ~1000 word article based on your experience and on the outcome of your effort to describe what you achieved, how you did it, what kinds of prompts you needed to write to control the AI, what you learned that might help others follow in your footsteps, and any cautions or other reflections you may have. Once complete and accepted, I will publish the articles on a blog for the project and Northwestern University will pay you $250 for your time and effort.
You can email pitches to me, 
Nick Diakopoulos
, at 
nad@northwestern.edu
 with the subject “Generative AI in the Newsroom Challenge”. Pitches will be evaluated on a rolling basis until March 15, 2023.]]></content:encoded>
</item>
</channel>
</rss>
